{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezu98/colab/blob/main/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install vaderSentiment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from openpyxl import Workbook, load_workbook\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "import datetime as dt\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "YFNlv189OpWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "import textstat\n",
        "from textstat import flesch_kincaid_grade, gunning_fog"
      ],
      "metadata": {
        "id": "IX6CrBx5nZn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "mzNfTBgJnoNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'results2.xlsx'"
      ],
      "metadata": {
        "id": "b9RjbLiokLs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Intensity Analyzer\n",
        "# Example text\n",
        "sample_text = \"The Monetary Policy Board of the Bank of Korea decided today to leave the Base Rate unchanged at 3.50% for the intermeeting period. Although inflation is projected to continue its underlying trend of a slowdown, uncertainties regarding the future path of inflation and growth have risen significantly due to a prolongation of restrictive monetary policy stances in major countries and heightened geopolitical risks. In addition, it is forecast that the pace of inflation slowdown will moderate more than previously expected, and it is necessary to monitor household debt growth. The Board, therefore, sees that it is appropriate to maintain its current restrictive policy stance. Regarding the need to raise the Base Rate further, the Board will make a judgement while assessing the changes in domestic and external policy conditions. The currently available information suggests that uncertainties regarding economic and inflationary trends have increased across the global economy, driven by a prolongation of restrictive monetary policy stances in major countries and by the Israel-Hamas conflict. Global economic growth is projected to continue slowing. Inflation in major countries still remains high, though falling gradually, and upside risks have increased due to the rise in global oil prices. In global financial markets, volatility of major price variables has increased with government bond yields rising significantly and with the U.S. dollar strengthening considerably. Looking ahead, the Board sees global economic growth and global financial markets as likely to be affected by the movements of global oil prices and the global inflation slowdown, by monetary policy changes in major countries and their effects, and by developments in the Israel-Hamas conflict. Domestic economic growth has continued to improve at a modest pace owing to the easing of sluggishness in exports, although the recovery in private consumption has been somewhat slow. Labor market conditions have been generally favorable, as both a low unemployment rate and a robust increase in the number of persons employed have continued. Going forward, domestic economic growth is expected to improve gradually with the easing of the sluggishness in exports. GDP growth for the year is expected to be generally consistent with the August forecast of 1.4%. However, uncertainties surrounding the economic outlook are judged to be elevated, affected by heightened geopolitical risks and by the prolongation of restrictive monetary policy stances in major countries. Consumer price inflation has risen from August to 3.7% in September, due to the increase in the price of energy and of agricultural products. However, both core inflation (excluding changes in food and energy prices from the CPI) and short-term inflation expectations among the general public have stayed at 3.3% in September, the same as in August. Looking ahead, it is forecast that consumer price inflation will fall to the lower-3% range at the end of this year and will continue to gradually moderate in 2024. However, upside risks to inflation have increased due to the effects of higher global oil prices and exchange rates, and due to the Israel-Hamas conflict. Accordingly, it is judged that the timing of consumer price inflation converging on the target level is more likely to be delayed than previously expected. Meanwhile, core inflation is also projected to maintain its underlying slowing trend, owing to the weakening of demand-side pressures. However, the pace of the slowdown is likely to be more modest than previously forecast due to the continuing spillover effects of accumulated cost pressure. In financial and foreign exchange markets, volatility has increased as the U.S. Federal Reserve has signaled a prolongation of a high policy rate and as geopolitical risks have expanded. Long-term Korean Treasury bond yields and the Korean won to U.S. dollar exchange rate have risen significantly and stock prices have fallen. Meanwhile, the risks to some non-bank financial sectors have eased. Housing prices have continued their upward trend, especially in Seoul and its surrounding areas. Household loans have continued to increase, mainly driven by housing-related loans. The Board will continue to conduct monetary policy in order to stabilize consumer price inflation at the target level over the medium-term horizon as it monitors economic growth, while paying attention to financial stability. While domestic economic growth is forecast to gradually improve, uncertainties surrounding the policy decision have also risen. The Board, therefore, will maintain a restrictive policy stance for a considerable time with an emphasis on ensuring price stability, while making a judgement regarding the need to raise the Base Rate further. In this process, the Board will thoroughly assess the inflation slowdown, financial stability risks, economic downside risks, monetary policy changes in major countries, household debt growth, and developments in geopolitical risks.\"\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "# Perform sentiment analysis\n",
        "scores = analyzer.polarity_scores(sample_text)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "XEjqfmw1Doi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Analyze\n",
        "\n",
        "### by loughran and mcdonald(2019)\n",
        "\n",
        "## loading master dictionary\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0].upper()  # Convert word to uppercase\n",
        "            negative_value = int(cols[7])\n",
        "            positive_value = int(cols[8])\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = {'negative': negative_value, 'positive': positive_value, 'uncertainty': uncertainty_value}\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary"
      ],
      "metadata": {
        "id": "1WI8u6caYHKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## evaluate sentiment indices\n",
        "\n",
        "def evaluate_sentiment_indices(text, master_dictionary):\n",
        "    words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    total_words_count, negative_score, positive_score, uncertain_words_count = 0, 0, 0, 0\n",
        "    negative_words_count, positive_words_count = 0, 0\n",
        "\n",
        "    for word in words_uppercase:\n",
        "        if word in master_dictionary:\n",
        "            total_words_count += 1  # Increment the total word count\n",
        "            if master_dictionary[word]['uncertainty'] != 0:\n",
        "                uncertain_words_count += 1\n",
        "            if master_dictionary[word]['negative'] > 0:\n",
        "                negative_words_count += 1\n",
        "            if master_dictionary[word]['positive'] > 0:\n",
        "                positive_words_count += 1\n",
        "\n",
        "    negative_index = negative_words_count / total_words_count  # Calculate the negative index\n",
        "    positive_index = positive_words_count / total_words_count  # Calculate the positive index\n",
        "    uncertainty_index = uncertain_words_count / total_words_count  # Calculate the uncertainty index\n",
        "\n",
        "    return negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "#    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)"
      ],
      "metadata": {
        "id": "kr1JHw5_XkgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Financial Stability Dictionary\n",
        "### by Correa, Ricardo & Keshav Garud & Juan M. Londono & Nathan Mislang, 2021. \"Sentiment in Central Banks' Financial Stability Reports,\" Review of Finance, vol. 25(1), pages 85-120.\n",
        "\n",
        "# Load the workbook\n",
        "file_path_fs = 'f.xlsx'\n",
        "workbook_fs = load_workbook(file_path_fs)\n",
        "worksheet_fs = workbook_fs['1 FS Dictionary']\n",
        "\n",
        "# Extract the words and their respective positive, negative, and assign values\n",
        "fs_words = []\n",
        "fs_positives = []\n",
        "fs_negatives = []\n",
        "fs_assigns = []\n",
        "\n",
        "for row in worksheet_fs.iter_rows(min_row=2, min_col=1, max_col=4):\n",
        "    if row[0].value:\n",
        "        fs_words.append(row[0].value)\n",
        "        fs_positives.append(row[1].value)\n",
        "        fs_negatives.append(row[2].value)\n",
        "        fs_assigns.append(row[3].value)\n",
        "\n",
        "# Define the function to count the words in the fs dictionary in the original text and calculate the sum of positive and negative values\n",
        "def count_fs_words(text, fs_words, fs_positives, fs_negatives):\n",
        "    text_words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    fs_positive_words = [word for word in text_words if word in fs_words and fs_positives[fs_words.index(word)] == 1]\n",
        "    fs_negative_words = [word for word in text_words if word in fs_words and fs_negatives[fs_words.index(word)] == 1]\n",
        "    fs_positive_sum = sum([fs_positives[fs_words.index(word)] for word in text_words if word in fs_words and fs_positives[fs_words.index(word)] == 1])\n",
        "    fs_negative_sum = sum([fs_negatives[fs_words.index(word)] for word in text_words if word in fs_words and fs_negatives[fs_words.index(word)] == 1])\n",
        "    return fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words"
      ],
      "metadata": {
        "id": "IGmv_UpydsUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Readiability\n",
        "\n",
        "def evaluate_readability(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "    num_syllables = 0\n",
        "    for word in words:\n",
        "        num_syllables += textstat.syllable_count(word)\n",
        "    return textstat.flesch_kincaid_grade(text), textstat.gunning_fog(text)"
      ],
      "metadata": {
        "id": "CIjhsXXqpvS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Example FOMC statement to evaluate - manually calculating\n",
        "\n",
        "date = \"2010.10\"\n",
        "\n",
        "fomc_statement = \"\"\"\n",
        "   The Monetary Policy Committee of the Bank of Korea decided today to maintain the Base Rate at its current level (2.25%) for the intermeeting period.\n",
        "\n",
        "\n",
        "\n",
        "In the global economy, emerging market economies have sustained their favorable performance, and the economies of major advanced countries have largely continued their moderate recovery trend, even though the pace of the recovery in the US economy has slowed somewhat. Looking ahead, there exists the possibility of the heightened volatility of economic activity and exchange rates in major countries acting as a risk factor for the global economy.\n",
        "\n",
        "\n",
        "\n",
        "The underlying upward trend of domestic business activity has been maintained. Even though consumption has faltered, exports have sustained their buoyancy and facilities investment has increased. And, led by the private sector, labor market conditions have shown an improving trend.\n",
        "\n",
        "\n",
        "\n",
        "The domestic economy is expected to continue on an underlying upward track, even in the presence of external risk.\n",
        "\n",
        "\n",
        "\n",
        "Consumer price inflation has increased due to a sudden rise in farm product prices, and in the future, upward pressures on the demand side are expected to continue, being associated with the continued upswing in activity. In the real estate market, housing sales prices have continued to decline in Seoul and its surrounding areas, while those in other areas have maintained their increase.\n",
        "\n",
        "\n",
        "In the financial markets, stock prices have risen, the Korean won has appreciated, and market interest rates have declined in response chiefly to the expanded inflow of foreign portfolio investment funds. The scale of the growth in mortgage lending has widened due to the increase in the numbers of newly occupied apartments, even though house transactions have been inactive.\n",
        "\n",
        "\n",
        "\n",
        "Looking ahead, the Committee will conduct monetary policy in such a way as to help the economy maintain price stability, while sustaining sound growth under the accommodative policy stance. In carrying out policy, it will take overall account of financial and economic conditions at home and abroad\n",
        "\n",
        "\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "\n",
        " ## run!!!\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Remove unnecessary line breaks and spaces\n",
        "    processed_text = re.sub(r'\\n+', ' ', fomc_statement)  # Replace multiple line breaks with a single space\n",
        "    processed_text = re.sub(r' +', ' ', processed_text)  # Replace multiple spaces with a single space\n",
        "\n",
        "    negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count = evaluate_sentiment_indices(processed_text, master_dictionary)\n",
        "    flesch_kincaid_grade, gunning_fog_index = evaluate_readability(processed_text)\n",
        "    fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words = count_fs_words(processed_text, fs_words, fs_positives, fs_negatives)\n",
        "\n",
        "    print(f\"\\nNumber of uncertain words: {uncertain_words_count}\")\n",
        "    print(f\"Number of negative words: {negative_words_count}\")\n",
        "    print(f\"Number of positive words: {positive_words_count}\")\n",
        "    print(f\"Number of total words: {total_words_count}\")\n",
        "    print(f\"Negative index for the FOMC statement: {negative_index:.2f}\")\n",
        "    print(f\"Positive index for the FOMC statement: {positive_index:.2f}\")\n",
        "    print(f\"Uncertainty index for the FOMC statement: {uncertainty_index:.2f}\")\n",
        "    print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_grade}\")\n",
        "    print(f\"Gunning Fog Index: {gunning_fog_index}\")\n",
        "    print(f\"FS positive sum: {fs_positive_sum}\")\n",
        "    print(f\"FS negative sum: {fs_negative_sum}\")\n",
        "    print(f\"FS_positive_words: {fs_positive_words}\")\n",
        "    print(f\"FS_negative_words: {fs_negative_words}\")\n",
        "\n",
        "    # Printing the actual words defined as positive, negative, and uncertainty\n",
        "    words = nltk.word_tokenize(processed_text)\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    words_with_sentiment = [(word, master_dictionary[word]) for word in words_uppercase if word in master_dictionary]\n",
        "    positive_words = [word for word, sentiment in words_with_sentiment if sentiment['positive'] > 0]\n",
        "    negative_words = [word for word, sentiment in words_with_sentiment if sentiment['negative'] > 0]\n",
        "    uncertain_words = [word for word, sentiment in words_with_sentiment if sentiment['uncertainty'] > 0]\n",
        "    print(f\"Positive words: {positive_words}\")\n",
        "    print(f\"Negative words: {negative_words}\")\n",
        "    print(f\"Uncertainty words: {uncertain_words}\")\n",
        "\n",
        "\n",
        "# Check if the file already exists, and load it if it does\n",
        "try:\n",
        "    workbook = load_workbook(file_path)\n",
        "    sheet = workbook.active\n",
        "except FileNotFoundError:\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.append(['Date', 'FOMC Statement', 'Positive Words', 'Negative Words', 'Uncertainty Words', 'Total Words', 'Positive Words List', 'Negative Words List', 'Uncertainty Words List', 'Flesch-Kincaid Grade Level', 'Gunning Fog Index', 'FS Positive_Counts', 'FS Negative_Counts', 'FS Positive Words', 'FS Negative Words'])\n",
        "\n",
        "\n",
        "# Append results to the Excel file\n",
        "sheet.append([date, processed_text, positive_words_count, negative_words_count, uncertain_words_count, total_words_count, ', '.join(positive_words), ', '.join(negative_words), ', '.join(uncertain_words), flesch_kincaid_grade, gunning_fog_index, fs_positive_sum, fs_negative_sum, '.join(fs_positive_words), ', '.join(fs_negative_words)'])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(file_path)\n"
      ],
      "metadata": {
        "id": "Q8Q6ub05ZNyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FOMC statement to evaluate - using existing library - cleaning needed. imprecise.\n",
        "\n",
        "# dataset = dataset, dataset_minutes, dataset_speechs\n",
        "# dataset = dataset_statement\n",
        "\n",
        "# Check if the file already exists, and load it if it does\n",
        "try:\n",
        "    workbook = load_workbook(file_path)\n",
        "    sheet = workbook.active\n",
        "except FileNotFoundError:\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.append(['Date', 'FOMC Statement', 'Positive Words', 'Negative Words', 'Uncertainty Words', 'Total Words', 'Positive Words List', 'Negative Words List', 'Uncertainty Words List', 'Flesch-Kincaid Grade Level', 'Gunning Fog Index', 'FS Positive_Counts', 'FS Negative_Counts', 'FS Positive Words', 'FS Negative Words'])\n",
        "\n",
        "# Loop through the dataset and process each row\n",
        "for index, row in dataset.iterrows():\n",
        "    # Retrieve the date and FOMC statement from the current row\n",
        "    date = row['date']  # Assuming the column name for the date is 'date'\n",
        "    fomc_statement = row['statement']  # Assuming the column name for the statement is 'statement'\n",
        "\n",
        "    # Your existing code\n",
        "    start = dt.datetime.now()\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Remove unnecessary line breaks and spaces\n",
        "    processed_text = re.sub(r'\\n+', ' ', fomc_statement)\n",
        "    processed_text = re.sub(r' +', ' ', processed_text)\n",
        "\n",
        "    # Perform sentiment analysis, readability calculation, and other necessary calculations here\n",
        "\n",
        "    negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count = evaluate_sentiment_indices(processed_text, master_dictionary)\n",
        "    flesch_kincaid_grade, gunning_fog_index = evaluate_readability(processed_text)\n",
        "    fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words = count_fs_words(processed_text, fs_words, fs_positives, fs_negatives)\n",
        "\n",
        "    # Printing the actual words defined as positive, negative, and uncertainty\n",
        "    words = nltk.word_tokenize(processed_text)\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    words_with_sentiment = [(word, master_dictionary[word]) for word in words_uppercase if word in master_dictionary]\n",
        "    positive_words = [word for word, sentiment in words_with_sentiment if sentiment['positive'] > 0]\n",
        "    negative_words = [word for word, sentiment in words_with_sentiment if sentiment['negative'] > 0]\n",
        "    uncertain_words = [word for word, sentiment in words_with_sentiment if sentiment['uncertainty'] > 0]\n",
        "\n",
        "    # Append results to the Excel file\n",
        "    sheet.append([date, processed_text, positive_words_count, negative_words_count, uncertain_words_count, total_words_count, ', '.join(positive_words), ', '.join(negative_words), ', '.join(uncertain_words), flesch_kincaid_grade, gunning_fog_index, fs_positive_sum, fs_negative_sum, ', '.join(fs_positive_words), ', '.join(fs_negative_words)])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(file_path)"
      ],
      "metadata": {
        "id": "-wOQIFYCvug1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Elminate stop words(conjunction, article, and preposition)\n",
        "\n",
        "# Function to remove stopwords from a piece of text\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    clean_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(clean_words)"
      ],
      "metadata": {
        "id": "zr0pQQlrkfUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the Excel file\n",
        "dataset_cleaning = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Get the stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Apply the remove_stopwords function to the 'FOMC Statement' column and save the result to 'FOMC Statement_no_stopwords'\n",
        "dataset_cleaning['FOMC Statement_no_stopwords'] = dataset_cleaning['FOMC Statement'].apply(remove_stopwords)\n",
        "\n",
        "# Calculate the number of words\n",
        "dataset_cleaning['words_cleaning'] = dataset_cleaning['FOMC Statement_no_stopwords'].str.split().apply(len)\n",
        "\n",
        "# Calculate the number of occurrences of the word 'will'\n",
        "dataset_cleaning['forward_words_will'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bwill\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'expect'\n",
        "dataset_cleaning['forward_words_expect'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bexpect\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'may'\n",
        "dataset_cleaning['forward_words_may'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bmay\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'might'\n",
        "dataset_cleaning['forward_words_might'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bmight\\b')\n",
        "\n",
        "# Save the modified DataFrame to a new Excel file\n",
        "with pd.ExcelWriter('new_results.xlsx', engine='openpyxl') as writer:\n",
        "    dataset_cleaning.to_excel(writer, index=False, sheet_name='Sheet1')"
      ],
      "metadata": {
        "id": "kGZ6Ar-alKid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Ziff's law and tf-idf score\n",
        "\n",
        "# Load the excel\n",
        "# file_path = 'results2.xlsx'\n",
        "dataset_ziff = pd.read_excel(file_path)\n",
        "# If Uncertainty Words List, it has vacant values. So use the following code.\n",
        "\n",
        "# You may need to adjust the column names based on your dataset\n",
        "dates = dataset_ziff['Date']\n",
        "statements = dataset_ziff['Negative Words List']\n",
        "\n",
        "# Create a dictionary to store words by their respective dates\n",
        "words_by_date = {}\n",
        "\n",
        "for index, statement in enumerate(statements):\n",
        "    # Filter the statements to consider only those after 2019\n",
        "    if dates[index].year < 2020:\n",
        "        continue\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(statement)\n",
        "\n",
        "    # Store the words with their respective date\n",
        "    date = dates[index]\n",
        "    if date not in words_by_date:\n",
        "        words_by_date[date] = []\n",
        "    words_by_date[date].extend(words)\n",
        "\n",
        "# Now words_by_date dictionary contains the words stored by their respective dates after 2019\n",
        "\n",
        "# Convert the words_by_date dictionary into a DataFrame\n",
        "data = {'Date': list(words_by_date.keys()), 'Words': list(words_by_date.values())}\n",
        "dataset_result_ziff = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "dataset_result_ziff.to_excel('result_word.xlsx', index=False)"
      ],
      "metadata": {
        "id": "fLqpDT4ZYN8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Ziff's law\n",
        "\n",
        "# Assuming you have already created the 'df' DataFrame\n",
        "words_list = dataset_result_ziff['Words'].tolist()\n",
        "words_flat = [word for sublist in words_list for word in sublist]\n",
        "\n",
        "# Ziff's Law\n",
        "word_counts = Counter(words_flat)\n",
        "sorted_word_counts = sorted(word_counts.values(), reverse=True)\n",
        "ranks = list(range(1, len(sorted_word_counts) + 1))\n",
        "log_ranks = np.log(ranks)\n",
        "log_counts = np.log(sorted_word_counts)\n",
        "\n",
        "# Plot the scatter plot\n",
        "plt.scatter(log_ranks, log_counts, color='b', alpha=0.5)\n",
        "plt.title(\"Scatter plot for Ziff's Law\")\n",
        "plt.xlabel(\"Log of Ranks\")\n",
        "plt.ylabel(\"Log of Word Frequencies\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WZdfqoLwksW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TF-idf score\n",
        "\n",
        "# TF-IDF Scores\n",
        "text = [' '.join(words) for words in words_list]\n",
        "\n",
        "# Create the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for the TF-IDF scores\n",
        "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=dataset_result_ziff['Date'])\n",
        "\n",
        "# Save the results to Excel files\n",
        "# ziffs_law_df = pd.DataFrame({'Ranks': ranks, 'Log Counts': log_counts, 'Log Ranks': log_ranks})\n",
        "# ziffs_law_df.to_excel('ziffs_law_results.xlsx', index=False)\n",
        "tfidf_df.to_excel('tfidf_scores.xlsx')"
      ],
      "metadata": {
        "id": "y1IYOLfkbdPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Relationship between words\n",
        "\n",
        "# Load the excel\n",
        "dataset_relationship = pd.read_excel(file_path)\n",
        "\n",
        "# Define the word of interest\n",
        "target_word = 'crisis'\n",
        "\n",
        "# Initialize a counter for the occurrences of the target word\n",
        "occurrences_counter = 0\n",
        "\n",
        "# Iterate through the dataset to process the FOMC statements\n",
        "for index, row in dataset_relationship.iterrows():\n",
        "    date = row['Date']\n",
        "    statement = row['FOMC Statement']\n",
        "\n",
        "    # Check if the date falls within your specified time span\n",
        "    if date.year < 2020:\n",
        "        continue\n",
        "\n",
        "    # Tokenize the FOMC statement into sentences\n",
        "    sentences = nltk.sent_tokenize(statement)\n",
        "\n",
        "    # Iterate through each sentence and check for occurrences of the target word\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        if target_word in words:\n",
        "            index_of_word = words.index(target_word)\n",
        "\n",
        "            # Extract the surrounding words\n",
        "            words_around = words[max(0, index_of_word - 20): min(len(words), index_of_word + 25)]\n",
        "            print(f\"Date: {date}, {' '.join(words_around)}\")\n",
        "\n",
        "\n",
        "            # Increment the counter\n",
        "            occurrences_counter += 1\n",
        "\n",
        "# Print the total number of occurrences of the target word\n",
        "print(f\"Total occurrences of '{target_word}': {occurrences_counter}\")"
      ],
      "metadata": {
        "id": "lxUlQjlPwq0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LDA\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "metadata": {
        "id": "nYX6ysVXeqgb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = pd.read_excel(file_path)\n",
        "\n",
        "# Concatenate all text values in 'FOMC Statement' into a list of strings\n",
        "concat_statements = dataset['FOMC Statement'].tolist()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_text = [word_tokenize(statement) for statement in concat_statements]\n",
        "\n",
        "# Prepare the vocabulary\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "roCdntDoUg-6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_word custom\n",
        "custom_stop_words = stop_words.union(['chairman', 'Alan', 'Greenspan', 'Jr', 'Roger', 'Ferguson', 'Chairman', 'billion', 'Chair', 'pace', 'Reserve', 'time'])  # Add 'chairman' to the list of stop words\n",
        "\n",
        "punctuation = set(string.punctuation)\n",
        "texts = [\n",
        "    [word for word in statement if word not in custom_stop_words and word not in punctuation]\n",
        "    for statement in tokenized_text\n",
        "]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Filter out words that occur in less than 1 document or more than 80% of the documents\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
        "\n",
        "# Bag-of-words representation of the documents\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
        "\n",
        "# Get the topics for each document\n",
        "topics = [lda_model[corpus[i]] for i in range(len(corpus))]\n",
        "print(topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enf50RksUJJp",
        "outputId": "13558e35-fc57-42c9-940e-cc70148b3e23"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.015*\"2\" + 0.013*\"securities\" + 0.013*\"'s\" + 0.013*\"employment\" + 0.013*\"appropriate\" + 0.011*\"continue\" + 0.010*\"support\" + 0.010*\"goals\" + 0.010*\"stance\" + 0.010*\"maximum\"\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.018*\"securities\" + 0.015*\"labor\" + 0.013*\"agency\" + 0.013*\"employment\" + 0.012*\"appropriate\" + 0.012*\"toward\" + 0.010*\"maximum\" + 0.010*\"'s\" + 0.010*\"mortgage-backed\" + 0.009*\"longer-term\"\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.023*\"labor\" + 0.021*\"2\" + 0.015*\"expected\" + 0.014*\"measures\" + 0.011*\"employment\" + 0.010*\"developments\" + 0.010*\"activity\" + 0.010*\"maximum\" + 0.009*\"remain\" + 0.009*\"objective\"\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.013*\"growth\" + 0.012*\"S.\" + 0.009*\"markets\" + 0.009*\"levels\" + 0.009*\"securities\" + 0.009*\"likely\" + 0.009*\"M.\" + 0.008*\"continue\" + 0.007*\"low\" + 0.007*\"housing\"\n",
            "\n",
            "[[(3, 0.9929595)], [(3, 0.99464905)], [(3, 0.99492216)], [(3, 0.99455625)], [(3, 0.9925411)], [(3, 0.99212724)], [(3, 0.9924927)], [(3, 0.9928854)], [(3, 0.99148136)], [(3, 0.9904985)], [(3, 0.9907897)], [(3, 0.99150115)], [(3, 0.99285275)], [(3, 0.9940928)], [(3, 0.9947949)], [(3, 0.9944965)], [(3, 0.993504)], [(1, 0.65821075), (3, 0.3390228)], [(3, 0.9946445)], [(3, 0.9947128)], [(2, 0.01293105), (3, 0.9827583)], [(3, 0.9934645)], [(3, 0.99314296)], [(1, 0.07404888), (3, 0.9231505)], [(3, 0.9949548)], [(3, 0.99619937)], [(3, 0.99656785)], [(3, 0.9961878)], [(3, 0.9960904)], [(3, 0.9957279)], [(3, 0.9961569)], [(3, 0.9962481)], [(3, 0.99660873)], [(3, 0.9973614)], [(3, 0.9971567)], [(3, 0.9964427)], [(3, 0.99620646)], [(1, 0.89440954), (3, 0.10139107)], [(3, 0.9957296)], [(1, 0.028515158), (3, 0.9690297)], [(3, 0.99624276)], [(1, 0.16066697), (3, 0.83717215)], [(1, 0.1634027), (3, 0.8344065)], [(1, 0.1426596), (3, 0.8549323)], [(1, 0.10471861), (3, 0.892895)], [(1, 0.09519634), (3, 0.9024043)], [(1, 0.19670579), (3, 0.80094355)], [(1, 0.14519122), (3, 0.85266197)], [(1, 0.34132814), (3, 0.6567823)], [(1, 0.33724353), (3, 0.6605681)], [(1, 0.43042552), (3, 0.567121)], [(1, 0.5648542), (3, 0.43262157)], [(1, 0.5081601), (3, 0.48937482)], [(1, 0.5107223), (3, 0.48685238)], [(1, 0.5734898), (3, 0.42439523)], [(1, 0.5812667), (3, 0.41637853)], [(1, 0.8779624), (3, 0.12018293)], [(1, 0.96299326), (3, 0.035107028)], [(1, 0.9976328)], [(1, 0.9974993)], [(1, 0.99748147)], [(1, 0.9975107)], [(1, 0.9975657)], [(1, 0.99760646)], [(1, 0.99791163)], [(1, 0.9978702)], [(1, 0.9980851)], [(1, 0.9980468)], [(1, 0.9981492)], [(1, 0.99800706)], [(1, 0.9979926)], [(1, 0.9980696)], [(1, 0.9981724)], [(1, 0.87991214), (2, 0.11856964)], [(1, 0.8627093), (2, 0.13581467)], [(1, 0.738521), (2, 0.25968614)], [(1, 0.7023162), (2, 0.29587576)], [(1, 0.5734501), (2, 0.42476794)], [(1, 0.5740261), (2, 0.42411378)], [(1, 0.6012245), (2, 0.39690903)], [(1, 0.5937001), (2, 0.4045146)], [(1, 0.5981157), (2, 0.40004364)], [(1, 0.17528313), (2, 0.8227848)], [(2, 0.9969282)], [(2, 0.9970027)], [(2, 0.9970318)], [(2, 0.9968577)], [(2, 0.99694353)], [(2, 0.9970643)], [(2, 0.99701875)], [(2, 0.9968052)], [(2, 0.9965362)], [(2, 0.9966729)], [(2, 0.99674016)], [(2, 0.99690753)], [(2, 0.99668115)], [(2, 0.9969022)], [(2, 0.9967265)], [(2, 0.9962888)], [(2, 0.9958073)], [(2, 0.99608666)], [(2, 0.9959253)], [(2, 0.994706)], [(2, 0.9945236)], [(2, 0.99423736)], [(2, 0.9944296)], [(2, 0.9947651)], [(2, 0.99491)], [(2, 0.9952785)], [(2, 0.994785)], [(2, 0.99513566)], [(2, 0.99563146)], [(2, 0.99539465)], [(2, 0.99497205)], [(0, 0.99772125)], [(2, 0.9945956)], [(2, 0.9945854)], [(0, 0.9873947)], [(0, 0.7209191), (2, 0.26478058), (3, 0.013498775)], [(0, 0.99738824)], [(0, 0.95808196), (2, 0.039433107)], [(0, 0.9499737), (2, 0.047589812)], [(0, 0.9335518), (2, 0.06410575)], [(0, 0.996818)], [(0, 0.9964103)], [(0, 0.9965532)], [(0, 0.9966293)], [(0, 0.99661416)], [(0, 0.9966451)], [(0, 0.99656904)], [(0, 0.9966601)], [(0, 0.9967856)], [(0, 0.9972975)], [(0, 0.9967945)], [(0, 0.99616003)], [(0, 0.9952062)], [(0, 0.96141493), (2, 0.035715517)], [(0, 0.9955897)], [(0, 0.9950738)], [(0, 0.9951092)], [(0, 0.9956069)], [(0, 0.995584)], [(0, 0.9950087)], [(0, 0.9954063)], [(0, 0.9950566)], [(0, 0.99511164)], [(0, 0.9949772)], [(0, 0.9951583)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the 'topics' list into a DataFrame\n",
        "topics_df = pd.DataFrame(topics, columns=['Topic_0', 'Topic_1', 'Topic_2'])\n",
        "\n",
        "# Combine 'dataset' with 'topics_df'\n",
        "combined_df = pd.concat([dataset['Date'], topics_df], axis=1)\n",
        "\n",
        "# Plot the time trends of the topics\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_0'], label='Topic 0')\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_1'], label='Topic 1')\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_2'], label='Topic 2')\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_3'], label='Topic 3')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Topic Proportions')\n",
        "plt.title('Topic Trends Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H-yXEw-RZ3NY",
        "outputId": "a086f2b2-e7ca-4354-f7a0-b1f01b3acb6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;31m# caller's responsibility to check for this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m             raise AssertionError(\n\u001b[0m\u001b[1;32m   1018\u001b[0m                 \u001b[0;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: 4 columns passed, passed data had 3 columns",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f39e0f45ccc0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert the 'topics' list into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtopics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Topic_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Combine 'dataset' with 'topics_df'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    747\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m     \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_finalize_columns_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;31m# GH#26429 do not raise user-facing AssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 4 columns passed, passed data had 3 columns"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize words and adjust the window size\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 10), stop_words='english')\n",
        "X = vectorizer.fit_transform(concat_statements)\n",
        "co_occurrence_matrix = X.T * X\n",
        "\n",
        "# Convert the co-occurrence matrix to a DataFrame\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix.toarray(), index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Select the co-occurrence frequencies for the specified word\n",
        "target_word = 'risks'\n",
        "relevant_words = co_occurrence_df.loc[target_word]\n",
        "\n",
        "# Print the co-occurrence frequencies of words related to the specified word\n",
        "print(relevant_words)"
      ],
      "metadata": {
        "id": "iHo0wwO8Nfag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_excel(file_path)\n",
        "# after 2019\n",
        "# Concatenate all text values in 'FOMC Statement' into a single value\n",
        "concat_statements = ' '.join(dataset['FOMC Statement'])"
      ],
      "metadata": {
        "id": "l9fkAY8w5mnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a word cloud image(for 2020~)\n",
        "wordcloud = WordCloud(width=700, height=400, background_color='white', colormap='viridis', stopwords=['and', 'the', 'to', 'of', 'in', 'for', 'a', 'Federal', 'Reserve', 'Committee', 'that', 'at', 'percent', 'williams', 'Vice', 'Voting', 'W Bowman', 'C Wiliams', 'Michelle', 'Jerome', 'H Powell', 'Bowman', 'W', 'H', 'Loretta', 'J', 'Last Update', 'Powell', 'Chair', 'Patrick', 'Harker', 'Randal', 'K', 'Lael', 'Brainard', 'has', 'on', 'as', 'have', 'month', 'per','its', 'Last', 'Update', 'Neel', 'Kashkari', 'target', 'range', 'funds', 'rate']).generate(concat_statements)\n",
        "\n",
        "# Generate a word cloud image(before 2020~)\n",
        "# wordcloud = WordCloud(width=700, height=400, background_color='white', colormap='viridis', stopwords=['and', 'the', 'to', 'of', 'in', 'for', 'a', 'Federal', 'Reserve', 'Committee', 'that', 'at', 'percent', 'williams', 'Vice', 'Voting', 'W Bowman', 'C Wiliams', 'Michelle', 'Jerome', 'H Powell', 'Bowman', 'W', 'H', 'Loretta', 'J', 'Last Update', 'Powell', 'Chair', 'Patrick', 'Harker', 'Randal', 'K', 'Lael', 'Brainard', 'has', 'on', 'as', 'have', 'month', 'per','its', 'Last', 'Update', 'Neel', 'Kashkari', 'funds', 'rate', 'Bernanke', 'S', 'Janet', 'L', 'Chairman', 'target', 'range', 'willaim']).generate(concat_statements)\n",
        "\n",
        "# Display the generated word cloud image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BrTwCKrc9wm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "!!! Web crawling of FOMC Statements"
      ],
      "metadata": {
        "id": "cw9QBHn4Q3FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Fedtools\n",
        "\n",
        "from FedTools import MonetaryPolicyCommittee\n",
        "from FedTools import BeigeBooks\n",
        "from FedTools import FederalReserveMins"
      ],
      "metadata": {
        "id": "ZLkUi-AOjMrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monetary_policy = MonetaryPolicyCommittee(\n",
        "            main_url = 'https://www.federalreserve.gov',\n",
        "            calendar_url = 'https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm',\n",
        "            start_year = 2000,\n",
        "            historical_split = 2014,\n",
        "            verbose = True,\n",
        "            thread_num = 10)"
      ],
      "metadata": {
        "id": "_cgNrWFJuYbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = monetary_policy.find_statements()\n",
        "dataset.reset_index(inplace=True)\n",
        "dataset.columns = ['date', 'statement']"
      ],
      "metadata": {
        "id": "xDW-Zfjor97Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from FedTools import BeigeBooks\n",
        "\n",
        "# dataset = BeigeBooks().find_beige_books()\n",
        "# BeigeBooks().pickle_data(\"directory.pkl\")"
      ],
      "metadata": {
        "id": "SE5oIrRQrX8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FedTools import FederalReserveMins\n",
        "\n",
        "dataset_minutes = FederalReserveMins().find_minutes()\n",
        "dataset_minutes.reset_index(inplace=True)\n",
        "dataset_minutes.columns = ['date', 'statement']"
      ],
      "metadata": {
        "id": "jJ9-4mL5rg6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "client_id = \"0MhoFcb7LR_ehhiyYObe\" # 개발자센터에서 발급받은 Client ID 값\n",
        "client_secret = \"e0nXtPkgPV\" # 개발자센터에서 발급받은 Client Secret 값\n",
        "encText = urllib.parse.quote(\"반갑습니다\")\n",
        "data = \"source=ko&target=en&text=\" + encText\n",
        "url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
        "request = urllib.request.Request(url)\n",
        "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "response = urllib.request.urlopen(request, data=data.encode(\"utf-8\"))\n",
        "rescode = response.getcode()\n",
        "if(rescode==200):\n",
        "    response_body = response.read()\n",
        "    print(response_body.decode('utf-8'))\n",
        "else:\n",
        "    print(\"Error Code:\" + rescode)"
      ],
      "metadata": {
        "id": "q6Ec-DQ48IAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}