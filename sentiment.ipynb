{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezu98/colab/blob/main/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install vaderSentiment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from openpyxl import Workbook, load_workbook\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "import datetime as dt\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "YFNlv189OpWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "import textstat\n",
        "from textstat import flesch_kincaid_grade, gunning_fog"
      ],
      "metadata": {
        "id": "IX6CrBx5nZn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "mzNfTBgJnoNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'results2.xlsx'"
      ],
      "metadata": {
        "id": "b9RjbLiokLs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Intensity Analyzer\n",
        "# Example text\n",
        "sample_text = \"The Monetary Policy Board of the Bank of Korea decided today to leave the Base Rate unchanged at 3.50% for the intermeeting period. Although inflation is projected to continue its underlying trend of a slowdown, uncertainties regarding the future path of inflation and growth have risen significantly due to a prolongation of restrictive monetary policy stances in major countries and heightened geopolitical risks. In addition, it is forecast that the pace of inflation slowdown will moderate more than previously expected, and it is necessary to monitor household debt growth. The Board, therefore, sees that it is appropriate to maintain its current restrictive policy stance. Regarding the need to raise the Base Rate further, the Board will make a judgement while assessing the changes in domestic and external policy conditions. The currently available information suggests that uncertainties regarding economic and inflationary trends have increased across the global economy, driven by a prolongation of restrictive monetary policy stances in major countries and by the Israel-Hamas conflict. Global economic growth is projected to continue slowing. Inflation in major countries still remains high, though falling gradually, and upside risks have increased due to the rise in global oil prices. In global financial markets, volatility of major price variables has increased with government bond yields rising significantly and with the U.S. dollar strengthening considerably. Looking ahead, the Board sees global economic growth and global financial markets as likely to be affected by the movements of global oil prices and the global inflation slowdown, by monetary policy changes in major countries and their effects, and by developments in the Israel-Hamas conflict. Domestic economic growth has continued to improve at a modest pace owing to the easing of sluggishness in exports, although the recovery in private consumption has been somewhat slow. Labor market conditions have been generally favorable, as both a low unemployment rate and a robust increase in the number of persons employed have continued. Going forward, domestic economic growth is expected to improve gradually with the easing of the sluggishness in exports. GDP growth for the year is expected to be generally consistent with the August forecast of 1.4%. However, uncertainties surrounding the economic outlook are judged to be elevated, affected by heightened geopolitical risks and by the prolongation of restrictive monetary policy stances in major countries. Consumer price inflation has risen from August to 3.7% in September, due to the increase in the price of energy and of agricultural products. However, both core inflation (excluding changes in food and energy prices from the CPI) and short-term inflation expectations among the general public have stayed at 3.3% in September, the same as in August. Looking ahead, it is forecast that consumer price inflation will fall to the lower-3% range at the end of this year and will continue to gradually moderate in 2024. However, upside risks to inflation have increased due to the effects of higher global oil prices and exchange rates, and due to the Israel-Hamas conflict. Accordingly, it is judged that the timing of consumer price inflation converging on the target level is more likely to be delayed than previously expected. Meanwhile, core inflation is also projected to maintain its underlying slowing trend, owing to the weakening of demand-side pressures. However, the pace of the slowdown is likely to be more modest than previously forecast due to the continuing spillover effects of accumulated cost pressure. In financial and foreign exchange markets, volatility has increased as the U.S. Federal Reserve has signaled a prolongation of a high policy rate and as geopolitical risks have expanded. Long-term Korean Treasury bond yields and the Korean won to U.S. dollar exchange rate have risen significantly and stock prices have fallen. Meanwhile, the risks to some non-bank financial sectors have eased. Housing prices have continued their upward trend, especially in Seoul and its surrounding areas. Household loans have continued to increase, mainly driven by housing-related loans. The Board will continue to conduct monetary policy in order to stabilize consumer price inflation at the target level over the medium-term horizon as it monitors economic growth, while paying attention to financial stability. While domestic economic growth is forecast to gradually improve, uncertainties surrounding the policy decision have also risen. The Board, therefore, will maintain a restrictive policy stance for a considerable time with an emphasis on ensuring price stability, while making a judgement regarding the need to raise the Base Rate further. In this process, the Board will thoroughly assess the inflation slowdown, financial stability risks, economic downside risks, monetary policy changes in major countries, household debt growth, and developments in geopolitical risks.\"\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "# Perform sentiment analysis\n",
        "scores = analyzer.polarity_scores(sample_text)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "XEjqfmw1Doi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Analyze\n",
        "\n",
        "### by loughran and mcdonald(2019)\n",
        "\n",
        "## loading master dictionary\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0].upper()  # Convert word to uppercase\n",
        "            negative_value = int(cols[7])\n",
        "            positive_value = int(cols[8])\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = {'negative': negative_value, 'positive': positive_value, 'uncertainty': uncertainty_value}\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary"
      ],
      "metadata": {
        "id": "1WI8u6caYHKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## evaluate sentiment indices\n",
        "\n",
        "def evaluate_sentiment_indices(text, master_dictionary):\n",
        "    words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    total_words_count, negative_score, positive_score, uncertain_words_count = 0, 0, 0, 0\n",
        "    negative_words_count, positive_words_count = 0, 0\n",
        "\n",
        "    for word in words_uppercase:\n",
        "        if word in master_dictionary:\n",
        "            total_words_count += 1  # Increment the total word count\n",
        "            if master_dictionary[word]['uncertainty'] != 0:\n",
        "                uncertain_words_count += 1\n",
        "            if master_dictionary[word]['negative'] > 0:\n",
        "                negative_words_count += 1\n",
        "            if master_dictionary[word]['positive'] > 0:\n",
        "                positive_words_count += 1\n",
        "\n",
        "    negative_index = negative_words_count / total_words_count  # Calculate the negative index\n",
        "    positive_index = positive_words_count / total_words_count  # Calculate the positive index\n",
        "    uncertainty_index = uncertain_words_count / total_words_count  # Calculate the uncertainty index\n",
        "\n",
        "    return negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "#    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)"
      ],
      "metadata": {
        "id": "kr1JHw5_XkgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Financial Stability Dictionary\n",
        "### by Correa, Ricardo & Keshav Garud & Juan M. Londono & Nathan Mislang, 2021. \"Sentiment in Central Banks' Financial Stability Reports,\" Review of Finance, vol. 25(1), pages 85-120.\n",
        "\n",
        "# Load the workbook\n",
        "file_path_fs = 'f.xlsx'\n",
        "workbook_fs = load_workbook(file_path_fs)\n",
        "worksheet_fs = workbook_fs['1 FS Dictionary']\n",
        "\n",
        "# Extract the words and their respective positive, negative, and assign values\n",
        "fs_words = []\n",
        "fs_positives = []\n",
        "fs_negatives = []\n",
        "fs_assigns = []\n",
        "\n",
        "for row in worksheet_fs.iter_rows(min_row=2, min_col=1, max_col=4):\n",
        "    if row[0].value:\n",
        "        fs_words.append(row[0].value)\n",
        "        fs_positives.append(row[1].value)\n",
        "        fs_negatives.append(row[2].value)\n",
        "        fs_assigns.append(row[3].value)\n",
        "\n",
        "# Define the function to count the words in the fs dictionary in the original text and calculate the sum of positive and negative values\n",
        "def count_fs_words(text, fs_words, fs_positives, fs_negatives):\n",
        "    text_words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    fs_positive_words = [word for word in text_words if word in fs_words and fs_positives[fs_words.index(word)] == 1]\n",
        "    fs_negative_words = [word for word in text_words if word in fs_words and fs_negatives[fs_words.index(word)] == 1]\n",
        "    fs_positive_sum = sum([fs_positives[fs_words.index(word)] for word in text_words if word in fs_words and fs_positives[fs_words.index(word)] == 1])\n",
        "    fs_negative_sum = sum([fs_negatives[fs_words.index(word)] for word in text_words if word in fs_words and fs_negatives[fs_words.index(word)] == 1])\n",
        "    return fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words"
      ],
      "metadata": {
        "id": "IGmv_UpydsUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Readiability\n",
        "\n",
        "def evaluate_readability(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "    num_syllables = 0\n",
        "    for word in words:\n",
        "        num_syllables += textstat.syllable_count(word)\n",
        "    return textstat.flesch_kincaid_grade(text), textstat.gunning_fog(text)"
      ],
      "metadata": {
        "id": "CIjhsXXqpvS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Example FOMC statement to evaluate - manually calculating\n",
        "\n",
        "date = \"2010.10\"\n",
        "\n",
        "fomc_statement = \"\"\"\n",
        "   The Monetary Policy Committee of the Bank of Korea decided today to maintain the Base Rate at its current level (2.25%) for the intermeeting period.\n",
        "\n",
        "\n",
        "\n",
        "In the global economy, emerging market economies have sustained their favorable performance, and the economies of major advanced countries have largely continued their moderate recovery trend, even though the pace of the recovery in the US economy has slowed somewhat. Looking ahead, there exists the possibility of the heightened volatility of economic activity and exchange rates in major countries acting as a risk factor for the global economy.\n",
        "\n",
        "\n",
        "\n",
        "The underlying upward trend of domestic business activity has been maintained. Even though consumption has faltered, exports have sustained their buoyancy and facilities investment has increased. And, led by the private sector, labor market conditions have shown an improving trend.\n",
        "\n",
        "\n",
        "\n",
        "The domestic economy is expected to continue on an underlying upward track, even in the presence of external risk.\n",
        "\n",
        "\n",
        "\n",
        "Consumer price inflation has increased due to a sudden rise in farm product prices, and in the future, upward pressures on the demand side are expected to continue, being associated with the continued upswing in activity. In the real estate market, housing sales prices have continued to decline in Seoul and its surrounding areas, while those in other areas have maintained their increase.\n",
        "\n",
        "\n",
        "In the financial markets, stock prices have risen, the Korean won has appreciated, and market interest rates have declined in response chiefly to the expanded inflow of foreign portfolio investment funds. The scale of the growth in mortgage lending has widened due to the increase in the numbers of newly occupied apartments, even though house transactions have been inactive.\n",
        "\n",
        "\n",
        "\n",
        "Looking ahead, the Committee will conduct monetary policy in such a way as to help the economy maintain price stability, while sustaining sound growth under the accommodative policy stance. In carrying out policy, it will take overall account of financial and economic conditions at home and abroad\n",
        "\n",
        "\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "\n",
        " ## run!!!\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Remove unnecessary line breaks and spaces\n",
        "    processed_text = re.sub(r'\\n+', ' ', fomc_statement)  # Replace multiple line breaks with a single space\n",
        "    processed_text = re.sub(r' +', ' ', processed_text)  # Replace multiple spaces with a single space\n",
        "\n",
        "    negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count = evaluate_sentiment_indices(processed_text, master_dictionary)\n",
        "    flesch_kincaid_grade, gunning_fog_index = evaluate_readability(processed_text)\n",
        "    fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words = count_fs_words(processed_text, fs_words, fs_positives, fs_negatives)\n",
        "\n",
        "    print(f\"\\nNumber of uncertain words: {uncertain_words_count}\")\n",
        "    print(f\"Number of negative words: {negative_words_count}\")\n",
        "    print(f\"Number of positive words: {positive_words_count}\")\n",
        "    print(f\"Number of total words: {total_words_count}\")\n",
        "    print(f\"Negative index for the FOMC statement: {negative_index:.2f}\")\n",
        "    print(f\"Positive index for the FOMC statement: {positive_index:.2f}\")\n",
        "    print(f\"Uncertainty index for the FOMC statement: {uncertainty_index:.2f}\")\n",
        "    print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_grade}\")\n",
        "    print(f\"Gunning Fog Index: {gunning_fog_index}\")\n",
        "    print(f\"FS positive sum: {fs_positive_sum}\")\n",
        "    print(f\"FS negative sum: {fs_negative_sum}\")\n",
        "    print(f\"FS_positive_words: {fs_positive_words}\")\n",
        "    print(f\"FS_negative_words: {fs_negative_words}\")\n",
        "\n",
        "    # Printing the actual words defined as positive, negative, and uncertainty\n",
        "    words = nltk.word_tokenize(processed_text)\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    words_with_sentiment = [(word, master_dictionary[word]) for word in words_uppercase if word in master_dictionary]\n",
        "    positive_words = [word for word, sentiment in words_with_sentiment if sentiment['positive'] > 0]\n",
        "    negative_words = [word for word, sentiment in words_with_sentiment if sentiment['negative'] > 0]\n",
        "    uncertain_words = [word for word, sentiment in words_with_sentiment if sentiment['uncertainty'] > 0]\n",
        "    print(f\"Positive words: {positive_words}\")\n",
        "    print(f\"Negative words: {negative_words}\")\n",
        "    print(f\"Uncertainty words: {uncertain_words}\")\n",
        "\n",
        "\n",
        "# Check if the file already exists, and load it if it does\n",
        "try:\n",
        "    workbook = load_workbook(file_path)\n",
        "    sheet = workbook.active\n",
        "except FileNotFoundError:\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.append(['Date', 'FOMC Statement', 'Positive Words', 'Negative Words', 'Uncertainty Words', 'Total Words', 'Positive Words List', 'Negative Words List', 'Uncertainty Words List', 'Flesch-Kincaid Grade Level', 'Gunning Fog Index', 'FS Positive_Counts', 'FS Negative_Counts', 'FS Positive Words', 'FS Negative Words'])\n",
        "\n",
        "\n",
        "# Append results to the Excel file\n",
        "sheet.append([date, processed_text, positive_words_count, negative_words_count, uncertain_words_count, total_words_count, ', '.join(positive_words), ', '.join(negative_words), ', '.join(uncertain_words), flesch_kincaid_grade, gunning_fog_index, fs_positive_sum, fs_negative_sum, '.join(fs_positive_words), ', '.join(fs_negative_words)'])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(file_path)\n"
      ],
      "metadata": {
        "id": "Q8Q6ub05ZNyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FOMC statement to evaluate - using existing library - cleaning needed. imprecise.\n",
        "\n",
        "# dataset = dataset, dataset_minutes, dataset_speechs\n",
        "# dataset = dataset_statement\n",
        "\n",
        "# Check if the file already exists, and load it if it does\n",
        "try:\n",
        "    workbook = load_workbook(file_path)\n",
        "    sheet = workbook.active\n",
        "except FileNotFoundError:\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.append(['Date', 'FOMC Statement', 'Positive Words', 'Negative Words', 'Uncertainty Words', 'Total Words', 'Positive Words List', 'Negative Words List', 'Uncertainty Words List', 'Flesch-Kincaid Grade Level', 'Gunning Fog Index', 'FS Positive_Counts', 'FS Negative_Counts', 'FS Positive Words', 'FS Negative Words'])\n",
        "\n",
        "# Loop through the dataset and process each row\n",
        "for index, row in dataset.iterrows():\n",
        "    # Retrieve the date and FOMC statement from the current row\n",
        "    date = row['date']  # Assuming the column name for the date is 'date'\n",
        "    fomc_statement = row['statement']  # Assuming the column name for the statement is 'statement'\n",
        "\n",
        "    # Your existing code\n",
        "    start = dt.datetime.now()\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Remove unnecessary line breaks and spaces\n",
        "    processed_text = re.sub(r'\\n+', ' ', fomc_statement)\n",
        "    processed_text = re.sub(r' +', ' ', processed_text)\n",
        "\n",
        "    # Perform sentiment analysis, readability calculation, and other necessary calculations here\n",
        "\n",
        "    negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count = evaluate_sentiment_indices(processed_text, master_dictionary)\n",
        "    flesch_kincaid_grade, gunning_fog_index = evaluate_readability(processed_text)\n",
        "    fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words = count_fs_words(processed_text, fs_words, fs_positives, fs_negatives)\n",
        "\n",
        "    # Printing the actual words defined as positive, negative, and uncertainty\n",
        "    words = nltk.word_tokenize(processed_text)\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    words_with_sentiment = [(word, master_dictionary[word]) for word in words_uppercase if word in master_dictionary]\n",
        "    positive_words = [word for word, sentiment in words_with_sentiment if sentiment['positive'] > 0]\n",
        "    negative_words = [word for word, sentiment in words_with_sentiment if sentiment['negative'] > 0]\n",
        "    uncertain_words = [word for word, sentiment in words_with_sentiment if sentiment['uncertainty'] > 0]\n",
        "\n",
        "    # Append results to the Excel file\n",
        "    sheet.append([date, processed_text, positive_words_count, negative_words_count, uncertain_words_count, total_words_count, ', '.join(positive_words), ', '.join(negative_words), ', '.join(uncertain_words), flesch_kincaid_grade, gunning_fog_index, fs_positive_sum, fs_negative_sum, ', '.join(fs_positive_words), ', '.join(fs_negative_words)])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(file_path)"
      ],
      "metadata": {
        "id": "-wOQIFYCvug1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Elminate stop words(conjunction, article, and preposition)\n",
        "\n",
        "# Function to remove stopwords from a piece of text\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    clean_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(clean_words)"
      ],
      "metadata": {
        "id": "zr0pQQlrkfUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the Excel file\n",
        "dataset_cleaning = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Get the stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Apply the remove_stopwords function to the 'FOMC Statement' column and save the result to 'FOMC Statement_no_stopwords'\n",
        "dataset_cleaning['FOMC Statement_no_stopwords'] = dataset_cleaning['FOMC Statement'].apply(remove_stopwords)\n",
        "\n",
        "# Calculate the number of words\n",
        "dataset_cleaning['words_cleaning'] = dataset_cleaning['FOMC Statement_no_stopwords'].str.split().apply(len)\n",
        "\n",
        "# Calculate the number of occurrences of the word 'will'\n",
        "dataset_cleaning['forward_words_will'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bwill\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'expect'\n",
        "dataset_cleaning['forward_words_expect'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bexpect\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'may'\n",
        "dataset_cleaning['forward_words_may'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bmay\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'might'\n",
        "dataset_cleaning['forward_words_might'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bmight\\b')\n",
        "\n",
        "# Save the modified DataFrame to a new Excel file\n",
        "with pd.ExcelWriter('new_results.xlsx', engine='openpyxl') as writer:\n",
        "    dataset_cleaning.to_excel(writer, index=False, sheet_name='Sheet1')"
      ],
      "metadata": {
        "id": "kGZ6Ar-alKid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Ziff's law and tf-idf score\n",
        "\n",
        "# Load the excel\n",
        "# file_path = 'results2.xlsx'\n",
        "dataset_ziff = pd.read_excel(file_path)\n",
        "# If Uncertainty Words List, it has vacant values. So use the following code.\n",
        "\n",
        "# You may need to adjust the column names based on your dataset\n",
        "dates = dataset_ziff['Date']\n",
        "statements = dataset_ziff['Negative Words List']\n",
        "\n",
        "# Create a dictionary to store words by their respective dates\n",
        "words_by_date = {}\n",
        "\n",
        "for index, statement in enumerate(statements):\n",
        "    # Filter the statements to consider only those after 2019\n",
        "    if dates[index].year < 2020:\n",
        "        continue\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(statement)\n",
        "\n",
        "    # Store the words with their respective date\n",
        "    date = dates[index]\n",
        "    if date not in words_by_date:\n",
        "        words_by_date[date] = []\n",
        "    words_by_date[date].extend(words)\n",
        "\n",
        "# Now words_by_date dictionary contains the words stored by their respective dates after 2019\n",
        "\n",
        "# Convert the words_by_date dictionary into a DataFrame\n",
        "data = {'Date': list(words_by_date.keys()), 'Words': list(words_by_date.values())}\n",
        "dataset_result_ziff = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "dataset_result_ziff.to_excel('result_word.xlsx', index=False)"
      ],
      "metadata": {
        "id": "fLqpDT4ZYN8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Ziff's law\n",
        "\n",
        "# Assuming you have already created the 'df' DataFrame\n",
        "words_list = dataset_result_ziff['Words'].tolist()\n",
        "words_flat = [word for sublist in words_list for word in sublist]\n",
        "\n",
        "# Ziff's Law\n",
        "word_counts = Counter(words_flat)\n",
        "sorted_word_counts = sorted(word_counts.values(), reverse=True)\n",
        "ranks = list(range(1, len(sorted_word_counts) + 1))\n",
        "log_ranks = np.log(ranks)\n",
        "log_counts = np.log(sorted_word_counts)\n",
        "\n",
        "# Plot the scatter plot\n",
        "plt.scatter(log_ranks, log_counts, color='b', alpha=0.5)\n",
        "plt.title(\"Scatter plot for Ziff's Law\")\n",
        "plt.xlabel(\"Log of Ranks\")\n",
        "plt.ylabel(\"Log of Word Frequencies\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WZdfqoLwksW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TF-idf score\n",
        "\n",
        "# TF-IDF Scores\n",
        "text = [' '.join(words) for words in words_list]\n",
        "\n",
        "# Create the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for the TF-IDF scores\n",
        "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=dataset_result_ziff['Date'])\n",
        "\n",
        "# Save the results to Excel files\n",
        "# ziffs_law_df = pd.DataFrame({'Ranks': ranks, 'Log Counts': log_counts, 'Log Ranks': log_ranks})\n",
        "# ziffs_law_df.to_excel('ziffs_law_results.xlsx', index=False)\n",
        "tfidf_df.to_excel('tfidf_scores.xlsx')"
      ],
      "metadata": {
        "id": "y1IYOLfkbdPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Relationship between words\n",
        "\n",
        "# Load the excel\n",
        "dataset_relationship = pd.read_excel(file_path)\n",
        "\n",
        "# Define the word of interest\n",
        "target_word = 'crisis'\n",
        "\n",
        "# Initialize a counter for the occurrences of the target word\n",
        "occurrences_counter = 0\n",
        "\n",
        "# Iterate through the dataset to process the FOMC statements\n",
        "for index, row in dataset_relationship.iterrows():\n",
        "    date = row['Date']\n",
        "    statement = row['FOMC Statement']\n",
        "\n",
        "    # Check if the date falls within your specified time span\n",
        "    if date.year < 2020:\n",
        "        continue\n",
        "\n",
        "    # Tokenize the FOMC statement into sentences\n",
        "    sentences = nltk.sent_tokenize(statement)\n",
        "\n",
        "    # Iterate through each sentence and check for occurrences of the target word\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        if target_word in words:\n",
        "            index_of_word = words.index(target_word)\n",
        "\n",
        "            # Extract the surrounding words\n",
        "            words_around = words[max(0, index_of_word - 20): min(len(words), index_of_word + 25)]\n",
        "            print(f\"Date: {date}, {' '.join(words_around)}\")\n",
        "\n",
        "\n",
        "            # Increment the counter\n",
        "            occurrences_counter += 1\n",
        "\n",
        "# Print the total number of occurrences of the target word\n",
        "print(f\"Total occurrences of '{target_word}': {occurrences_counter}\")"
      ],
      "metadata": {
        "id": "lxUlQjlPwq0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LDA\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "metadata": {
        "id": "nYX6ysVXeqgb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = pd.read_excel(file_path)\n",
        "\n",
        "# Concatenate all text values in 'FOMC Statement' into a list of strings\n",
        "concat_statements = dataset['FOMC Statement'].tolist()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_text = [word_tokenize(statement) for statement in concat_statements]\n",
        "\n",
        "# Prepare the vocabulary\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "roCdntDoUg-6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_word custom\n",
        "custom_stop_words = stop_words.union(['chairman', 'Alan', 'Greenspan', 'Jr', 'Roger', 'Ferguson', 'Chairman', 'billion', 'Chair', 'pace', 'Reserve', 'time'])  # Add 'chairman' to the list of stop words\n",
        "\n",
        "punctuation = set(string.punctuation)\n",
        "texts = [\n",
        "    [word for word in statement if word not in custom_stop_words and word not in punctuation]\n",
        "    for statement in tokenized_text\n",
        "]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Filter out words that occur in less than 1 document or more than 80% of the documents\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
        "\n",
        "# Bag-of-words representation of the documents\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
        "\n",
        "# Get the topics for each document\n",
        "topics = [lda_model[corpus[i]] for i in range(len(corpus))]\n",
        "print(topics)"
      ],
      "metadata": {
        "id": "Enf50RksUJJp",
        "outputId": "18a189f0-bda2-4498-d6a3-77ee0d8609c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.015*\"growth\" + 0.015*\"S.\" + 0.011*\"markets\" + 0.011*\"M.\" + 0.009*\"likely\" + 0.008*\"remain\" + 0.008*\"credit\" + 0.008*\"housing\" + 0.007*\"resource\" + 0.007*\"moderate\"\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.016*\"labor\" + 0.016*\"securities\" + 0.011*\"employment\" + 0.011*\"agency\" + 0.011*\"2\" + 0.009*\"maximum\" + 0.009*\"levels\" + 0.009*\"mandate\" + 0.008*\"'s\" + 0.008*\"holdings\"\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.019*\"2\" + 0.014*\"employment\" + 0.012*\"labor\" + 0.012*\"'s\" + 0.011*\"maximum\" + 0.011*\"appropriate\" + 0.010*\"activity\" + 0.010*\"information\" + 0.009*\"developments\" + 0.009*\"stance\"\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.021*\"securities\" + 0.016*\"Bank\" + 0.015*\"agency\" + 0.011*\"mortgage-backed\" + 0.010*\"Desk\" + 0.010*\"Treasury\" + 0.009*\"markets\" + 0.009*\"directs\" + 0.009*\"operations\" + 0.009*\"continue\"\n",
            "\n",
            "[[(0, 0.99303514)], [(0, 0.9947085)], [(0, 0.99497133)], [(0, 0.99462277)], [(0, 0.9925924)], [(0, 0.9921759)], [(0, 0.99256456)], [(0, 0.9929596)], [(0, 0.9915636)], [(0, 0.99070823)], [(0, 0.99088585)], [(0, 0.9915907)], [(0, 0.99288696)], [(0, 0.99410933)], [(0, 0.99482024)], [(0, 0.9945015)], [(0, 0.99355066)], [(3, 0.9959653)], [(0, 0.99468803)], [(0, 0.9948156)], [(0, 0.99352187)], [(0, 0.99356765)], [(0, 0.99319077)], [(0, 0.9039187), (3, 0.093279995)], [(0, 0.99498427)], [(0, 0.99618816)], [(0, 0.9965469)], [(0, 0.9961936)], [(0, 0.9961087)], [(0, 0.99573463)], [(0, 0.99615633)], [(0, 0.99625945)], [(0, 0.99648565)], [(0, 0.9973442)], [(0, 0.9971425)], [(0, 0.9964464)], [(0, 0.99618906)], [(3, 0.9939625)], [(0, 0.99574757)], [(0, 0.7846773), (1, 0.21289368)], [(0, 0.7209254), (1, 0.27664205)], [(0, 0.4507433), (1, 0.54713166)], [(0, 0.41473073), (1, 0.5831153)], [(0, 0.31778657), (1, 0.6798531)], [(0, 0.20232771), (1, 0.7953262)], [(0, 0.15986264), (1, 0.83780146)], [(0, 0.10900618), (1, 0.88870496)], [(0, 0.09103496), (1, 0.9068961)], [(0, 0.06258731), (1, 0.9355286)], [(1, 0.98882043)], [(1, 0.99632305)], [(1, 0.9960979)], [(1, 0.99631375)], [(1, 0.99639875)], [(1, 0.9968363)], [(0, 0.038803644), (1, 0.9588881)], [(1, 0.9971845)], [(1, 0.99714357)], [(1, 0.9976499)], [(1, 0.9975188)], [(1, 0.9975048)], [(1, 0.9975319)], [(1, 0.9975915)], [(1, 0.997629)], [(1, 0.9979244)], [(1, 0.99788743)], [(1, 0.9980984)], [(1, 0.99806666)], [(1, 0.9981612)], [(1, 0.99802685)], [(1, 0.9980209)], [(1, 0.9980915)], [(1, 0.9982088)], [(1, 0.9977356)], [(1, 0.9977717)], [(1, 0.99732655)], [(1, 0.99730325)], [(1, 0.9973596)], [(1, 0.9972297)], [(1, 0.9972179)], [(1, 0.9973414)], [(1, 0.9972544)], [(1, 0.99711365)], [(1, 0.9660983), (2, 0.031920966)], [(1, 0.99703634)], [(1, 0.9970574)], [(1, 0.99689114)], [(1, 0.9969672)], [(1, 0.9970836)], [(1, 0.9970425)], [(1, 0.9968178)], [(1, 0.99654436)], [(1, 0.96772134), (2, 0.030102566)], [(1, 0.94921815), (2, 0.04864795)], [(1, 0.91269493), (2, 0.0852963)], [(1, 0.88919705), (2, 0.108621225)], [(1, 0.8783373), (2, 0.11961844)], [(1, 0.8581275), (2, 0.13971144)], [(1, 0.674252), (2, 0.32330462)], [(1, 0.50939924), (2, 0.48782828)], [(1, 0.46676666), (2, 0.5306625)], [(1, 0.38620436), (2, 0.6110956)], [(1, 0.13210216), (2, 0.86440396)], [(2, 0.9944819)], [(2, 0.9941919)], [(2, 0.99438405)], [(2, 0.9947619)], [(2, 0.99492306)], [(2, 0.99531037)], [(2, 0.9948112)], [(2, 0.9950979)], [(2, 0.995615)], [(2, 0.99540454)], [(2, 0.9949707)], [(3, 0.9978103)], [(2, 0.9946227)], [(2, 0.9946866)], [(2, 0.9875211)], [(2, 0.8744921), (3, 0.12387727)], [(2, 0.074729), (3, 0.92355067)], [(2, 0.99317265)], [(2, 0.99630207)], [(2, 0.99644905)], [(2, 0.99688053)], [(2, 0.9964586)], [(2, 0.9965951)], [(2, 0.996662)], [(2, 0.9966429)], [(2, 0.9966767)], [(2, 0.9965982)], [(2, 0.9966932)], [(2, 0.99682885)], [(2, 0.9973296)], [(2, 0.99682444)], [(2, 0.99618644)], [(2, 0.99528396)], [(2, 0.9957661)], [(2, 0.9956432)], [(2, 0.9951293)], [(2, 0.9951656)], [(2, 0.9956607)], [(2, 0.9956371)], [(2, 0.9950666)], [(2, 0.9954554)], [(2, 0.99508584)], [(2, 0.9951601)], [(2, 0.9950488)], [(2, 0.99520326)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################\n",
        "### TOPIC MODELING OF FOMC MINUTES ###\n",
        "######################################\n",
        "\n",
        "### IMPORT LIBRARIES ###\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim import models\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "from wordcloud import WordCloud\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp.max_length = 1500000 # In case max_length is set to lower than this (ensure sufficient memory)\n",
        "\n",
        "### GRAB THE DOCUMENTS BY PARSING URLs ###\n",
        "\n",
        "# Define URLs for the specific FOMC minutes\n",
        "URLPath = r'https://www.federalreserve.gov/monetarypolicy/fomcminutes'\n",
        "URLExt = r'.htm'\n",
        "\n",
        "# List for FOMC minutes from 2007 onward\n",
        "MinutesList = ['20071031', '20071211', # 2007 FOMC minutes (part-year on new URL format)\n",
        "           '20080130', '20080318', '20080430', '20080625', '20080805', '20080916', '20081029', '20081216', # 2008 FOMC minutes\n",
        "           '20090128', '20090318', '20090429', '20090624', '20090812', '20090923', '20091104', '20091216', # 2009 FOMC minutes\n",
        "           '20100127', '20100316', '20100428', '20100623', '20100810', '20100921', '20101103', '20101214', # 2010 FOMC minutes\n",
        "           '20110126', '20110315', '20110427', '20110622', '20110809', '20110921', '20111102', '20111213', # 2011 FOMC minutes\n",
        "           '20120125', '20120313', '20120425', '20120620', '20120801', '20120913', '20121024', '20121212', # 2012 FOMC minutes\n",
        "           '20130130', '20133020', '20130501', '20130619', '20130731', '20130918', '20131030', '20131218', # 2013 FOMC minutes\n",
        "           '20140129', '20140319', '20140430', '20140618', '20140730', '20140917', '20141029', '20141217', # 2014 FOMC minutes\n",
        "           '20150128', '20150318', '20150429', '20150617', '20150729', '20150917', '20151028', '20151216', # 2015 FOMC minutes\n",
        "           '20160127', '20160316', '20160427', '20160615', '20160727', '20160921', '20161102', '20161214', # 2016 FOMC minutes\n",
        "           '20172001', '20170315', '20170503', '20170614', '20170726', '20170920', '20171101', '20171213', # 2017 FOMC minutes\n",
        "           '20180131', '20180321', '20180502', '20180613', '20180801', '20180926', '20181108', '20181219', # 2018 FOMC minutes\n",
        "           '20190130', '20190320', '20190501', '20190619', '20190731', '20190918', '20191030', '20191211', # 2019 FOMC minutes\n",
        "           '20200129', '20200315', '20200429', '20200610', '20200729'] # 2020 FOMC minutes\n",
        "\n",
        "### SETTING UP THE CORPUS ###\n",
        "\n",
        "FOMCMinutes = [] # A list of lists to form the corpus\n",
        "FOMCWordCloud = [] # Single list version of the corpus for WordCloud\n",
        "FOMCTopix = [] # List to store minutes ID (date) and weight of each para\n",
        "\n",
        "# Define function to prepare corpus\n",
        "def PrepareCorpus(urlpath, urlext, minslist, minparalength):\n",
        "\n",
        "    fomcmins = []\n",
        "    fomcwordcloud = []\n",
        "    fomctopix = []\n",
        "\n",
        "    for minutes in minslist:\n",
        "\n",
        "        response = requests.get(urlpath + minutes + urlext) # Get the URL response\n",
        "        soup = BeautifulSoup(response.content, 'lxml') # Parse the response\n",
        "\n",
        "        # Extract minutes content and convert to string\n",
        "        minsTxt = str(soup.find(\"div\", {\"id\": \"content\"})) # Contained within the 'div' tag\n",
        "\n",
        "        # Clean text - stage 1\n",
        "        minsTxt = minsTxt.strip()  # Remove white space at the beginning and end\n",
        "        minsTxt = minsTxt.replace('\\r', '') # Replace the \\r with null\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        while '  ' in minsTxt:\n",
        "            minsTxt = minsTxt.replace('  ', ' ') # Remove extra spaces\n",
        "\n",
        "        # Clean text - stage 2, using regex (as SpaCy incorrectly parses certain HTML tags)\n",
        "        minsTxt = re.sub(r'(<[^>]*>)|' # Remove content within HTML tags\n",
        "                         '([_]+)|' # Remove series of underscores\n",
        "                         '(http[^\\s]+)|' # Remove website addresses\n",
        "                         '((a|p)\\.m\\.)', # Remove \"a.m\" and \"p.m.\"\n",
        "                         '', minsTxt) # Replace with null\n",
        "\n",
        "        # Find length of minutes document for calculating paragraph weights\n",
        "        minsTxtParas = minsTxt.split('\\n') # List of paras in minsTxt, where minsTxt is split based on new line characters\n",
        "        cum_paras = 0 # Set up variable for cumulative word-count in all paras for a given minutes transcript\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only including paragraphs larger than 'minparalength'\n",
        "                cum_paras += len(para)\n",
        "\n",
        "        # Extract paragraphs\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only extract paragraphs larger than 'minparalength'\n",
        "\n",
        "                topixTmp = [] # Temporary list to store minutes date & para weight tuple\n",
        "                topixTmp.append(minutes) # First element of tuple (minutes date)\n",
        "                topixTmp.append(len(para)/cum_paras) # Second element of tuple (para weight), NB. Calculating weights based on pre-SpaCy-parsed text\n",
        "\n",
        "                # Parse cleaned para with SpaCy\n",
        "                minsPara = nlp(para)\n",
        "\n",
        "                minsTmp = [] # Temporary list to store individual tokens\n",
        "\n",
        "                # Further cleaning and selection of text characteristics\n",
        "                for token in minsPara:\n",
        "                    if token.is_stop == False and token.is_punct == False and (token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ ==\"VERB\"): # Retain words that are not a stop word nor punctuation, and only if a Noun, Adjective or Verb\n",
        "                        minsTmp.append(token.lemma_.lower()) # Convert to lower case and retain the lemmatized version of the word (this is a string object)\n",
        "                        fomcwordcloud.append(token.lemma_.lower()) # Add word to WordCloud list\n",
        "                fomcmins.append(minsTmp) # Add para to corpus 'list of lists'\n",
        "                fomctopix.append(topixTmp) # Add minutes date & para weight tuple to list for storing\n",
        "\n",
        "    return fomcmins, fomcwordcloud, fomctopix\n",
        "\n",
        "# Prepare corpus\n",
        "FOMCMinutes, FOMCWordCloud, FOMCTopix = PrepareCorpus(urlpath=URLPath, urlext=URLExt, minslist=MinutesList, minparalength=200)\n",
        "\n",
        "# Generate and plot WordCloud for full corpus\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(','.join(FOMCWordCloud)) # NB. 'join' method used to convert the documents list to text format\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "### NUMERIC REPRESENTATION OF CORPUS USING TF-IDF ###\n",
        "\n",
        "# Form dictionary by mapping word IDs to words\n",
        "ID2word = corpora.Dictionary(FOMCMinutes)\n",
        "\n",
        "# Set up Bag of Words and TFIDF\n",
        "corpus = [ID2word.doc2bow(doc) for doc in FOMCMinutes] # Apply Bag of Words to all documents in corpus\n",
        "TFIDF = models.TfidfModel(corpus) # Fit TF-IDF model\n",
        "trans_TFIDF = TFIDF[corpus] # Apply TF-IDF model\n",
        "\n",
        "### SET UP LDA MODEL ###\n",
        "\n",
        "SEED = 130 # Set random seed\n",
        "NUM_topics = 8 # Set number of topics\n",
        "ALPHA = 0.15 # Set alpha\n",
        "ETA = 1.25 # Set eta\n",
        "\n",
        "# Train LDA model using the corpus\n",
        "lda_model = gensim.models.LdaMulticore(corpus=trans_TFIDF, num_topics=NUM_topics, id2word=ID2word, random_state=SEED, alpha=ALPHA, eta=ETA, passes=100)\n",
        "\n",
        "# Print topics generated from the training corpus\n",
        "pprint(lda_model.print_topics(num_words=10))\n",
        "\n",
        "### CALCULATE COHERENCE SCORE ###\n",
        "\n",
        "# Set up coherence model\n",
        "coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=FOMCMinutes, dictionary=ID2word, coherence='c_v')\n",
        "\n",
        "# Calculate and print coherence\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('-'*50)\n",
        "print('\\nCoherence Score:', coherence_lda)\n",
        "print('-'*50)\n",
        "\n",
        "### GENERATE WEIGHTED TOPIC PROPORTIONS FOR CORPUS ###\n",
        "\n",
        "para_no = 0 # Set document counter\n",
        "for para in FOMCTopix:\n",
        "    TFIDF_para = TFIDF[corpus[para_no]] # Apply TFIDF model to individual minutes documents\n",
        "    # Generate and store weighted topic proportions for each para\n",
        "    for topic_weight in lda_model.get_document_topics(TFIDF_para): # List of tuples (\"topic number\", \"topic proportion\") for each para, where 'topic_weight' is the (iterating) tuple\n",
        "        FOMCTopix[para_no].append(FOMCTopix[para_no][1]*topic_weight[1]) # Weights are the second element of the pre-appended list, topic proportions are the second element of each tuple\n",
        "    para_no += 1\n",
        "\n",
        "### GENERATE AGGREGATE TOPIC MIX OVER EACH MINUTES TRANSCRIPT ###\n",
        "\n",
        "# Form dataframe of weighted topic proportions (paragraphs) - include any chosen topic names\n",
        "FOMCTopixDF = pd.DataFrame(FOMCTopix, columns=['Date', 'Weight', 'Inflation', 'Topic 2', 'Consumption', 'Topic 4', 'Market', 'Topic 6', 'Topic 7', 'Policy'])\n",
        "\n",
        "# Aggregate topic mix by minutes documents (weighted sum of paragraphs)\n",
        "TopixAggDF = pd.pivot_table(FOMCTopixDF, values=['Inflation', 'Topic 2', 'Consumption', 'Topic 4', 'Market', 'Topic 6', 'Topic 7', 'Policy'], index='Date', aggfunc=np.sum)\n",
        "\n",
        "# Plot results - select which topics to print\n",
        "TopixAggDF.plot(y=['Inflation', 'Consumption', 'Market', 'Policy'], kind='line', use_index=True)\n",
        "\n",
        "### PRINT TOPIC WORD CLOUDS ###\n",
        "\n",
        "topic = 0 # Initialize counter\n",
        "while topic < NUM_topics:\n",
        "    # Get topics and frequencies and store in a dictionary structure\n",
        "    topic_words_freq = dict(lda_model.show_topic(topic, topn=50)) # NB. the 'dict()' constructor builds dictionaries from sequences (lists) of key-value pairs - this is needed as input for the 'generate_from_frequencies' word cloud function\n",
        "    topic += 1\n",
        "\n",
        "    # Generate Word Cloud for topic using frequencies\n",
        "    wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(topic_words_freq)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "### TEST COHERENCE BY VARYING KEY PARAMETERS ###\n",
        "\n",
        "# Coherence values for varying eta\n",
        "def compute_coherence_values_ETA(corpus, dictionary, num_topics, seed, alpha, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for eta in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=seed, alpha=alpha, eta=eta/100, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_ETA(corpus=trans_TFIDF, dictionary=ID2word, num_topics=NUM_topics, seed=SEED, alpha=ALPHA, texts=FOMCMinutes, start=115, limit=175, step=5)\n",
        "\n",
        "# Plot graph of coherence values by varying eta\n",
        "limit=175; start=115; step=5;\n",
        "#x = range(start, limit, step)\n",
        "x_axis = []\n",
        "for x in range(start, limit, step):\n",
        "    x_axis.append(x/100)\n",
        "plt.plot(x_axis, coherence_values)\n",
        "plt.xlabel(\"Eta\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Coherence values for varying seed\n",
        "def compute_coherence_values_SEED(corpus, dictionary, alpha, num_topics, eta, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for seed in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, alpha=alpha, num_topics=num_topics, eta=eta, random_state=seed, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_SEED(corpus=trans_TFIDF, dictionary=ID2word, alpha=ALPHA, num_topics=NUM_topics, eta=ETA, texts=FOMCMinutes, start=60, limit=165, step=5)\n",
        "\n",
        "# Plot graph of coherence values by varying seed\n",
        "limit=165; start=60; step=5;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Random Seed\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Coherence values for varying alpha\n",
        "def compute_coherence_values_ALPHA(corpus, dictionary, num_topics, seed, eta, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for alpha in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=seed, eta=eta, alpha=alpha/20, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_ALPHA(dictionary=ID2word, corpus=trans_TFIDF, num_topics=NUM_topics, seed=SEED, eta=ETA, texts=FOMCMinutes, start=1, limit=20, step=1)\n",
        "\n",
        "# Plot graph of coherence values by varying alpha\n",
        "limit=20; start=1; step=1;\n",
        "x_axis = []\n",
        "for x in range(start, limit, step):\n",
        "    x_axis.append(x/20)\n",
        "plt.plot(x_axis, coherence_values)\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Coherence values for varying number of topics\n",
        "def compute_coherence_values_TOPICS(corpus, dictionary, alpha, seed, eta, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, alpha=alpha, num_topics=num_topics, random_state=seed, eta=eta, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_TOPICS(corpus=trans_TFIDF, dictionary=ID2word, alpha=ALPHA, seed=SEED, eta=ETA, texts=FOMCMinutes, start=2, limit=11, step=1)\n",
        "\n",
        "# Plot graph of coherence values by varying number of topics\n",
        "limit=11; start=2; step=1;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lNRDXsWlY6ve",
        "outputId": "593c703e-8c09-499b-947d-dd0476cb8346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e83d00bd360b>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Prepare corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mFOMCMinutes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFOMCWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFOMCTopix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrepareCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mURLPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mURLExt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminslist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMinutesList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminparalength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Generate and plot WordCloud for full corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-e83d00bd360b>\u001b[0m in \u001b[0;36mPrepareCorpus\u001b[0;34m(urlpath, urlext, minslist, minparalength)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mminutes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminslist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminutes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murlext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get the URL response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Parse the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize words and adjust the window size\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 10), stop_words='english')\n",
        "X = vectorizer.fit_transform(concat_statements)\n",
        "co_occurrence_matrix = X.T * X\n",
        "\n",
        "# Convert the co-occurrence matrix to a DataFrame\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix.toarray(), index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Select the co-occurrence frequencies for the specified word\n",
        "target_word = 'risks'\n",
        "relevant_words = co_occurrence_df.loc[target_word]\n",
        "\n",
        "# Print the co-occurrence frequencies of words related to the specified word\n",
        "print(relevant_words)"
      ],
      "metadata": {
        "id": "iHo0wwO8Nfag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_excel(file_path)\n",
        "# after 2019\n",
        "# Concatenate all text values in 'FOMC Statement' into a single value\n",
        "concat_statements = ' '.join(dataset['FOMC Statement'])"
      ],
      "metadata": {
        "id": "l9fkAY8w5mnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a word cloud image(for 2020~)\n",
        "wordcloud = WordCloud(width=700, height=400, background_color='white', colormap='viridis', stopwords=['and', 'the', 'to', 'of', 'in', 'for', 'a', 'Federal', 'Reserve', 'Committee', 'that', 'at', 'percent', 'williams', 'Vice', 'Voting', 'W Bowman', 'C Wiliams', 'Michelle', 'Jerome', 'H Powell', 'Bowman', 'W', 'H', 'Loretta', 'J', 'Last Update', 'Powell', 'Chair', 'Patrick', 'Harker', 'Randal', 'K', 'Lael', 'Brainard', 'has', 'on', 'as', 'have', 'month', 'per','its', 'Last', 'Update', 'Neel', 'Kashkari', 'target', 'range', 'funds', 'rate']).generate(concat_statements)\n",
        "\n",
        "# Generate a word cloud image(before 2020~)\n",
        "# wordcloud = WordCloud(width=700, height=400, background_color='white', colormap='viridis', stopwords=['and', 'the', 'to', 'of', 'in', 'for', 'a', 'Federal', 'Reserve', 'Committee', 'that', 'at', 'percent', 'williams', 'Vice', 'Voting', 'W Bowman', 'C Wiliams', 'Michelle', 'Jerome', 'H Powell', 'Bowman', 'W', 'H', 'Loretta', 'J', 'Last Update', 'Powell', 'Chair', 'Patrick', 'Harker', 'Randal', 'K', 'Lael', 'Brainard', 'has', 'on', 'as', 'have', 'month', 'per','its', 'Last', 'Update', 'Neel', 'Kashkari', 'funds', 'rate', 'Bernanke', 'S', 'Janet', 'L', 'Chairman', 'target', 'range', 'willaim']).generate(concat_statements)\n",
        "\n",
        "# Display the generated word cloud image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BrTwCKrc9wm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "!!! Web crawling of FOMC Statements"
      ],
      "metadata": {
        "id": "cw9QBHn4Q3FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Fedtools\n",
        "\n",
        "from FedTools import MonetaryPolicyCommittee\n",
        "from FedTools import BeigeBooks\n",
        "from FedTools import FederalReserveMins"
      ],
      "metadata": {
        "id": "ZLkUi-AOjMrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monetary_policy = MonetaryPolicyCommittee(\n",
        "            main_url = 'https://www.federalreserve.gov',\n",
        "            calendar_url = 'https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm',\n",
        "            start_year = 2000,\n",
        "            historical_split = 2014,\n",
        "            verbose = True,\n",
        "            thread_num = 10)"
      ],
      "metadata": {
        "id": "_cgNrWFJuYbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = monetary_policy.find_statements()\n",
        "dataset.reset_index(inplace=True)\n",
        "dataset.columns = ['date', 'statement']"
      ],
      "metadata": {
        "id": "xDW-Zfjor97Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from FedTools import BeigeBooks\n",
        "\n",
        "# dataset = BeigeBooks().find_beige_books()\n",
        "# BeigeBooks().pickle_data(\"directory.pkl\")"
      ],
      "metadata": {
        "id": "SE5oIrRQrX8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FedTools import FederalReserveMins\n",
        "\n",
        "dataset_minutes = FederalReserveMins().find_minutes()\n",
        "dataset_minutes.reset_index(inplace=True)\n",
        "dataset_minutes.columns = ['date', 'statement']"
      ],
      "metadata": {
        "id": "jJ9-4mL5rg6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "client_id = \"0MhoFcb7LR_ehhiyYObe\" #   Client ID \n",
        "client_secret = \"e0nXtPkgPV\" #   Client Secret \n",
        "encText = urllib.parse.quote(\"\")\n",
        "data = \"source=ko&target=en&text=\" + encText\n",
        "url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
        "request = urllib.request.Request(url)\n",
        "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "response = urllib.request.urlopen(request, data=data.encode(\"utf-8\"))\n",
        "rescode = response.getcode()\n",
        "if(rescode==200):\n",
        "    response_body = response.read()\n",
        "    print(response_body.decode('utf-8'))\n",
        "else:\n",
        "    print(\"Error Code:\" + rescode)"
      ],
      "metadata": {
        "id": "q6Ec-DQ48IAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colaboratory   ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}