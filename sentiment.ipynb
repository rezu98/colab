{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezu98/colab/blob/main/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install vaderSentiment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from openpyxl import Workbook, load_workbook\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "import datetime as dt\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "YFNlv189OpWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "import textstat\n",
        "from textstat import flesch_kincaid_grade, gunning_fog"
      ],
      "metadata": {
        "id": "IX6CrBx5nZn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "mzNfTBgJnoNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'results2.xlsx'"
      ],
      "metadata": {
        "id": "b9RjbLiokLs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Intensity Analyzer\n",
        "# Example text\n",
        "sample_text = \"The Monetary Policy Board of the Bank of Korea decided today to leave the Base Rate unchanged at 3.50% for the intermeeting period. Although inflation is projected to continue its underlying trend of a slowdown, uncertainties regarding the future path of inflation and growth have risen significantly due to a prolongation of restrictive monetary policy stances in major countries and heightened geopolitical risks. In addition, it is forecast that the pace of inflation slowdown will moderate more than previously expected, and it is necessary to monitor household debt growth. The Board, therefore, sees that it is appropriate to maintain its current restrictive policy stance. Regarding the need to raise the Base Rate further, the Board will make a judgement while assessing the changes in domestic and external policy conditions. The currently available information suggests that uncertainties regarding economic and inflationary trends have increased across the global economy, driven by a prolongation of restrictive monetary policy stances in major countries and by the Israel-Hamas conflict. Global economic growth is projected to continue slowing. Inflation in major countries still remains high, though falling gradually, and upside risks have increased due to the rise in global oil prices. In global financial markets, volatility of major price variables has increased with government bond yields rising significantly and with the U.S. dollar strengthening considerably. Looking ahead, the Board sees global economic growth and global financial markets as likely to be affected by the movements of global oil prices and the global inflation slowdown, by monetary policy changes in major countries and their effects, and by developments in the Israel-Hamas conflict. Domestic economic growth has continued to improve at a modest pace owing to the easing of sluggishness in exports, although the recovery in private consumption has been somewhat slow. Labor market conditions have been generally favorable, as both a low unemployment rate and a robust increase in the number of persons employed have continued. Going forward, domestic economic growth is expected to improve gradually with the easing of the sluggishness in exports. GDP growth for the year is expected to be generally consistent with the August forecast of 1.4%. However, uncertainties surrounding the economic outlook are judged to be elevated, affected by heightened geopolitical risks and by the prolongation of restrictive monetary policy stances in major countries. Consumer price inflation has risen from August to 3.7% in September, due to the increase in the price of energy and of agricultural products. However, both core inflation (excluding changes in food and energy prices from the CPI) and short-term inflation expectations among the general public have stayed at 3.3% in September, the same as in August. Looking ahead, it is forecast that consumer price inflation will fall to the lower-3% range at the end of this year and will continue to gradually moderate in 2024. However, upside risks to inflation have increased due to the effects of higher global oil prices and exchange rates, and due to the Israel-Hamas conflict. Accordingly, it is judged that the timing of consumer price inflation converging on the target level is more likely to be delayed than previously expected. Meanwhile, core inflation is also projected to maintain its underlying slowing trend, owing to the weakening of demand-side pressures. However, the pace of the slowdown is likely to be more modest than previously forecast due to the continuing spillover effects of accumulated cost pressure. In financial and foreign exchange markets, volatility has increased as the U.S. Federal Reserve has signaled a prolongation of a high policy rate and as geopolitical risks have expanded. Long-term Korean Treasury bond yields and the Korean won to U.S. dollar exchange rate have risen significantly and stock prices have fallen. Meanwhile, the risks to some non-bank financial sectors have eased. Housing prices have continued their upward trend, especially in Seoul and its surrounding areas. Household loans have continued to increase, mainly driven by housing-related loans. The Board will continue to conduct monetary policy in order to stabilize consumer price inflation at the target level over the medium-term horizon as it monitors economic growth, while paying attention to financial stability. While domestic economic growth is forecast to gradually improve, uncertainties surrounding the policy decision have also risen. The Board, therefore, will maintain a restrictive policy stance for a considerable time with an emphasis on ensuring price stability, while making a judgement regarding the need to raise the Base Rate further. In this process, the Board will thoroughly assess the inflation slowdown, financial stability risks, economic downside risks, monetary policy changes in major countries, household debt growth, and developments in geopolitical risks.\"\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "# Perform sentiment analysis\n",
        "scores = analyzer.polarity_scores(sample_text)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "XEjqfmw1Doi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Analyze\n",
        "\n",
        "### by loughran and mcdonald(2019)\n",
        "\n",
        "## loading master dictionary\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0].upper()  # Convert word to uppercase\n",
        "            negative_value = int(cols[7])\n",
        "            positive_value = int(cols[8])\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = {'negative': negative_value, 'positive': positive_value, 'uncertainty': uncertainty_value}\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary"
      ],
      "metadata": {
        "id": "1WI8u6caYHKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## evaluate sentiment indices\n",
        "\n",
        "def evaluate_sentiment_indices(text, master_dictionary):\n",
        "    words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    total_words_count, negative_score, positive_score, uncertain_words_count = 0, 0, 0, 0\n",
        "    negative_words_count, positive_words_count = 0, 0\n",
        "\n",
        "    for word in words_uppercase:\n",
        "        if word in master_dictionary:\n",
        "            total_words_count += 1  # Increment the total word count\n",
        "            if master_dictionary[word]['uncertainty'] != 0:\n",
        "                uncertain_words_count += 1\n",
        "            if master_dictionary[word]['negative'] > 0:\n",
        "                negative_words_count += 1\n",
        "            if master_dictionary[word]['positive'] > 0:\n",
        "                positive_words_count += 1\n",
        "\n",
        "    negative_index = negative_words_count / total_words_count  # Calculate the negative index\n",
        "    positive_index = positive_words_count / total_words_count  # Calculate the positive index\n",
        "    uncertainty_index = uncertain_words_count / total_words_count  # Calculate the uncertainty index\n",
        "\n",
        "    return negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "#    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)"
      ],
      "metadata": {
        "id": "kr1JHw5_XkgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Financial Stability Dictionary\n",
        "### by Correa, Ricardo & Keshav Garud & Juan M. Londono & Nathan Mislang, 2021. \"Sentiment in Central Banks' Financial Stability Reports,\" Review of Finance, vol. 25(1), pages 85-120.\n",
        "\n",
        "# Load the workbook\n",
        "file_path_fs = 'f.xlsx'\n",
        "workbook_fs = load_workbook(file_path_fs)\n",
        "worksheet_fs = workbook_fs['1 FS Dictionary']\n",
        "\n",
        "# Extract the words and their respective positive, negative, and assign values\n",
        "fs_words = []\n",
        "fs_positives = []\n",
        "fs_negatives = []\n",
        "fs_assigns = []\n",
        "\n",
        "for row in worksheet_fs.iter_rows(min_row=2, min_col=1, max_col=4):\n",
        "    if row[0].value:\n",
        "        fs_words.append(row[0].value)\n",
        "        fs_positives.append(row[1].value)\n",
        "        fs_negatives.append(row[2].value)\n",
        "        fs_assigns.append(row[3].value)\n",
        "\n",
        "# Define the function to count the words in the fs dictionary in the original text and calculate the sum of positive and negative values\n",
        "def count_fs_words(text, fs_words, fs_positives, fs_negatives):\n",
        "    text_words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    fs_positive_words = [word for word in text_words if word in fs_words and fs_positives[fs_words.index(word)] == 1]\n",
        "    fs_negative_words = [word for word in text_words if word in fs_words and fs_negatives[fs_words.index(word)] == 1]\n",
        "    fs_positive_sum = sum([fs_positives[fs_words.index(word)] for word in text_words if word in fs_words and fs_positives[fs_words.index(word)] == 1])\n",
        "    fs_negative_sum = sum([fs_negatives[fs_words.index(word)] for word in text_words if word in fs_words and fs_negatives[fs_words.index(word)] == 1])\n",
        "    return fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words"
      ],
      "metadata": {
        "id": "IGmv_UpydsUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Readiability\n",
        "\n",
        "def evaluate_readability(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "    num_syllables = 0\n",
        "    for word in words:\n",
        "        num_syllables += textstat.syllable_count(word)\n",
        "    return textstat.flesch_kincaid_grade(text), textstat.gunning_fog(text)"
      ],
      "metadata": {
        "id": "CIjhsXXqpvS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Example FOMC statement to evaluate - manually calculating\n",
        "\n",
        "date = \"2023.3\"\n",
        "\n",
        "fomc_statement = \"\"\"\n",
        "\n",
        "Good afternoon, the Vice-President and I welcome you to our press conference.\n",
        "\n",
        "Inflation continues to decline but is still expected to remain too high for too long. We are determined to ensure that inflation returns to our two per cent medium-term target in a timely manner. In order to reinforce progress towards our target, the Governing Council today decided to raise the three key ECB interest rates by 25 basis points.\n",
        "\n",
        "The rate increase today reflects our assessment of the inflation outlook in light of the incoming economic and financial data, the dynamics of underlying inflation, and the strength of monetary policy transmission. The September ECB staff macroeconomic projections for the euro area see average inflation at 5.6 per cent in 2023, 3.2 per cent in 2024 and 2.1 per cent in 2025. This is an upward revision for 2023 and 2024 and a downward revision for 2025. The upward revision for 2023 and 2024 mainly reflects a higher path for energy prices. Underlying price pressures remain high, even though most indicators have started to ease. ECB staff have slightly revised down the projected path for inflation excluding energy and food, to an average of 5.1 per cent in 2023, 2.9 per cent in 2024 and 2.2 per cent in 2025. Our past interest rate increases continue to be transmitted forcefully. Financing conditions have tightened further and are increasingly dampening demand, which is an important factor in bringing inflation back to target. With the increasing impact of our tightening on domestic demand and the weakening international trade environment, ECB staff have lowered their economic growth projections significantly. They now expect the euro area economy to expand by 0.7 per cent in 2023, 1.0 per cent in 2024 and 1.5 per cent in 2025.\n",
        "\n",
        "Based on our current assessment, we consider that the key ECB interest rates have reached levels that, maintained for a sufficiently long duration, will make a substantial contribution to the timely return of inflation to our target. Our future decisions will ensure that the key ECB interest rates will be set at sufficiently restrictive levels for as long as necessary. We will continue to follow a data-dependent approach to determining the appropriate level and duration of restriction. In particular, our interest rate decisions will be based on our assessment of the inflation outlook in light of the incoming economic and financial data, the dynamics of underlying inflation, and the strength of monetary policy transmission.\n",
        "\n",
        "The decisions taken today are set out in a press release available on our website.\n",
        "\n",
        "I will now outline in more detail how we see the economy and inflation developing and will then explain our assessment of financial and monetary conditions.\n",
        "\n",
        "Economic activity\n",
        "The economy is likely to remain subdued in the coming months. It broadly stagnated over the first half of the year, and recent indicators suggest it has also been weak in the third quarter. Lower demand for the euro area’s exports and the impact of tight financing conditions are dampening growth, including through lower residential and business investment. The services sector, which had so far been resilient, is now also weakening. Over time, economic momentum should pick up, as real incomes are expected to rise, supported by falling inflation, rising wages and a strong labour market, and this will underpin consumer spending.\n",
        "\n",
        "The labour market has so far remained resilient despite the slowing economy. The unemployment rate stayed at its historical low of 6.4 per cent in July. While employment grew by 0.2 per cent in the second quarter, momentum is slowing. The services sector, which has been a major driver of employment growth since mid-2022, is now also creating fewer jobs.\n",
        "\n",
        "As the energy crisis fades, governments should continue to roll back the related support measures. This is essential to avoid driving up medium-term inflationary pressures, which would otherwise call for an even stronger monetary policy response. Fiscal policies should be designed to make our economy more productive and to gradually bring down high public debt. Policies to enhance the euro area’s supply capacity – which would be supported by the full implementation of the Next Generation EU programme – can help reduce price pressures in the medium term, while supporting the green transition. The reform of the EU’s economic governance framework should be concluded before the end of this year and progress towards Capital Markets Union should be accelerated.\n",
        "\n",
        "Inflation\n",
        "Inflation declined to 5.3 per cent in July but remained at that level in August, according to Eurostat’s flash estimate. Its decline was interrupted because energy prices rose compared with July. Food price inflation has come down from its peak in March but was still almost 10 per cent in August. In the coming months, the sharp price increases recorded in the autumn of 2022 will drop out of the yearly rates, thus pulling inflation down.\n",
        "\n",
        "Inflation excluding energy and food fell to 5.3 per cent in August, from 5.5 per cent in July. Goods inflation declined to 4.8 per cent in August, from 5.0 per cent in July and 5.5 per cent in June, owing to better supply conditions, previous drops in energy prices, easing price pressures in the earlier stages of the production chain and weaker demand. Services inflation edged down to 5.5 per cent but was still kept up by strong spending on holidays and travel and by the high growth of wages. The annual growth rate of compensation per employee remained constant at 5.5 per cent in the second quarter of the year. The contribution of labour costs to annual domestic inflation increased in the second quarter, in part owing to weaker productivity, while the contribution of profits fell for the first time since early 2022.\n",
        "\n",
        "Most measures of underlying inflation are starting to fall as demand and supply have become more aligned and the contribution of past energy price increases is fading out. At the same time, domestic price pressures remain strong.\n",
        "\n",
        "Most measures of longer-term inflation expectations currently stand at around 2 per cent. But some indicators have increased and need to be monitored closely.\n",
        "\n",
        "Risk assessment\n",
        "The risks to economic growth are tilted to the downside. Growth could be slower if the effects of monetary policy are more forceful than expected, or if the world economy weakens, for instance owing to a further slowdown in China. Conversely, growth could be higher than projected if the strong labour market, rising real incomes and receding uncertainty mean that people and businesses become more confident and spend more.\n",
        "\n",
        "Upside risks to inflation include potential renewed upward pressures on the costs of energy and food. Adverse weather conditions, and the unfolding climate crisis more broadly, could push food prices up by more than expected. A lasting rise in inflation expectations above our target, or higher than anticipated increases in wages or profit margins, could also drive inflation higher, including over the medium term. By contrast, weaker demand – for example due to a stronger transmission of monetary policy or a worsening of the economic environment outside the euro area – would lead to lower price pressures, especially over the medium term.\n",
        "\n",
        "Financial and monetary conditions\n",
        "Our monetary policy tightening continues to be transmitted strongly to broader financing conditions. Funding has again become more expensive for banks, as savers are replacing overnight deposits with time deposits that pay more interest and the ECB’s targeted longer-term refinancing operations are being phased out. Average lending rates for business loans and mortgages continued to increase in July, to 4.9 per cent and 3.8 per cent respectively.\n",
        "\n",
        "Credit dynamics have weakened further. Loans to firms grew at an annual rate of 2.2 per cent in July, down from 3.0 per cent in June. Loans to households also grew less strongly, by 1.3 per cent, after 1.7 per cent in June. In annualised terms based on the last three months of data, household loans declined by 0.8 per cent, which is the strongest contraction since the start of the euro. Amid weak lending and the reduction in the Eurosystem balance sheet, the annual growth rate of M3 fell from 0.6 per cent in June to an all-time low of -0.4 per cent in July. In annualised terms over the past three months, M3 contracted by 1.5 per cent.\n",
        "\n",
        "Conclusion\n",
        "Inflation continues to decline but is still expected to remain too high for too long. We are determined to ensure that inflation returns to our two per cent medium-term target in a timely manner. In order to reinforce progress towards our target, the Governing Council today decided to raise the three key ECB interest rates by 25 basis points. Based on our current assessment, we consider that the key ECB interest rates have reached levels that, maintained for a sufficiently long duration, will make a substantial contribution to the timely return of inflation to our target. Our future decisions will ensure that the key ECB interest rates will be set at sufficiently restrictive levels for as long as necessary. We will continue to follow a data-dependent approach to determining the appropriate level and duration of restriction.\n",
        "\n",
        "In any case, we stand ready to adjust all of our instruments within our mandate to ensure that inflation returns to our medium-term target and to preserve the smooth functioning of monetary policy transmission.\n",
        "\n",
        "We are now ready to take your questions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "\n",
        " ## run!!!\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Remove unnecessary line breaks and spaces\n",
        "    processed_text = re.sub(r'\\n+', ' ', fomc_statement)  # Replace multiple line breaks with a single space\n",
        "    processed_text = re.sub(r' +', ' ', processed_text)  # Replace multiple spaces with a single space\n",
        "\n",
        "    negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count = evaluate_sentiment_indices(processed_text, master_dictionary)\n",
        "    flesch_kincaid_grade, gunning_fog_index = evaluate_readability(processed_text)\n",
        "    fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words = count_fs_words(processed_text, fs_words, fs_positives, fs_negatives)\n",
        "\n",
        "    print(f\"\\nNumber of uncertain words: {uncertain_words_count}\")\n",
        "    print(f\"Number of negative words: {negative_words_count}\")\n",
        "    print(f\"Number of positive words: {positive_words_count}\")\n",
        "    print(f\"Number of total words: {total_words_count}\")\n",
        "    print(f\"Negative index for the FOMC statement: {negative_index:.2f}\")\n",
        "    print(f\"Positive index for the FOMC statement: {positive_index:.2f}\")\n",
        "    print(f\"Uncertainty index for the FOMC statement: {uncertainty_index:.2f}\")\n",
        "    print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_grade}\")\n",
        "    print(f\"Gunning Fog Index: {gunning_fog_index}\")\n",
        "    print(f\"FS positive sum: {fs_positive_sum}\")\n",
        "    print(f\"FS negative sum: {fs_negative_sum}\")\n",
        "    print(f\"FS_positive_words: {fs_positive_words}\")\n",
        "    print(f\"FS_negative_words: {fs_negative_words}\")\n",
        "\n",
        "    # Printing the actual words defined as positive, negative, and uncertainty\n",
        "    words = nltk.word_tokenize(processed_text)\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    words_with_sentiment = [(word, master_dictionary[word]) for word in words_uppercase if word in master_dictionary]\n",
        "    positive_words = [word for word, sentiment in words_with_sentiment if sentiment['positive'] > 0]\n",
        "    negative_words = [word for word, sentiment in words_with_sentiment if sentiment['negative'] > 0]\n",
        "    uncertain_words = [word for word, sentiment in words_with_sentiment if sentiment['uncertainty'] > 0]\n",
        "    print(f\"Positive words: {positive_words}\")\n",
        "    print(f\"Negative words: {negative_words}\")\n",
        "    print(f\"Uncertainty words: {uncertain_words}\")\n",
        "\n",
        "\n",
        "# Check if the file already exists, and load it if it does\n",
        "try:\n",
        "    workbook = load_workbook(file_path)\n",
        "    sheet = workbook.active\n",
        "except FileNotFoundError:\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.append(['Date', 'FOMC Statement', 'Positive Words', 'Negative Words', 'Uncertainty Words', 'Total Words', 'Positive Words List', 'Negative Words List', 'Uncertainty Words List', 'Flesch-Kincaid Grade Level', 'Gunning Fog Index', 'FS Positive_Counts', 'FS Negative_Counts', 'FS Positive Words', 'FS Negative Words'])\n",
        "\n",
        "\n",
        "# Append results to the Excel file\n",
        "sheet.append([date, processed_text, positive_words_count, negative_words_count, uncertain_words_count, total_words_count, ', '.join(positive_words), ', '.join(negative_words), ', '.join(uncertain_words), flesch_kincaid_grade, gunning_fog_index, fs_positive_sum, fs_negative_sum, '.join(fs_positive_words), ', '.join(fs_negative_words)'])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(file_path)\n"
      ],
      "metadata": {
        "id": "Q8Q6ub05ZNyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FOMC statement to evaluate - using existing library - cleaning needed. imprecise.\n",
        "\n",
        "# dataset = dataset, dataset_minutes, dataset_speechs\n",
        "# dataset = dataset_statement\n",
        "\n",
        "# Check if the file already exists, and load it if it does\n",
        "try:\n",
        "    workbook = load_workbook(file_path)\n",
        "    sheet = workbook.active\n",
        "except FileNotFoundError:\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.append(['Date', 'FOMC Statement', 'Positive Words', 'Negative Words', 'Uncertainty Words', 'Total Words', 'Positive Words List', 'Negative Words List', 'Uncertainty Words List', 'Flesch-Kincaid Grade Level', 'Gunning Fog Index', 'FS Positive_Counts', 'FS Negative_Counts', 'FS Positive Words', 'FS Negative Words'])\n",
        "\n",
        "# Loop through the dataset and process each row\n",
        "for index, row in dataset.iterrows():\n",
        "    # Retrieve the date and FOMC statement from the current row\n",
        "    date = row['date']  # Assuming the column name for the date is 'date'\n",
        "    fomc_statement = row['statement']  # Assuming the column name for the statement is 'statement'\n",
        "\n",
        "    # Your existing code\n",
        "    start = dt.datetime.now()\n",
        "    md = r'c.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Remove unnecessary line breaks and spaces\n",
        "    processed_text = re.sub(r'\\n+', ' ', fomc_statement)\n",
        "    processed_text = re.sub(r' +', ' ', processed_text)\n",
        "\n",
        "    # Perform sentiment analysis, readability calculation, and other necessary calculations here\n",
        "\n",
        "    negative_index, positive_index, uncertainty_index, total_words_count, uncertain_words_count, negative_words_count, positive_words_count = evaluate_sentiment_indices(processed_text, master_dictionary)\n",
        "    flesch_kincaid_grade, gunning_fog_index = evaluate_readability(processed_text)\n",
        "    fs_positive_sum, fs_negative_sum, fs_positive_words, fs_negative_words = count_fs_words(processed_text, fs_words, fs_positives, fs_negatives)\n",
        "\n",
        "    # Printing the actual words defined as positive, negative, and uncertainty\n",
        "    words = nltk.word_tokenize(processed_text)\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    words_with_sentiment = [(word, master_dictionary[word]) for word in words_uppercase if word in master_dictionary]\n",
        "    positive_words = [word for word, sentiment in words_with_sentiment if sentiment['positive'] > 0]\n",
        "    negative_words = [word for word, sentiment in words_with_sentiment if sentiment['negative'] > 0]\n",
        "    uncertain_words = [word for word, sentiment in words_with_sentiment if sentiment['uncertainty'] > 0]\n",
        "\n",
        "    # Append results to the Excel file\n",
        "    sheet.append([date, processed_text, positive_words_count, negative_words_count, uncertain_words_count, total_words_count, ', '.join(positive_words), ', '.join(negative_words), ', '.join(uncertain_words), flesch_kincaid_grade, gunning_fog_index, fs_positive_sum, fs_negative_sum, ', '.join(fs_positive_words), ', '.join(fs_negative_words)])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(file_path)"
      ],
      "metadata": {
        "id": "-wOQIFYCvug1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Elminate stop words(conjunction, article, and preposition)\n",
        "\n",
        "# Function to remove stopwords from a piece of text\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    clean_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(clean_words)"
      ],
      "metadata": {
        "id": "zr0pQQlrkfUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the Excel file\n",
        "dataset_cleaning = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Get the stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Apply the remove_stopwords function to the 'FOMC Statement' column and save the result to 'FOMC Statement_no_stopwords'\n",
        "dataset_cleaning['FOMC Statement_no_stopwords'] = dataset_cleaning['FOMC Statement'].apply(remove_stopwords)\n",
        "\n",
        "# Calculate the number of words\n",
        "dataset_cleaning['words_cleaning'] = dataset_cleaning['FOMC Statement_no_stopwords'].str.split().apply(len)\n",
        "\n",
        "# Calculate the number of occurrences of the word 'will'\n",
        "dataset_cleaning['forward_words_will'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bwill\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'expect'\n",
        "dataset_cleaning['forward_words_expect'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bexpect\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'may'\n",
        "dataset_cleaning['forward_words_may'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bmay\\b')\n",
        "\n",
        "# Calculate the number of occurrences of the word 'might'\n",
        "dataset_cleaning['forward_words_might'] = dataset_cleaning['FOMC Statement'].str.count(r'\\bmight\\b')\n",
        "\n",
        "# Save the modified DataFrame to a new Excel file\n",
        "with pd.ExcelWriter('new_results.xlsx', engine='openpyxl') as writer:\n",
        "    dataset_cleaning.to_excel(writer, index=False, sheet_name='Sheet1')"
      ],
      "metadata": {
        "id": "kGZ6Ar-alKid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Ziff's law and tf-idf score\n",
        "\n",
        "# Load the excel\n",
        "# file_path = 'results2.xlsx'\n",
        "dataset_ziff = pd.read_excel(file_path)\n",
        "# If Uncertainty Words List, it has vacant values. So use the following code.\n",
        "\n",
        "# You may need to adjust the column names based on your dataset\n",
        "dates = dataset_ziff['Date']\n",
        "statements = dataset_ziff['Negative Words List']\n",
        "\n",
        "# Create a dictionary to store words by their respective dates\n",
        "words_by_date = {}\n",
        "\n",
        "for index, statement in enumerate(statements):\n",
        "    # Filter the statements to consider only those after 2019\n",
        "    if dates[index].year < 2020:\n",
        "        continue\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(statement)\n",
        "\n",
        "    # Store the words with their respective date\n",
        "    date = dates[index]\n",
        "    if date not in words_by_date:\n",
        "        words_by_date[date] = []\n",
        "    words_by_date[date].extend(words)\n",
        "\n",
        "# Now words_by_date dictionary contains the words stored by their respective dates after 2019\n",
        "\n",
        "# Convert the words_by_date dictionary into a DataFrame\n",
        "data = {'Date': list(words_by_date.keys()), 'Words': list(words_by_date.values())}\n",
        "dataset_result_ziff = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "dataset_result_ziff.to_excel('result_word.xlsx', index=False)"
      ],
      "metadata": {
        "id": "fLqpDT4ZYN8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Ziff's law\n",
        "\n",
        "# Assuming you have already created the 'df' DataFrame\n",
        "words_list = dataset_result_ziff['Words'].tolist()\n",
        "words_flat = [word for sublist in words_list for word in sublist]\n",
        "\n",
        "# Ziff's Law\n",
        "word_counts = Counter(words_flat)\n",
        "sorted_word_counts = sorted(word_counts.values(), reverse=True)\n",
        "ranks = list(range(1, len(sorted_word_counts) + 1))\n",
        "log_ranks = np.log(ranks)\n",
        "log_counts = np.log(sorted_word_counts)\n",
        "\n",
        "# Plot the scatter plot\n",
        "plt.scatter(log_ranks, log_counts, color='b', alpha=0.5)\n",
        "plt.title(\"Scatter plot for Ziff's Law\")\n",
        "plt.xlabel(\"Log of Ranks\")\n",
        "plt.ylabel(\"Log of Word Frequencies\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WZdfqoLwksW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TF-idf score\n",
        "\n",
        "# TF-IDF Scores\n",
        "text = [' '.join(words) for words in words_list]\n",
        "\n",
        "# Create the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for the TF-IDF scores\n",
        "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=dataset_result_ziff['Date'])\n",
        "\n",
        "# Save the results to Excel files\n",
        "# ziffs_law_df = pd.DataFrame({'Ranks': ranks, 'Log Counts': log_counts, 'Log Ranks': log_ranks})\n",
        "# ziffs_law_df.to_excel('ziffs_law_results.xlsx', index=False)\n",
        "tfidf_df.to_excel('tfidf_scores.xlsx')"
      ],
      "metadata": {
        "id": "y1IYOLfkbdPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Relationship between words\n",
        "\n",
        "# Load the excel\n",
        "dataset_relationship = pd.read_excel(file_path)\n",
        "\n",
        "# Define the word of interest\n",
        "target_word = 'data-dependent'\n",
        "\n",
        "# Initialize a counter for the occurrences of the target word\n",
        "occurrences_counter = 0\n",
        "\n",
        "# Iterate through the dataset to process the FOMC statements\n",
        "for index, row in dataset_relationship.iterrows():\n",
        "    date = row['Date']\n",
        "    statement = row['FOMC Statement']\n",
        "\n",
        "    # Check if the date falls within your specified time span\n",
        "    if date.year < 2020:\n",
        "        continue\n",
        "\n",
        "    # Tokenize the FOMC statement into sentences\n",
        "    sentences = nltk.sent_tokenize(statement)\n",
        "\n",
        "    # Iterate through each sentence and check for occurrences of the target word\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        if target_word in words:\n",
        "            index_of_word = words.index(target_word)\n",
        "\n",
        "            # Extract the surrounding words\n",
        "            words_around = words[max(0, index_of_word - 20): min(len(words), index_of_word + 25)]\n",
        "            print(f\"Date: {date}, {' '.join(words_around)}\")\n",
        "\n",
        "\n",
        "            # Increment the counter\n",
        "            occurrences_counter += 1\n",
        "\n",
        "# Print the total number of occurrences of the target word\n",
        "print(f\"Total occurrences of '{target_word}': {occurrences_counter}\")"
      ],
      "metadata": {
        "id": "lxUlQjlPwq0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA"
      ],
      "metadata": {
        "id": "cnnbNp_UqNqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### LDA\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "metadata": {
        "id": "nYX6ysVXeqgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = pd.read_excel(file_path)\n",
        "\n",
        "# Concatenate all text values in 'FOMC Statement' into a list of strings\n",
        "concat_statements = dataset['FOMC Statement'].tolist()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_text = [word_tokenize(statement) for statement in concat_statements]\n",
        "\n",
        "# Prepare the vocabulary\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "roCdntDoUg-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_word custom\n",
        "custom_stop_words = stop_words.union(['chairman', 'Alan', 'Greenspan', 'Jr', 'Roger', 'Ferguson', 'Chairman', 'billion', 'Chair', 'pace', 'Reserve', 'time'])  # Add 'chairman' to the list of stop words\n",
        "\n",
        "punctuation = set(string.punctuation)\n",
        "texts = [\n",
        "    [word for word in statement if word not in custom_stop_words and word not in punctuation]\n",
        "    for statement in tokenized_text\n",
        "]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Filter out words that occur in less than 1 document or more than 80% of the documents\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
        "\n",
        "# Bag-of-words representation of the documents\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
        "\n",
        "# Get the topics for each document\n",
        "topics = [lda_model[corpus[i]] for i in range(len(corpus))]\n",
        "print(topics)"
      ],
      "metadata": {
        "id": "Enf50RksUJJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the 'topics' list into a DataFrame\n",
        "topics_df = pd.DataFrame(topics, columns=['Topic_0', 'Topic_1', 'Topic_2'])\n",
        "\n",
        "# Combine 'dataset' with 'topics_df'\n",
        "combined_df = pd.concat([dataset['Date'], topics_df], axis=1)\n",
        "\n",
        "# Plot the time trends of the topics\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_0'], label='Topic 0')\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_1'], label='Topic 1')\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_2'], label='Topic 2')\n",
        "plt.plot(combined_df['Date'], combined_df['Topic_3'], label='Topic 3')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Topic Proportions')\n",
        "plt.title('Topic Trends Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H-yXEw-RZ3NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize words and adjust the window size\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 10), stop_words='english')\n",
        "X = vectorizer.fit_transform(concat_statements)\n",
        "co_occurrence_matrix = X.T * X\n",
        "\n",
        "# Convert the co-occurrence matrix to a DataFrame\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix.toarray(), index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Select the co-occurrence frequencies for the specified word\n",
        "target_word = 'risks'\n",
        "relevant_words = co_occurrence_df.loc[target_word]\n",
        "\n",
        "# Print the co-occurrence frequencies of words related to the specified word\n",
        "print(relevant_words)"
      ],
      "metadata": {
        "id": "iHo0wwO8Nfag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_excel(file_path)\n",
        "# after 2019\n",
        "# Concatenate all text values in 'FOMC Statement' into a single value\n",
        "concat_statements = ' '.join(dataset['FOMC Statement'])"
      ],
      "metadata": {
        "id": "l9fkAY8w5mnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a word cloud image(for 2020~)\n",
        "wordcloud = WordCloud(width=700, height=400, background_color='white', colormap='viridis', stopwords=['and', 'the', 'to', 'of', 'in', 'for', 'a', 'Federal', 'Reserve', 'Committee', 'that', 'at', 'percent', 'williams', 'Vice', 'Voting', 'W Bowman', 'C Wiliams', 'Michelle', 'Jerome', 'H Powell', 'Bowman', 'W', 'H', 'Loretta', 'J', 'Last Update', 'Powell', 'Chair', 'Patrick', 'Harker', 'Randal', 'K', 'Lael', 'Brainard', 'has', 'on', 'as', 'have', 'month', 'per','its', 'Last', 'Update', 'Neel', 'Kashkari', 'target', 'range', 'funds', 'rate']).generate(concat_statements)\n",
        "\n",
        "# Generate a word cloud image(before 2020~)\n",
        "# wordcloud = WordCloud(width=700, height=400, background_color='white', colormap='viridis', stopwords=['and', 'the', 'to', 'of', 'in', 'for', 'a', 'Federal', 'Reserve', 'Committee', 'that', 'at', 'percent', 'williams', 'Vice', 'Voting', 'W Bowman', 'C Wiliams', 'Michelle', 'Jerome', 'H Powell', 'Bowman', 'W', 'H', 'Loretta', 'J', 'Last Update', 'Powell', 'Chair', 'Patrick', 'Harker', 'Randal', 'K', 'Lael', 'Brainard', 'has', 'on', 'as', 'have', 'month', 'per','its', 'Last', 'Update', 'Neel', 'Kashkari', 'funds', 'rate', 'Bernanke', 'S', 'Janet', 'L', 'Chairman', 'target', 'range', 'willaim']).generate(concat_statements)\n",
        "\n",
        "# Display the generated word cloud image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BrTwCKrc9wm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "!!! Web crawling of FOMC Statements"
      ],
      "metadata": {
        "id": "cw9QBHn4Q3FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Fedtools\n",
        "\n",
        "from FedTools import MonetaryPolicyCommittee\n",
        "from FedTools import BeigeBooks\n",
        "from FedTools import FederalReserveMins"
      ],
      "metadata": {
        "id": "ZLkUi-AOjMrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monetary_policy = MonetaryPolicyCommittee(\n",
        "            main_url = 'https://www.federalreserve.gov',\n",
        "            calendar_url = 'https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm',\n",
        "            start_year = 2000,\n",
        "            historical_split = 2014,\n",
        "            verbose = True,\n",
        "            thread_num = 10)"
      ],
      "metadata": {
        "id": "_cgNrWFJuYbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = monetary_policy.find_statements()\n",
        "dataset.reset_index(inplace=True)\n",
        "dataset.columns = ['date', 'statement']"
      ],
      "metadata": {
        "id": "xDW-Zfjor97Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from FedTools import BeigeBooks\n",
        "\n",
        "# dataset = BeigeBooks().find_beige_books()\n",
        "# BeigeBooks().pickle_data(\"directory.pkl\")"
      ],
      "metadata": {
        "id": "SE5oIrRQrX8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FedTools import FederalReserveMins\n",
        "\n",
        "dataset_minutes = FederalReserveMins().find_minutes()\n",
        "dataset_minutes.reset_index(inplace=True)\n",
        "dataset_minutes.columns = ['date', 'statement']"
      ],
      "metadata": {
        "id": "jJ9-4mL5rg6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "client_id = \"0MhoFcb7LR_ehhiyYObe\" # 개발자센터에서 발급받은 Client ID 값\n",
        "client_secret = \"e0nXtPkgPV\" # 개발자센터에서 발급받은 Client Secret 값\n",
        "encText = urllib.parse.quote(\"반갑습니다\")\n",
        "data = \"source=ko&target=en&text=\" + encText\n",
        "url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
        "request = urllib.request.Request(url)\n",
        "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "response = urllib.request.urlopen(request, data=data.encode(\"utf-8\"))\n",
        "rescode = response.getcode()\n",
        "if(rescode==200):\n",
        "    response_body = response.read()\n",
        "    print(response_body.decode('utf-8'))\n",
        "else:\n",
        "    print(\"Error Code:\" + rescode)"
      ],
      "metadata": {
        "id": "q6Ec-DQ48IAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}