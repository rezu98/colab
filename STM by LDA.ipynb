{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHDh4+BwPqY8sIwOLtlnD+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezu98/colab/blob/main/STM%20by%20LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "o-h0mLzCBexS",
        "outputId": "7cf9e80c-42a9-48f4-c72b-aeec7d8a26ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.3)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.8.7)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "######################################\n",
        "### TOPIC MODELING OF FOMC MINUTES ###\n",
        "######################################\n",
        "\n",
        "### IMPORT LIBRARIES ###\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim import models\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "from wordcloud import WordCloud\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "!pip install pyLDAvis\n",
        "!pip install matplotlib\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp.max_length = 1500000 # In case max_length is set to lower than this (ensure sufficient memory)"
      ],
      "metadata": {
        "id": "XAcjKBFNJMi7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Define Dictionary by loughran and mcdonald(2019)\n",
        "\n",
        "## loading master dictionary\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0].upper()  # Convert word to uppercase\n",
        "            negative_value = int(cols[7])\n",
        "            positive_value = int(cols[8])\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = {'negative': negative_value, 'positive': positive_value, 'uncertainty': uncertainty_value}\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary"
      ],
      "metadata": {
        "id": "yLiS1O5Vx6G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRAB THE DOCUMENTS BY PARSING URLs ###\n",
        "\n",
        "# Define URLs for the specific FOMC minutes\n",
        "URLPath = r'https://www.federalreserve.gov/monetarypolicy/fomcminutes'\n",
        "URLExt = r'.htm'\n",
        "\n",
        "# List for FOMC minutes from 2007 onward\n",
        "MinutesList = ['20071031', '20071211', # 2007 FOMC minutes (part-year on new URL format)\n",
        "           '20080130', '20080318', '20080430', '20080625', '20080805', '20080916', '20081029', '20081216', # 2008 FOMC minutes\n",
        "           '20090128', '20090318', '20090429', '20090624', '20090812', '20090923', '20091104', '20091216', # 2009 FOMC minutes\n",
        "           '20100127', '20100316', '20100428', '20100623', '20100810', '20100921', '20101103', '20101214', # 2010 FOMC minutes\n",
        "           '20110126', '20110315', '20110427', '20110622', '20110809', '20110921', '20111102', '20111213', # 2011 FOMC minutes\n",
        "           '20120125', '20120313', '20120425', '20120620', '20120801', '20120913', '20121024', '20121212', # 2012 FOMC minutes\n",
        "           '20130130', '20133020', '20130501', '20130619', '20130731', '20130918', '20131030', '20131218', # 2013 FOMC minutes\n",
        "           '20140129', '20140319', '20140430', '20140618', '20140730', '20140917', '20141029', '20141217', # 2014 FOMC minutes\n",
        "           '20150128', '20150318', '20150429', '20150617', '20150729', '20150917', '20151028', '20151216', # 2015 FOMC minutes\n",
        "           '20160127', '20160316', '20160427', '20160615', '20160727', '20160921', '20161102', '20161214', # 2016 FOMC minutes\n",
        "           '20172001', '20170315', '20170503', '20170614', '20170726', '20170920', '20171101', '20171213', # 2017 FOMC minutes\n",
        "           '20180131', '20180321', '20180502', '20180613', '20180801', '20180926', '20181108', '20181219', # 2018 FOMC minutes\n",
        "           '20190130', '20190320', '20190501', '20190619', '20190731', '20190918', '20191030', '20191211', # 2019 FOMC minutes\n",
        "           '20200129', '20200315', '20200429', '20200610', '20200729'] # 2020 FOMC minutes\n"
      ],
      "metadata": {
        "id": "acHbqjltJU7J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define URLs for the specific FOMC statements\n",
        "URLPath = r'https://www.federalreserve.gov/newsevents/pressreleases/monetary'\n",
        "URLExt = r'.htm'\n",
        "\n",
        "# List for FOMC statements\n",
        "MinutesList = ['20230920a', '20230726', '20230614a', '20230503a', '20230322a', '20230201a', '20221214a', '20221102a', '20220921a', '20220727a', '20220615a', '20220504a', '20220316a', '20220126a',\n",
        "               '20211215a', '20211103a', '20210922a', '20210728a', '20210616a', '20210428a', '20210317a', '20210127a',\n",
        "               '20201216a']"
      ],
      "metadata": {
        "id": "NQEoRPKHTbkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SETTING UP THE CORPUS_FOMC Minutes ###\n",
        "\n",
        "FOMCMinutes = [] # A list of lists to form the corpus\n",
        "FOMCWordCloud = [] # Single list version of the corpus for WordCloud\n",
        "FOMCTopix = [] # List to store minutes ID (date) and weight of each para\n",
        "\n",
        "# Define your custom stopwords - 내 버전, for FOMC Statement\n",
        "custom_stopwords = [\"vote\", \"meeting\", \"rate\", \"unanimous\", \"hold\", \"office\", \"successor\", \"schedule\", \"report\", \"system\\x92s\", \"information\", \"agency\", \"quarter\", \"second\", \"year\", \"half\", \"ratio\"\n",
        ", \"intermeeting\", \"governor\"]  # Add your own stopwords to this list\n",
        "\n",
        "# Define function to prepare corpus\n",
        "def PrepareCorpus(urlpath, urlext, minslist, minparalength):\n",
        "\n",
        "    fomcmins = []\n",
        "    fomcwordcloud = []\n",
        "    fomctopix = []\n",
        "\n",
        "    for minutes in minslist:\n",
        "\n",
        "        response = requests.get(urlpath + minutes + urlext) # Get the URL response\n",
        "        soup = BeautifulSoup(response.content, 'lxml') # Parse the response\n",
        "\n",
        "        # Extract minutes content and convert to string\n",
        "        minsTxt = str(soup.find(\"div\", {\"id\": \"content\"})) # Contained within the 'div' tag\n",
        "\n",
        "        # Clean text - stage 1\n",
        "        minsTxt = minsTxt.strip()  # Remove white space at the beginning and end\n",
        "        minsTxt = minsTxt.replace('\\r', '') # Replace the \\r with null\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        while '  ' in minsTxt:\n",
        "            minsTxt = minsTxt.replace('  ', ' ') # Remove extra spaces\n",
        "\n",
        "        # Clean text - stage 2, using regex (as SpaCy incorrectly parses certain HTML tags)\n",
        "        minsTxt = re.sub(r'(<[^>]*>)|' # Remove content within HTML tags\n",
        "                         '([_]+)|' # Remove series of underscores\n",
        "                         '(http[^\\s]+)|' # Remove website addresses\n",
        "                         '((a|p)\\.m\\.)', # Remove \"a.m\" and \"p.m.\"\n",
        "                         '', minsTxt) # Replace with null\n",
        "\n",
        "        # Find length of minutes document for calculating paragraph weights\n",
        "        minsTxtParas = minsTxt.split('\\n') # List of paras in minsTxt, where minsTxt is split based on new line characters\n",
        "        cum_paras = 0 # Set up variable for cumulative word-count in all paras for a given minutes transcript\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only including paragraphs larger than 'minparalength'\n",
        "                cum_paras += len(para)\n",
        "\n",
        "        # Extract paragraphs\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only extract paragraphs larger than 'minparalength'\n",
        "\n",
        "                topixTmp = [] # Temporary list to store minutes date & para weight tuple\n",
        "                topixTmp.append(minutes) # First element of tuple (minutes date)\n",
        "                topixTmp.append(len(para)/cum_paras) # Second element of tuple (para weight), NB. Calculating weights based on pre-SpaCy-parsed text\n",
        "\n",
        "                # Parse cleaned para with SpaCy\n",
        "                minsPara = nlp(para)\n",
        "\n",
        "                minsTmp = [] # Temporary list to store individual tokens\n",
        "\n",
        "                # Further cleaning and selection of text characteristics\n",
        "                for token in minsPara:\n",
        "                    if token.is_stop == False and token.is_punct == False and (token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ ==\"VERB\") and token.lemma_.lower() not in custom_stopwords: # Retain words that are not a stop word nor punctuation, and only if a Noun, Adjective or Verb\n",
        "                        minsTmp.append(token.lemma_.lower()) # Convert to lower case and retain the lemmatized version of the word (this is a string object)\n",
        "                        fomcwordcloud.append(token.lemma_.lower()) # Add word to WordCloud list\n",
        "                fomcmins.append(minsTmp) # Add para to corpus 'list of lists'\n",
        "                fomctopix.append(topixTmp) # Add minutes date & para weight tuple to list for storing\n",
        "\n",
        "    return fomcmins, fomcwordcloud, fomctopix\n",
        "\n",
        "# Prepare corpus\n",
        "\n",
        "FOMCMinutes, FOMCWordCloud, FOMCTopix = PrepareCorpus(urlpath=URLPath, urlext=URLExt, minslist=MinutesList, minparalength=200)"
      ],
      "metadata": {
        "id": "DKxYNvm8OQ78"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SETTING UP THE CORPUS by webcrawling_FOMC Statements ###\n",
        "\n",
        "FOMCMinutes = [] # A list of lists to form the corpus\n",
        "FOMCWordCloud = [] # Single list version of the corpus for WordCloud\n",
        "FOMCTopix = [] # List to store minutes ID (date) and weight of each para\n",
        "\n",
        "\n",
        "# Define your custom stopwords - 내 버전, for FOMC Statement\n",
        "custom_stopwords = [\"enable\", \"disabled\", \"browser\", \"access\", \"information\", \"link\", \"provide\", \"email\"]  # Add your own stopwords to this list\n",
        "\n",
        "\n",
        "# Define function to prepare corpus\n",
        "def PrepareCorpus(urlpath, urlext, minslist, minparalength):\n",
        "\n",
        "    fomcmins = []\n",
        "    fomcwordcloud = []\n",
        "    fomctopix = []\n",
        "\n",
        "    for minutes in minslist:\n",
        "\n",
        "        response = requests.get(urlpath + minutes + urlext) # Get the URL response\n",
        "        soup = BeautifulSoup(response.content, 'lxml') # Parse the response\n",
        "\n",
        "        # Extract minutes content and convert to string\n",
        "        minsTxt = str(soup.find(\"div\", {\"id\": \"content\"})) # Contained within the 'div' tag\n",
        "\n",
        "        # Clean text - stage 1\n",
        "        minsTxt = minsTxt.strip()  # Remove white space at the beginning and end\n",
        "        minsTxt = minsTxt.replace('\\r', '') # Replace the \\r with null\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        while '  ' in minsTxt:\n",
        "            minsTxt = minsTxt.replace('  ', ' ') # Remove extra spaces\n",
        "\n",
        "        # Clean text - stage 2, using regex (as SpaCy incorrectly parses certain HTML tags)\n",
        "        minsTxt = re.sub(r'(<[^>]*>)|' # Remove content within HTML tags\n",
        "                         '([_]+)|' # Remove series of underscores\n",
        "                         '(http[^\\s]+)|' # Remove website addresses\n",
        "                         '((a|p)\\.m\\.)', # Remove \"a.m\" and \"p.m.\"\n",
        "                         '', minsTxt) # Replace with null\n",
        "\n",
        "        # Remove sentences after 'Voting' using regular expressions - for FOMC Statement\n",
        "        minsTxt = re.sub(r'Voting.*$', '', minsTxt, flags=re.MULTILINE)\n",
        "\n",
        "        # Find length of minutes document for calculating paragraph weights\n",
        "        minsTxtParas = minsTxt.split('\\n') # List of paras in minsTxt, where minsTxt is split based on new line characters\n",
        "        cum_paras = 0 # Set up variable for cumulative word-count in all paras for a given minutes transcript\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only including paragraphs larger than 'minparalength'\n",
        "                cum_paras += len(para)\n",
        "\n",
        "        # Extract paragraphs\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only extract paragraphs larger than 'minparalength'\n",
        "\n",
        "                topixTmp = [] # Temporary list to store minutes date & para weight tuple\n",
        "                topixTmp.append(minutes) # First element of tuple (minutes date)\n",
        "                topixTmp.append(len(para)/cum_paras) # Second element of tuple (para weight), NB. Calculating weights based on pre-SpaCy-parsed text\n",
        "\n",
        "                # Parse cleaned para with SpaCy\n",
        "                minsPara = nlp(para)\n",
        "\n",
        "                minsTmp = [] # Temporary list to store individual tokens\n",
        "\n",
        "                # Further cleaning and selection of text characteristics\n",
        "                for token in minsPara:\n",
        "                    if token.is_stop == False and token.is_punct == False and (token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ ==\"VERB\") and token.lemma_.lower() not in custom_stopwords: # Retain words that are not a stop word nor punctuation, and only if a Noun, Adjective or Verb\n",
        "                        minsTmp.append(token.lemma_.lower()) # Convert to lower case and retain the lemmatized version of the word (this is a string object)\n",
        "                        fomcwordcloud.append(token.lemma_.lower()) # Add word to WordCloud list\n",
        "                fomcmins.append(minsTmp) # Add para to corpus 'list of lists'\n",
        "                fomctopix.append(topixTmp) # Add minutes date & para weight tuple to list for storing\n",
        "\n",
        "    return fomcmins, fomcwordcloud, fomctopix\n",
        "\n",
        "# Prepare corpus\n",
        "FOMCMinutes, FOMCWordCloud, FOMCTopix = PrepareCorpus(urlpath=URLPath, urlext=URLExt, minslist=MinutesList, minparalength=100)\n",
        "\n",
        "# FOMCMinutes: Eliminate stopword, punctures, and only store NOUN, ADJ, VERB > lower case and retain lemmatized word > store individual tokens to minsTmp\n",
        "# FOMCTopix: Minutes date, len(para)/cum_paras of each para\n",
        "# Warning: only extract if len(para) > minparalength(100)"
      ],
      "metadata": {
        "id": "i0uZVjZcwscu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SETTING UP THE CORPUS by manually ###\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "dataset_bok = pd.read_excel('results2.xlsx')\n",
        "\n",
        "FOMCMinutes = [] # A list of lists to form the corpus\n",
        "FOMCWordCloud = [] # Single list version of the corpus for WordCloud\n",
        "FOMCTopix = [] # List to store minutes ID (date) and weight of each para\n",
        "\n",
        "\n",
        "# Define your custom stopwords - 내 버전, for FOMC Statement\n",
        "custom_stopwords = [\"enable\", \"disabled\", \"browser\", \"access\", \"information\", \"link\", \"provide\", \"email\"]  # Add your own stopwords to this list\n",
        "\n",
        "# Define function to prepare corpus from a dataset\n",
        "\n",
        "def PrepareCorpusFromDataset(dataset, minparalength):\n",
        "    fomcmins = []\n",
        "    fomcwordcloud = []\n",
        "    fomctopix = []\n",
        "\n",
        "    for index, row in dataset.iterrows():\n",
        "        # Extract statement content from the 'FOMC Statement' column\n",
        "        minsTxt = row['FOMC Statement']\n",
        "\n",
        "        # Clean text - stage 1\n",
        "        minsTxt = minsTxt.strip()  # Remove white space at the beginning and end\n",
        "        minsTxt = minsTxt.replace('\\r', '') # Replace the \\r with null\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        minsTxt = minsTxt.replace(' ', ' ') # Replace \" \" with space.\n",
        "        while '  ' in minsTxt:\n",
        "            minsTxt = minsTxt.replace('  ', ' ') # Remove extra spaces\n",
        "\n",
        "        # Clean text - stage 2, using regex (as SpaCy incorrectly parses certain HTML tags)\n",
        "        minsTxt = re.sub(r'(<[^>]*>)|' # Remove content within HTML tags\n",
        "                         '([_]+)|' # Remove series of underscores\n",
        "                         '(http[^\\s]+)|' # Remove website addresses\n",
        "                         '((a|p)\\.m\\.)', # Remove \"a.m\" and \"p.m.\"\n",
        "                         '', minsTxt) # Replace with null\n",
        "\n",
        "        # Remove sentences after 'Voting' using regular expressions - for FOMC Statement\n",
        "        minsTxt = re.sub(r'Voting.*$', '', minsTxt, flags=re.MULTILINE)\n",
        "\n",
        "        # Find length of minutes document for calculating paragraph weights\n",
        "        minsTxtParas = minsTxt.split('\\n') # List of paras in minsTxt, where minsTxt is split based on new line characters\n",
        "        cum_paras = 0 # Set up variable for cumulative word-count in all paras for a given minutes transcript\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only including paragraphs larger than 'minparalength'\n",
        "                cum_paras += len(para)\n",
        "\n",
        "        # Extract paragraphs\n",
        "        for para in minsTxtParas:\n",
        "            if len(para)>minparalength: # Only extract paragraphs larger than 'minparalength'\n",
        "\n",
        "                topixTmp = [] # Temporary list to store minutes date & para weight tuple\n",
        "                topixTmp.append(index) # First element of tuple (minutes date)\n",
        "                topixTmp.append(len(para)/cum_paras) # Second element of tuple (para weight), NB. Calculating weights based on pre-SpaCy-parsed text\n",
        "\n",
        "                # Parse cleaned para with SpaCy\n",
        "                minsPara = nlp(para)\n",
        "\n",
        "                minsTmp = [] # Temporary list to store individual tokens\n",
        "\n",
        "                # Further cleaning and selection of text characteristics\n",
        "                for token in minsPara:\n",
        "                    if token.is_stop == False and token.is_punct == False and (token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ ==\"VERB\") and token.lemma_.lower() not in custom_stopwords: # Retain words that are not a stop word nor punctuation, and only if a Noun, Adjective or Verb\n",
        "                        minsTmp.append(token.lemma_.lower()) # Convert to lower case and retain the lemmatized version of the word (this is a string object)\n",
        "                        fomcwordcloud.append(token.lemma_.lower()) # Add word to WordCloud list\n",
        "                fomcmins.append(minsTmp) # Add para to corpus 'list of lists'\n",
        "                fomctopix.append(topixTmp) # Add minutes date & para weight tuple to list for storing\n",
        "\n",
        "    return fomcmins, fomcwordcloud, fomctopix\n",
        "\n",
        "# Prepare corpus\n",
        "FOMCMinutes, FOMCWordCloud, FOMCTopix = PrepareCorpusFromDataset(dataset_bok, minparalength=100)  # Adjust minparalength as needed\n",
        "\n",
        "# FOMCMinutes: Eliminate stopword, punctures, and only store NOUN, ADJ, VERB > lower case and retain lemmatized word > store individual tokens to minsTmp\n",
        "# FOMCTopix: Minutes date, len(para)/cum_paras of each para\n",
        "# Warning: only extract if len(para) > minparalength(200)"
      ],
      "metadata": {
        "id": "gFxX5DlQDhA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and plot WordCloud for full corpus\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(','.join(FOMCWordCloud)) # NB. 'join' method used to convert the documents list to text format\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TRGP2vfkem9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### NUMERIC REPRESENTATION OF CORPUS USING TF-IDF ###\n",
        "\n",
        "# Form dictionary by mapping word IDs to words\n",
        "ID2word = corpora.Dictionary(FOMCMinutes)\n",
        "\n",
        "# Set up Bag of Words and TFIDF\n",
        "corpus = [ID2word.doc2bow(doc) for doc in FOMCMinutes] # Apply Bag of Words to all documents in corpus\n",
        "TFIDF = models.TfidfModel(corpus) # Fit TF-IDF model\n",
        "trans_TFIDF = TFIDF[corpus] # Apply TF-IDF model\n",
        "\n",
        "# 전체 corpora를 dictionary화한 후 TFIDF를 구해 corpus별 TFIDF를 매칭"
      ],
      "metadata": {
        "id": "9YaEGRkvKl95"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SET UP LDA MODEL ###\n",
        "\n",
        "SEED = 130 # Set random seed\n",
        "NUM_topics = 4 # Set number of topics\n",
        "ALPHA = 0.15 # Set alpha\n",
        "ETA = 1.25 # Set eta\n",
        "\n",
        "# Train LDA model using the corpus\n",
        "lda_model = gensim.models.LdaMulticore(corpus=trans_TFIDF, num_topics=NUM_topics, id2word=ID2word, random_state=SEED, alpha=ALPHA, eta=ETA, passes=100)\n",
        "lda_model.save('lda_model')\n",
        "# 비지도학습"
      ],
      "metadata": {
        "id": "zzBTRiKlKvNQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CALCULATE COHERENCE SCORE ###\n",
        "\n",
        "# Set up coherence model\n",
        "coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=FOMCMinutes, dictionary=ID2word, coherence='c_v')\n",
        "\n",
        "# Calculate coherence\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('-'*50)\n",
        "print('\\nCoherence Score:', coherence_lda)\n",
        "print('-'*50)"
      ],
      "metadata": {
        "id": "KHbIqP_YKv5n",
        "outputId": "ea680de6-8795-45aa-a6ef-f2a6ad3a7181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "\n",
            "Coherence Score: 0.4807537559674327\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### GENERATE WEIGHTED TOPIC PROPORTIONS FOR CORPUS ###\n",
        "\n",
        "para_no = 0 # Set document counter\n",
        "for para in FOMCTopix:\n",
        "    TFIDF_para = TFIDF[corpus[para_no]] # Apply TFIDF model to individual minutes documents\n",
        "    # Generate and store weighted topic proportions for each para\n",
        "    for topic_weight in lda_model.get_document_topics(TFIDF_para): # List of tuples (\"topic number\", \"topic proportion\") for each para, where 'topic_weight' is the (iterating) tuple\n",
        "        FOMCTopix[para_no].append(FOMCTopix[para_no][1]*topic_weight[1]) # Weights are the second element of the pre-appended list, topic proportions are the second element of each tuple\n",
        "    para_no += 1\n",
        "\n",
        "# FOMCTopix: in each para embedding TFIDF_para, calculate topic weight by TFIDF of corpus. After that FOMCTopix = [date, len(para)/cum(para), TFIDF of each topics] of each para"
      ],
      "metadata": {
        "id": "827SzGKqLEFp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show topics\n",
        "\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmSa7bnV9-9E",
        "outputId": "731db438-6c43-4b21-ebb6-24f2840364bf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.010*\"security\" + 0.009*\"operation\" + 0.007*\"transaction\" + 0.006*\"foreign\" + 0.006*\"currency\"')\n",
            "(1, '0.013*\"inflation\" + 0.008*\"participant\" + 0.007*\"economic\" + 0.006*\"policy\" + 0.006*\"percent\"')\n",
            "(2, '0.010*\"governor\" + 0.005*\"loan\" + 0.005*\"consumer\" + 0.004*\"yield\" + 0.004*\"credit\"')\n",
            "(3, '0.006*\"directive\" + 0.006*\"instruct\" + 0.006*\"accordance\" + 0.006*\"execute\" + 0.005*\"government\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Form dataframe of weighted topic proportions (paragraphs) - include any chosen topic names\n",
        "FOMCTopixDF = pd.DataFrame(FOMCTopix, columns=['Date', 'Weight', 'ForeignCurrency', 'Inflation', 'Credit', 'Policy'])\n",
        "\n",
        "# Aggregate topic mix by minutes documents (weighted sum of paragraphs)\n",
        "TopixAggDF = pd.pivot_table(FOMCTopixDF, values=['ForeignCurrency', 'Inflation', 'Credit', 'Policy'], index='Date', aggfunc=np.sum)\n",
        "\n",
        "# Plot results - select which topics to print and change to accumulated area type\n",
        "TopixAggDF.plot(y=['ForeignCurrency', 'Inflation', 'Credit', 'Policy'], kind='line', use_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "9j92joYMud8D",
        "outputId": "7e265735-48d5-4f77-9965-eff1d850c1a6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-47f39af78322>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTopixAggDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ForeignCurrency'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Inflation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Credit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Policy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m             ]\n\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;34mf\"Called plot accessor for type {type(data).__name__}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0;34m\"expected Series or DataFrame\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_get_plot_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1886\u001b[0;31m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pandas_plotting_backends\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1887\u001b[0m     \u001b[0;31m# entry_points lost dict API ~ PY 3.10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m     \u001b[0;31m# https://github.com/python/importlib_metadata/issues/298\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_load_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1815\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfigs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m             >>> df = pd.DataFrame({'x': np.random.randn(n),\n\u001b[1;32m   1819\u001b[0m             ...                    'y': np.random.randn(n)})\n",
            "\u001b[0;31mImportError\u001b[0m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Assuming TopixAggDF is your dataframe\n",
        "\n",
        "fig = go.Figure(data=\n",
        "    go.Parcoords(\n",
        "        line = dict(color = TopixAggDF['ForeignCurrency'],\n",
        "                   colorscale = 'Tealrose',\n",
        "                   showscale = True,\n",
        "                   cmin = 0,\n",
        "                   cmax = 0.5),\n",
        "        dimensions = list([\n",
        "            dict(range = [0,0.5],\n",
        "                 label = 'Foreign Currency', values = TopixAggDF['ForeignCurrency']),\n",
        "            dict(range = [0,0.5],\n",
        "                 label = 'Inflaton', values = TopixAggDF['Inflaton']),\n",
        "            dict(range = [0,0.5],\n",
        "                 label = 'Credit', values = TopixAggDF['Credit']),\n",
        "            dict(range = [0,0.5],\n",
        "                 label = 'Policy', values = TopixAggDF['Policy'])\n",
        "        ])\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "ToKShMYGm7eJ",
        "outputId": "9f3f4056-e476-4140-c91f-a74d8ae95567"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;31m#  the TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Inflaton'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-94c0ebf029e9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                  label = 'Foreign Currency', values = TopixAggDF['ForeignCurrency']),\n\u001b[1;32m     15\u001b[0m             dict(range = [0,0.5],\n\u001b[0;32m---> 16\u001b[0;31m                  label = 'Inflaton', values = TopixAggDF['Inflaton']),\n\u001b[0m\u001b[1;32m     17\u001b[0m             dict(range = [0,0.5],\n\u001b[1;32m     18\u001b[0m                  label = 'Credit', values = TopixAggDF['Credit']),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m             \u001b[0;31m# this is a cached value, mark it so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_as_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m     _index_shared_docs[\n\u001b[1;32m   3806\u001b[0m         \u001b[0;34m\"get_indexer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Inflaton'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TopixAggDF['Inflation']"
      ],
      "metadata": {
        "id": "DKWqd7_c-GKx",
        "outputId": "0b7b5039-afe1-4d1c-807c-6841a4603ee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "20071031    0.388144\n",
              "20071211    0.346348\n",
              "20080130    0.321417\n",
              "20080318    0.369243\n",
              "20080430    0.393085\n",
              "              ...   \n",
              "20200129    0.367646\n",
              "20200315    0.333670\n",
              "20200429    0.323382\n",
              "20200610    0.480197\n",
              "20200729    0.404726\n",
              "Name: Inflation, Length: 100, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TopixAggDF"
      ],
      "metadata": {
        "id": "I5RGHNcV9lIK",
        "outputId": "d5c29a07-2664-42ac-b6c4-43a8ce938817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Credit  Foreign Currency  Inflation    Policy\n",
              "Date                                                     \n",
              "20071031  0.511145          0.048737   0.388144  0.051974\n",
              "20071211  0.507564          0.100615   0.346348  0.045474\n",
              "20080130  0.385658          0.240751   0.321417  0.052174\n",
              "20080318  0.415504          0.169578   0.369243  0.045675\n",
              "20080430  0.422822          0.141809   0.393085  0.042285\n",
              "...            ...               ...        ...       ...\n",
              "20200129  0.264139          0.305041   0.367646  0.063174\n",
              "20200315  0.369864          0.245087   0.333670  0.051380\n",
              "20200429  0.395543          0.236097   0.323382  0.044978\n",
              "20200610  0.290744          0.178632   0.480197  0.050427\n",
              "20200729  0.369103          0.168851   0.404726  0.057321\n",
              "\n",
              "[100 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbdd1e2b-7dda-447d-8d2a-682bb19f1d45\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Credit</th>\n",
              "      <th>Foreign Currency</th>\n",
              "      <th>Inflation</th>\n",
              "      <th>Policy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20071031</th>\n",
              "      <td>0.511145</td>\n",
              "      <td>0.048737</td>\n",
              "      <td>0.388144</td>\n",
              "      <td>0.051974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20071211</th>\n",
              "      <td>0.507564</td>\n",
              "      <td>0.100615</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>0.045474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20080130</th>\n",
              "      <td>0.385658</td>\n",
              "      <td>0.240751</td>\n",
              "      <td>0.321417</td>\n",
              "      <td>0.052174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20080318</th>\n",
              "      <td>0.415504</td>\n",
              "      <td>0.169578</td>\n",
              "      <td>0.369243</td>\n",
              "      <td>0.045675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20080430</th>\n",
              "      <td>0.422822</td>\n",
              "      <td>0.141809</td>\n",
              "      <td>0.393085</td>\n",
              "      <td>0.042285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20200129</th>\n",
              "      <td>0.264139</td>\n",
              "      <td>0.305041</td>\n",
              "      <td>0.367646</td>\n",
              "      <td>0.063174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20200315</th>\n",
              "      <td>0.369864</td>\n",
              "      <td>0.245087</td>\n",
              "      <td>0.333670</td>\n",
              "      <td>0.051380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20200429</th>\n",
              "      <td>0.395543</td>\n",
              "      <td>0.236097</td>\n",
              "      <td>0.323382</td>\n",
              "      <td>0.044978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20200610</th>\n",
              "      <td>0.290744</td>\n",
              "      <td>0.178632</td>\n",
              "      <td>0.480197</td>\n",
              "      <td>0.050427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20200729</th>\n",
              "      <td>0.369103</td>\n",
              "      <td>0.168851</td>\n",
              "      <td>0.404726</td>\n",
              "      <td>0.057321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbdd1e2b-7dda-447d-8d2a-682bb19f1d45')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dbdd1e2b-7dda-447d-8d2a-682bb19f1d45 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dbdd1e2b-7dda-447d-8d2a-682bb19f1d45');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2354e476-c71d-4c0a-88af-2f27ae65161e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2354e476-c71d-4c0a-88af-2f27ae65161e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2354e476-c71d-4c0a-88af-2f27ae65161e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### PRINT TOPIC WORD CLOUDS ###\n",
        "\n",
        "topic = 0 # Initialize counter\n",
        "while topic < NUM_topics:\n",
        "    # Get topics and frequencies and store in a dictionary structure\n",
        "    topic_words_freq = dict(lda_model.show_topic(topic, topn=50)) # NB. the 'dict()' constructor builds dictionaries from sequences (lists) of key-value pairs - this is needed as input for the 'generate_from_frequencies' word cloud function\n",
        "    topic += 1\n",
        "\n",
        "    # Generate Word Cloud for topic using frequencies\n",
        "    wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(topic_words_freq)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kQF6PQoELRm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TEST COHERENCE BY VARYING KEY PARAMETERS ###\n",
        "\n",
        "# Coherence values for varying eta\n",
        "def compute_coherence_values_ETA(corpus, dictionary, num_topics, seed, alpha, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for eta in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=seed, alpha=alpha, eta=eta/100, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_ETA(corpus=trans_TFIDF, dictionary=ID2word, num_topics=NUM_topics, seed=SEED, alpha=ALPHA, texts=FOMCMinutes, start=115, limit=175, step=5)\n",
        "\n",
        "# Plot graph of coherence values by varying eta\n",
        "limit=175; start=115; step=5;\n",
        "#x = range(start, limit, step)\n",
        "x_axis = []\n",
        "for x in range(start, limit, step):\n",
        "    x_axis.append(x/100)\n",
        "plt.plot(x_axis, coherence_values)\n",
        "plt.xlabel(\"Eta\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iFEYnnJ-RzXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Coherence values for varying seed\n",
        "def compute_coherence_values_SEED(corpus, dictionary, alpha, num_topics, eta, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for seed in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, alpha=alpha, num_topics=num_topics, eta=eta, random_state=seed, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_SEED(corpus=trans_TFIDF, dictionary=ID2word, alpha=ALPHA, num_topics=NUM_topics, eta=ETA, texts=FOMCMinutes, start=60, limit=165, step=5)\n",
        "\n",
        "# Plot graph of coherence values by varying seed\n",
        "limit=165; start=60; step=5;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Random Seed\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "evs0SVAwR9qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Coherence values for varying alpha\n",
        "def compute_coherence_values_ALPHA(corpus, dictionary, num_topics, seed, eta, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for alpha in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=seed, eta=eta, alpha=alpha/20, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_ALPHA(dictionary=ID2word, corpus=trans_TFIDF, num_topics=NUM_topics, seed=SEED, eta=ETA, texts=FOMCMinutes, start=1, limit=20, step=1)\n",
        "\n",
        "# Plot graph of coherence values by varying alpha\n",
        "limit=20; start=1; step=1;\n",
        "x_axis = []\n",
        "for x in range(start, limit, step):\n",
        "    x_axis.append(x/20)\n",
        "plt.plot(x_axis, coherence_values)\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fm_cbN2eSBdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coherence values for varying number of topics\n",
        "def compute_coherence_values_TOPICS(corpus, dictionary, alpha, seed, eta, texts, start, limit, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, alpha=alpha, num_topics=num_topics, random_state=seed, eta=eta, passes=100)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values_TOPICS(corpus=trans_TFIDF, dictionary=ID2word, alpha=ALPHA, seed=SEED, eta=ETA, texts=FOMCMinutes, start=2, limit=11, step=1)\n",
        "\n",
        "# Plot graph of coherence values by varying number of topics\n",
        "limit=11; start=2; step=1;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "whGgWNbnBfI5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}