{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezu98/colab/blob/main/Colaboratory%EC%97%90_%EC%98%A4%EC%8B%A0_%EA%B2%83%EC%9D%84_%ED%99%98%EC%98%81%ED%95%A9%EB%8B%88%EB%8B%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "H0P1WTKR3wKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "Lk24R8N63z8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "_BBkuBCo33zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece `_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "metadata": {
        "id": "luBovsdw3_1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "8aIVteQHwxxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "tokenizer.encode(\"한국어 모델을 공유합니다.\")\n",
        "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ],
      "metadata": {
        "id": "dH2a_hAn4K6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
        "print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
      ],
      "metadata": {
        "id": "hWkGFmmYw_xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(\"금융통화위원회는 다음 통화정책방향 결정시까지 한국은행 기준금리를 현 수준(3.50%)에서 유지하여 통화정책을 운용하기로 하였다. 물가상승률이 기조적인 둔화 흐름을 이어갈 것으로 전망되지만 주요국의 통화긴축 기조 장기화, 지정학적 리스크 증대 등으로 물가 및 성장 전망 경로의 불확실성이 크게 높아진 가운데 물가상승률의 둔화 속도가 당초 예상보다 완만해질 것으로 전망되고, 가계부채의 증가 흐름도 지켜볼 필요가 있는 만큼 현재의 긴축 기조를 유지하는 것이 적절하다고 보았다. 추가 인상 필요성은 대내외 정책 여건의 변화를 점검하면서 판단해 나갈 것이다. 세계경제는 주요국의 통화긴축 기조 장기화, 이스라엘·하마스 사태 등의 영향으로 경기 및 인플레이션 흐름과 관련한 불확실성이 증대되었다. 글로벌 경기는 성장세 둔화가 이어질 것으로 전망되며, 주요국 인플레이션은 점차 둔화되고 있지만 여전히 높은 수준인 가운데 국제유가 상승 등으로 상방 리스크가 증대되었다. 국제금융시장에서는 국채금리가 큰 폭 상승하고 미 달러화가 상당폭 강세를 나타내는 등 주요 가격변수의 변동성이 확대되었다. 앞으로 세계경제와 국제금융시장은 국제유가 움직임 및 글로벌 인플레이션의 둔화 흐름, 주요국의 통화정책 변화 및 파급효과, 이스라엘·하마스 사태의 전개양상 등에 영향받을 것으로 보인다. 국내경제는 소비 회복세가 다소 더딘 모습이지만 수출 부진이 완화되면서 성장세가 완만한 개선 흐름을 이어갔다. 고용은 낮은 실업률과 견조한 취업자수 증가가 이어지는 등 전반적으로 양호한 상황이다. 앞으로 국내경제는 수출 부진 완화로 성장세가 점차 개선되면서 금년 성장률도 지난 8월 전망치(1.4%)에 대체로 부합할 것으로 예상된다. 다만 지정학적 리스크 증대, 주요국의 통화긴축 기조 장기화 등의 영향으로 향후 성장 경로의 불확실성이 높아진 것으로 판단된다. 소비자물가는 에너지 및 농산물 가격 상승 등으로 9월중 상승률이 3.7%로 전월보다 높아졌지만, 근원인플레이션율(식료품 및 에너지 제외 지수)과 단기 기대인플레이션율은 모두 9월중 3.3%로 전월과 같은 수준을 나타내었다. 앞으로 소비자물가 상승률은 금년말에는 3%대 초반으로 낮아지고 내년에도 완만한 둔화 흐름을 이어갈 것으로 보인다. 다만 높아진 국제유가와 환율의 파급영향, 이스라엘·하마스 사태 등으로 물가의 상방 리스크가 높아짐에 따라 소비자물가 상승률이 목표수준으로 수렴하는 시기도 당초 예상보다 늦춰질 가능성이 커진 것으로 판단된다. 근원물가도 수요압력 약화 등으로 기조적인 둔화 흐름을 이어가겠으나 누적된 비용인상 압력의 파급영향 지속 등으로 둔화 속도는 당초 예상보다 완만해질 가능성이 높은 것으로 판단된다. 금융·외환시장은 미 연준의 높은 정책금리 장기화 시사, 지정학적 리스크 증대 등으로 변동성이 확대된 가운데 장기 국고채 금리와 원/달러 환율이 상당폭 상승하고 주가는 하락하였다. 일부 비은행부문의 리스크는 진정되는 모습이다. 주택가격은 수도권을 중심으로 상승세가 이어졌으며 가계대출은 주택관련대출을 중심으로 증가세가 지속되었다. 금융통화위원회는 앞으로 성장세를 점검하면서 중기적 시계에서 물가상승률이 목표수준에서 안정될 수 있도록 하는 한편 금융안정에 유의하여 통화정책을 운용해 나갈 것이다. 국내경제는 성장세가 점차 개선되는 가운데 정책 여건의 불확실성도 높아진 상황이다. 따라서 물가안정에 중점을 두고 긴축 기조를 상당기간 지속하면서 추가 인상 필요성을 판단해 나갈 것이다. 이 과정에서 인플레이션 둔화 흐름, 금융안정 측면의 리스크와 성장의 하방위험, 가계부채 증가 추이, 주요국의 통화정책 변화, 지정학적 리스크의 전개양상 등을 면밀히 점검해 나갈 것이다.\"))"
      ],
      "metadata": {
        "id": "fXeNF1ikCpcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "cMR6gXvtDLPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uzCRhIGODLbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "dRSIINwJNB0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the KoSenticNet sentiment dictionary\n",
        "kosenticnet_df = pd.read_csv('path_to_kosenticnet.csv')"
      ],
      "metadata": {
        "id": "4O1POD1tGM8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the KoNLPy tagger\n",
        "okt = Okt()\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"이 영화는 정말 멋있어요.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = okt.morphs(sentence)"
      ],
      "metadata": {
        "id": "DD8WSEhrD-G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.pos(u'이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요'))"
      ],
      "metadata": {
        "id": "emyFNKMyDoVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the sentiment data from kosenticnet.py\n",
        "kosenticnet_data = {ksenticnet[\"가게\"] = ['0', '0.124', '-0.05', '0.203', '#interest', '#admiration', 'positive', '0.09', '매점', '상점', '판매점']}\n",
        "\n",
        "# Extract the sentiment values and create a dictionary\n",
        "sentiment_dictionary = {}\n",
        "for word, data in kosenticnet_data.items():\n",
        "    sentiment_value = data[0]\n",
        "    sentiment_dictionary[word] = float(sentiment_value)\n",
        "\n",
        "# Example usage\n",
        "sample_word = \"불확실성\"\n",
        "if sample_word in sentiment_dictionary:\n",
        "    sentiment_score = sentiment_dictionary[sample_word]\n",
        "    print(f\"The sentiment score for '{sample_word}' is: {sentiment_score}\")\n",
        "else:\n",
        "    print(f\"No sentiment score found for '{sample_word}' in the dictionary.\")\n",
        "\n",
        "# You can further save the sentiment dictionary to a file for later use\n",
        "# For example, to save the dictionary to a CSV file\n",
        "df = pd.DataFrame(list(sentiment_dictionary.items()), columns=['Word', 'Sentiment_Score'])\n",
        "df.to_csv('custom_sentiment_dictionary.csv', index=False)"
      ],
      "metadata": {
        "id": "3TpN3KY6IG7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example string\n",
        "example_string = \"가게0\"\n",
        "\n",
        "# Split the string into word and number\n",
        "match = re.match(r'(\\D+)(\\d+)', example_string)\n",
        "\n",
        "if match:\n",
        "    word = match.group(1)\n",
        "    number = match.group(2)\n",
        "    print(f\"Word: {word}, Number: {number}\")\n",
        "else:\n",
        "    print(\"Pattern not found in the string.\")"
      ],
      "metadata": {
        "id": "s7rVLoH0I4kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('ccc.csv', header=None, names=['text'])"
      ],
      "metadata": {
        "id": "KnhWrZV9NGio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('ccc.csv', header=None, names=['word', 'sentiment'])\n",
        "\n",
        "# Display the dataframe\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDdLw6vBNISQ",
        "outputId": "90c6a698-0118-4252-838c-a988cf82e120"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      word  sentiment\n",
            "0      가게       0.000\n",
            "1      가격      -0.003\n",
            "2      가계       0.000\n",
            "3      가곡       0.045\n",
            "4      가공       0.000\n",
            "...    ...        ...\n",
            "5460  희석하       0.000\n",
            "5461   희열       0.635\n",
            "5462   힐난      -0.540\n",
            "5463  힐난하      -0.700\n",
            "5464    힘       0.775\n",
            "\n",
            "[5465 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install vaderSentiment\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFNlv189OpWA",
        "outputId": "a33378f8-00de-423f-e111-814d4ead4a3d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "sample_text = \"The Monetary Policy Board of the Bank of Korea decided today to leave the Base Rate unchanged at 3.50% for the intermeeting period. Although inflation is projected to continue its underlying trend of a slowdown, uncertainties regarding the future path of inflation and growth have risen significantly due to a prolongation of restrictive monetary policy stances in major countries and heightened geopolitical risks. In addition, it is forecast that the pace of inflation slowdown will moderate more than previously expected, and it is necessary to monitor household debt growth. The Board, therefore, sees that it is appropriate to maintain its current restrictive policy stance. Regarding the need to raise the Base Rate further, the Board will make a judgement while assessing the changes in domestic and external policy conditions. The currently available information suggests that uncertainties regarding economic and inflationary trends have increased across the global economy, driven by a prolongation of restrictive monetary policy stances in major countries and by the Israel-Hamas conflict. Global economic growth is projected to continue slowing. Inflation in major countries still remains high, though falling gradually, and upside risks have increased due to the rise in global oil prices. In global financial markets, volatility of major price variables has increased with government bond yields rising significantly and with the U.S. dollar strengthening considerably. Looking ahead, the Board sees global economic growth and global financial markets as likely to be affected by the movements of global oil prices and the global inflation slowdown, by monetary policy changes in major countries and their effects, and by developments in the Israel-Hamas conflict. Domestic economic growth has continued to improve at a modest pace owing to the easing of sluggishness in exports, although the recovery in private consumption has been somewhat slow. Labor market conditions have been generally favorable, as both a low unemployment rate and a robust increase in the number of persons employed have continued. Going forward, domestic economic growth is expected to improve gradually with the easing of the sluggishness in exports. GDP growth for the year is expected to be generally consistent with the August forecast of 1.4%. However, uncertainties surrounding the economic outlook are judged to be elevated, affected by heightened geopolitical risks and by the prolongation of restrictive monetary policy stances in major countries. Consumer price inflation has risen from August to 3.7% in September, due to the increase in the price of energy and of agricultural products. However, both core inflation (excluding changes in food and energy prices from the CPI) and short-term inflation expectations among the general public have stayed at 3.3% in September, the same as in August. Looking ahead, it is forecast that consumer price inflation will fall to the lower-3% range at the end of this year and will continue to gradually moderate in 2024. However, upside risks to inflation have increased due to the effects of higher global oil prices and exchange rates, and due to the Israel-Hamas conflict. Accordingly, it is judged that the timing of consumer price inflation converging on the target level is more likely to be delayed than previously expected. Meanwhile, core inflation is also projected to maintain its underlying slowing trend, owing to the weakening of demand-side pressures. However, the pace of the slowdown is likely to be more modest than previously forecast due to the continuing spillover effects of accumulated cost pressure. In financial and foreign exchange markets, volatility has increased as the U.S. Federal Reserve has signaled a prolongation of a high policy rate and as geopolitical risks have expanded. Long-term Korean Treasury bond yields and the Korean won to U.S. dollar exchange rate have risen significantly and stock prices have fallen. Meanwhile, the risks to some non-bank financial sectors have eased. Housing prices have continued their upward trend, especially in Seoul and its surrounding areas. Household loans have continued to increase, mainly driven by housing-related loans. The Board will continue to conduct monetary policy in order to stabilize consumer price inflation at the target level over the medium-term horizon as it monitors economic growth, while paying attention to financial stability. While domestic economic growth is forecast to gradually improve, uncertainties surrounding the policy decision have also risen. The Board, therefore, will maintain a restrictive policy stance for a considerable time with an emphasis on ensuring price stability, while making a judgement regarding the need to raise the Base Rate further. In this process, the Board will thoroughly assess the inflation slowdown, financial stability risks, economic downside risks, monetary policy changes in major countries, household debt growth, and developments in geopolitical risks.\"\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Perform sentiment analysis\n",
        "scores = analyzer.polarity_scores(sample_text)\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEjqfmw1Doi8",
        "outputId": "9a2d9649-bf79-4168-beca-d2f5f3b86657"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.077, 'neu': 0.828, 'pos': 0.095, 'compound': 0.9534}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XQG6VUiUGVLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import sys\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0]\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = uncertainty_value\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    md = r'd.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Example word to check\n",
        "    word_to_check = 'UNCERTAINTY'  # Replace with the word you want to check\n",
        "    if word_to_check in master_dictionary:\n",
        "        uncertainty_value = master_dictionary[word_to_check]\n",
        "        if uncertainty_value == 0:\n",
        "            print(f\"The word '{word_to_check}' is not classified as an uncertain word.\")\n",
        "        else:\n",
        "            print(f\"The word '{word_to_check}' is classified as an uncertain word since the year {uncertainty_value}.\")\n",
        "    else:\n",
        "        print(f\"The word '{word_to_check}' is not found in the dictionary.\")\n",
        "\n",
        "    print(f'\\nRuntime: {(dt.datetime.now()-start)}')\n",
        "    print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZldOIPTVzLR",
        "outputId": "5b28ff1e-3668-4cdb-afca-b7f05564bf65"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tue Oct 24 15:40:47 2023\n",
            "PROGRAM NAME: /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "\n",
            "\n",
            "\n",
            "Master Dictionary loaded from file:\n",
            "  d.csv\n",
            "\n",
            "  master_dictionary has 86,531 words.\n",
            "\n",
            "The word 'UNCERTAINTY' is classified as an uncertain word since the year 2009.\n",
            "\n",
            "Runtime: 0:00:00.218213\n",
            "\n",
            "Normal termination.\n",
            "Tue Oct 24 15:40:47 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0]\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = uncertainty_value\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary\n",
        "\n",
        "def evaluate_uncertainty_index(text, master_dictionary):\n",
        "    words = text.split()\n",
        "    uncertain_words_count = 0\n",
        "    for word in words:\n",
        "        if word in master_dictionary and master_dictionary[word] != 0:\n",
        "            uncertain_words_count += 1\n",
        "    uncertainty_index = uncertain_words_count / len(words)  # Calculate the uncertainty index\n",
        "    return uncertainty_index\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    md = r'd.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Example FOMC statement to evaluate\n",
        "    fomc_statement = \"The committee is uncertain about the economic outlook due to the recent market fluctuations.\"\n",
        "\n",
        "    uncertainty_index = evaluate_uncertainty_index(fomc_statement, master_dictionary)\n",
        "    print(f\"\\nUncertainty index for the FOMC statement: {uncertainty_index:.2f}\")\n",
        "\n",
        "    print(f'\\nRuntime: {(dt.datetime.now()-start)}')\n",
        "    print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2yHvin7SCAQ",
        "outputId": "1141b4a1-d313-4e03-ba17-b53bff90784f"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tue Oct 24 15:46:13 2023\n",
            "PROGRAM NAME: /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "\n",
            "\n",
            "\n",
            "Master Dictionary loaded from file:\n",
            "  d.csv\n",
            "\n",
            "  master_dictionary has 86,531 words.\n",
            "\n",
            "\n",
            "Uncertainty index for the FOMC statement: 0.00\n",
            "\n",
            "Runtime: 0:00:00.259261\n",
            "\n",
            "Normal termination.\n",
            "Tue Oct 24 15:46:13 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0]\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = uncertainty_value\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary\n",
        "\n",
        "def evaluate_uncertainty_index(text, master_dictionary):\n",
        "    words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    uncertain_words_count = 0\n",
        "    for word in words:\n",
        "        if word in master_dictionary and master_dictionary[word] != 0:\n",
        "            uncertain_words_count += 1\n",
        "    uncertainty_index = uncertain_words_count / len(words)  # Calculate the uncertainty index\n",
        "    return uncertainty_index\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    md = r'd.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Example FOMC statement to evaluate\n",
        "    fomc_statement = \"The committee is uncertain about the economic outlook due to the recent market fluctuations.\"\n",
        "\n",
        "    uncertainty_index = evaluate_uncertainty_index(fomc_statement, master_dictionary)\n",
        "    print(f\"\\nUncertainty index for the FOMC statement: {uncertainty_index:.2f}\")\n",
        "\n",
        "    print(f'\\nRuntime: {(dt.datetime.now()-start)}')\n",
        "    print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvhYPN5QXvEp",
        "outputId": "c4cc2291-aae3-4ba2-ecb0-b00962a45a51"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tue Oct 24 15:48:18 2023\n",
            "PROGRAM NAME: /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "\n",
            "\n",
            "\n",
            "Master Dictionary loaded from file:\n",
            "  d.csv\n",
            "\n",
            "  master_dictionary has 86,531 words.\n",
            "\n",
            "\n",
            "Uncertainty index for the FOMC statement: 0.00\n",
            "\n",
            "Runtime: 0:00:00.267558\n",
            "\n",
            "Normal termination.\n",
            "Tue Oct 24 15:48:18 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
        "    start_local = dt.datetime.now()\n",
        "    # Setup dictionaries\n",
        "    _master_dictionary = {}\n",
        "\n",
        "    # Loop through words and load dictionaries\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        _total_documents = 0\n",
        "        _md_header = f.readline()  # Consume header line\n",
        "        print()\n",
        "        for line in f:\n",
        "            cols = line.rstrip('\\n').split(',')\n",
        "            word = cols[0].upper()  # Convert word to uppercase\n",
        "            negative_score = int(cols[8])\n",
        "            positive_score = int(cols[9])\n",
        "            uncertainty_value = int(cols[9])\n",
        "            _master_dictionary[word] = {'negative': negative_score, 'positive': positive_score, 'uncertainty': uncertainty_value}\n",
        "\n",
        "    if print_flag:\n",
        "        print('\\r', end='')  # clear line\n",
        "        print(f'\\nMaster Dictionary loaded from file:\\n  {file_path}\\n')\n",
        "        print(f'  master_dictionary has {len(_master_dictionary):,} words.\\n')\n",
        "\n",
        "    if get_other:\n",
        "        return _master_dictionary, _md_header, _total_documents\n",
        "    else:\n",
        "        return _master_dictionary\n",
        "\n",
        "def evaluate_sentiment_indices(text, master_dictionary):\n",
        "    words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    words_uppercase = [word.upper() for word in words]  # Convert all words to uppercase\n",
        "    negative_score, positive_score, uncertain_words_count = 0, 0, 0\n",
        "    negative_words_count, positive_words_count = 0, 0\n",
        "    for word in words_uppercase:\n",
        "        if word in master_dictionary:\n",
        "            if master_dictionary[word]['uncertainty'] != 0:\n",
        "                uncertain_words_count += 1\n",
        "            negative_score += master_dictionary[word]['negative']\n",
        "            positive_score += master_dictionary[word]['positive']\n",
        "            if master_dictionary[word]['negative'] > 0:\n",
        "                negative_words_count += 1\n",
        "            if master_dictionary[word]['positive'] > 0:\n",
        "                positive_words_count += 1\n",
        "\n",
        "    negative_index = negative_score / len(words)  # Calculate the negative index\n",
        "    positive_index = positive_score / len(words)  # Calculate the positive index\n",
        "    uncertainty_index = uncertain_words_count / len(words)  # Calculate the uncertainty index\n",
        "\n",
        "    return negative_index, positive_index, uncertainty_index, uncertain_words_count, negative_words_count, positive_words_count\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = dt.datetime.now()\n",
        "#    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
        "    md = r'd.csv'  # Update with your file path\n",
        "    master_dictionary = load_masterdictionary(md, True)\n",
        "\n",
        "    # Example FOMC statement to evaluate\n",
        "    fomc_statement = \"The Monetary Policy Board of the Bank of Korea decided today to leave the Base Rate unchanged at 3.50% for the intermeeting period. It is forecast that inflation will remain above the target level for a considerable time although it is projected to continue to slow. The Board, therefore, sees that it is appropriate to maintain its current restrictive policy stance. Regarding the need to raise the Base Rate further, the Board will make a judgement while assessing the changes in domestic and external policy conditions. The currently available information suggests that global economic growth has been more favorable than expected, but growth is projected to gradually slow due to the restrictive monetary policy stance being sustained in major countries and due to the contraction in bank credit supply. Global inflation still remains high, while continuing its slowdown, and core inflation is declining at a relatively slow pace. In global financial markets, the U.S. dollar initially weakened as the U.S. Federal Reserve signaled a potential end to rate hikes, but then it has fluctuated since mid-May affected by economic indicators exceeding market expectations and by developments in U.S. debt ceiling negotiations. Long-term government bond yields in major countries have risen after having fluctuated within a narrow range. Looking ahead, the Board sees global economic growth and global financial markets as likely to be affected by the pace of global inflation slowdown, monetary policy changes in major countries, U.S. dollar trends, risks to small and medium-sized U.S. banks, debt ceiling negotiations in the U.S., and the recovery in the Chinese economy. Domestic economic growth has continued to slow, with ongoing sluggishness of exports and investment, although private consumption has shown a modest recovery led by services. Labor market conditions have generally continued to be favorable, but the increase in the number of persons employed has declined due to the economic slowdown. Going forward, domestic economic growth is expected to remain weak for some time. From the second half of this year, however, it is expected to recover gradually with an easing of the sluggishness in the IT industry and the impact of the Chinese economic recovery. GDP growth for this year is projected to be 1.4%, lower than the February forecast of 1.6%, but uncertainties regarding the timing of a rebound in the IT industry, the domestic impact of the recovery in the Chinese economy, and economic growth in major advanced countries are all judged to be high. Consumer price inflation has continued to moderate as expected, declining from 4.2% in March to 3.7% in April. This is mainly because the decline in the price of petroleum products has widened and the rise in the prices of processed food products has weakened. Core inflation (excluding changes in food and energy prices from the CPI) has stayed at 4.0%, and short-term inflation expectations among the general public have moved down to 3.5% in May. Looking ahead, it is forecast that consumer price inflation will fall considerably owing to the base effect from the sharp rises in global oil prices last year, and then will rise slightly and fluctuate at around the 3% level until the end of this year. Consumer price inflation for this year is expected to be consistent with the February forecast of 3.5%. Meanwhile, it is judged that the pace of core inflation slowdown is likely to be more modest than previously forecast due to accumulated cost pressure and favorable demand in services. Core inflation is projected to be 3.3%, which is higher than the February forecast of 3.0%. The inflation path is likely to be affected by movements of global oil prices and exchange rates, the degree of economic slowdown at home and abroad, and any further increase in public utility fees. In financial and foreign exchange markets, the Korean won to U.S. dollar exchange rate has fluctuated considerably due to trends in the trade balance, expectations of an end to policy rate hikes by the U.S. Federal Reserve, and negotiations on the U.S. debt ceiling. Long-term Korean Treasury bond yields have shown a modest increase, influenced by the movements of government bond yields in major countries. Household loans have slightly increased and the extent of the decline in housing prices has narrowed. The Board will continue to conduct monetary policy in order to stabilize consumer price inflation at the target level over the medium-term horizon as it monitors economic growth, while paying attention to financial stability. Domestic economic growth is expected to remain low, but inflation is projected to remain above the target level for a considerable time. Moreover, uncertainties surrounding the policy decision are judged to be high. The Board, therefore, will maintain a restrictive policy stance for a considerable time with an emphasis on ensuring price stability. Regarding the need to raise the Base Rate further, the Board will make a judgement while thoroughly assessing the pace of inflation slowdown, the economic downside risks and financial stability risks, the effects of the Base Rate raises, and monetary policy changes in major countries.\"\n",
        "\n",
        "    negative_index, positive_index, uncertainty_index, uncertain_words_count, negative_words_count, positive_words_count = evaluate_sentiment_indices(fomc_statement, master_dictionary)\n",
        "    print(f\"\\nNumber of uncertain words: {uncertain_words_count}\")\n",
        "    print(f\"Number of negative words: {negative_words_count}\")\n",
        "    print(f\"Number of positive words: {positive_words_count}\")\n",
        "    print(f\"Negative index for the FOMC statement: {negative_index:.2f}\")\n",
        "    print(f\"Positive index for the FOMC statement: {positive_index:.2f}\")\n",
        "    print(f\"Uncertainty index for the FOMC statement: {uncertainty_index:.2f}\")\n",
        "\n",
        "#    print(f'\\nRuntime: {(dt.datetime.now()-start)}')\n",
        "#    print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WI8u6caYHKJ",
        "outputId": "7c78d8d9-8c64-4fe9-9fc5-3d00a90c4444"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tue Oct 24 16:01:37 2023\n",
            "PROGRAM NAME: /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "\n",
            "\n",
            "\n",
            "Master Dictionary loaded from file:\n",
            "  d.csv\n",
            "\n",
            "  master_dictionary has 86,531 words.\n",
            "\n",
            "\n",
            "Number of uncertain words: 11\n",
            "Number of negative words: 8\n",
            "Number of positive words: 11\n",
            "Negative index for the FOMC statement: 17.38\n",
            "Positive index for the FOMC statement: 23.89\n",
            "Uncertainty index for the FOMC statement: 0.01\n",
            "\n",
            "Runtime: 0:00:00.429868\n",
            "\n",
            "Normal termination.\n",
            "Tue Oct 24 16:01:37 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words(head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "Lx6DCL-3XY6f",
        "outputId": "9d47cf26-7ef8-4fea-b327-2e00583f2947"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-8c7118df0c95>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-7kFZPjSCJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마스크 인풋\n",
        "valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\")"
      ],
      "metadata": {
        "id": "OB0GqczN7RT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "\n",
        "    SEQ_LEN = 64 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
        "\n",
        "    tokens, masks, segments, targets = [], [], [], []\n",
        "\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        # token : 문장을 토큰화함\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], truncation=True, padding='max_length', max_length=SEQ_LEN)\n",
        "\n",
        "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "\n",
        "        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
        "        tokens.append(token)\n",
        "        masks.append(mask)\n",
        "        segments.append(segment)\n",
        "\n",
        "        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n",
        "        targets.append(data_df[LABEL_COLUMN][i])\n",
        "\n",
        "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    return [tokens, masks, segments], targets\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x, data_y\n",
        "\n",
        "SEQ_LEN = 64\n",
        "BATCH_SIZE = 32\n",
        "# 긍부정 문장을 포함하고 있는 칼럼\n",
        "DATA_COLUMN = \"document\"\n",
        "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n",
        "LABEL_COLUMN = \"label\"\n",
        "\n",
        "# train 데이터를 버트 인풋에 맞게 변환\n",
        "train_x, train_y = load_data(train)"
      ],
      "metadata": {
        "id": "w47L9gYPxdWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 성능을 검증한 test 데이터를 버트 인풋에 맞게 변환\n",
        "test_x, test_y = load_data(test)"
      ],
      "metadata": {
        "id": "1SQe8aSpxsru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
        "# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
        "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
        "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
        "# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
        "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"
      ],
      "metadata": {
        "id": "NycfOSPf67ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_outputs"
      ],
      "metadata": {
        "id": "tp11cdxa7ER2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_outputs = bert_outputs[1]"
      ],
      "metadata": {
        "id": "kOK8jh0X7Txf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rectified Adam 옵티마이저 사용\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "# 총 batch size * 4 epoch = 2344 * 4\n",
        "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*2, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)\n"
      ],
      "metadata": {
        "id": "q1YFB9zU7YeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n",
        "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n",
        "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
        "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "qgPjX3dy7ae7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.summary()\n"
      ],
      "metadata": {
        "id": "i6f9If0X7bfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.fit(train_x, train_y, epochs=2, shuffle=True, batch_size=64, validation_data=(test_x, test_y))"
      ],
      "metadata": {
        "id": "PEcnqi6Hx6zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_convert_data(data_df):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        tokens.append(token)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def predict_load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_x = predict_convert_data(data_df)\n",
        "    return data_x"
      ],
      "metadata": {
        "id": "oqQJ0szJx9Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = predict_load_data(test)"
      ],
      "metadata": {
        "id": "jj8kNEl-x9gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set\n"
      ],
      "metadata": {
        "id": "QE1OlfvYx-3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = sentiment_model.predict(test_set)\n"
      ],
      "metadata": {
        "id": "QKg38wBDx_3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 부정이면 0, 긍정이면 1 출력\n",
        "preds"
      ],
      "metadata": {
        "id": "QUlW68qvyBFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = test['label']\n",
        "# F1 Score 확인\n",
        "print(classification_report(y_true, np.round(preds,0)))"
      ],
      "metadata": {
        "id": "2GXhmXZnyC60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "QOrtISAXyE7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_convert_data(data):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "    token = tokenizer.encode(data, max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "\n",
        "    num_zeros = token.count(0)\n",
        "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "    segment = [0]*SEQ_LEN\n",
        "\n",
        "    tokens.append(token)\n",
        "    segments.append(segment)\n",
        "    masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "\n",
        "def movie_evaluation_predict(sentence):\n",
        "    data_x = sentence_convert_data(sentence)\n",
        "    predict = sentiment_model.predict(data_x)\n",
        "    predict_value = np.ravel(predict)\n",
        "    predict_answer = np.round(predict_value,0).item()\n",
        "\n",
        "    if predict_answer == 0:\n",
        "      print(\"(부정 확률 : %.2f) 부정적인 영화 평가입니다.\" % (1-predict_value))\n",
        "    elif predict_answer == 1:\n",
        "      print(\"(긍정 확률 : %.2f) 긍정적인 영화 평가입니다.\" % predict_value)"
      ],
      "metadata": {
        "id": "CN5j1CR8yKne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_evaluation_predict(\"보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에 \")"
      ],
      "metadata": {
        "id": "ZX4bkdh9yNHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}