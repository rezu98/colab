{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlK3mXawTpSIyCDlStfqtR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezu98/colab/blob/main/iti.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 공통 라이브러리\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 데이터 처리 라이브러리\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 크롤링 관련 라이브러리\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "# 문장 분리 라이브러리\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "# 언어모형 라이브러리 다운로드\n",
        "DOWNLOAD_LIB = False\n",
        "if (DOWNLOAD_LIB):\n",
        "    nltk.download('punkt')\n",
        "    url = 'https://github.com/monologg/KoBERT-Transformers/raw/master/kobert_transformers/tokenization_kobert.py'\n",
        "    os.popen('wget -q --no-check-certificate ' + url)\n",
        "    os.popen('pip install -q datasets')\n",
        "    os.popen('pip install -q kobert-transformers')\n",
        "\n",
        "# 언어모형 관련 라이브러리\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import Dataset, disable_progress_bar\n",
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from tokenization_kobert import KoBertTokenizer\n",
        "disable_progress_bar()\n",
        "\n",
        "# 차트 관련 라이브러리\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import font_manager, rc\n",
        "\n",
        "\n",
        "# 사전훈련 언어모형 저장경로\n",
        "ROOT_DIR = './'\n",
        "TOKENIZER = ROOT_DIR + 'tokenizer'\n",
        "ORG_MODEL = ROOT_DIR + 'org_model'\n",
        "\n",
        "# 뉴스기사 링크, 본문, 라벨링 파일 위치\n",
        "LINKS = ROOT_DIR + 'links'\n",
        "TEXTS = ROOT_DIR + 'texts'\n",
        "LABELS = ROOT_DIR + 'labels'\n",
        "os.makedirs(LINKS, exist_ok=True)\n",
        "os.makedirs(TEXTS, exist_ok=True)\n",
        "os.makedirs(LABELS, exist_ok=True)\n",
        "\n",
        "# 특수 문자\n",
        "TAB = '\\t'\n",
        "LF = '\\n'\n",
        "\n",
        "# 텍스트 저장 형식\n",
        "TEXT_HEADER = TAB.join(['date', 'article', 'text']) + LF\n",
        "\n",
        "# GPU 사용 가능하도록 device 얻기\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# 차트 기본 옵션\n",
        "COLORSET = ['dodgerblue', 'lightskyblue']\n",
        "MARKERS = ['D', 's']\n",
        "LEFT_OPTION = {\n",
        "    'suffix': '(좌측)',\n",
        "    'legend_loc': 'upper left',\n",
        "}\n",
        "RIGHT_OPTION = {\n",
        "    'suffix': '(우측)',\n",
        "    'legend_loc': 'upper right',\n",
        "    'colorset': ['lightcoral']\n",
        "}\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (10, 4)\n",
        "\n",
        "# 폰트 설정\n",
        "font_path = ROOT_DIR + 'NanumGothic.ttf'\n",
        "font = font_manager.FontProperties(fname=font_path).get_name()\n",
        "plt.rc('font', family=font, size=10)\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "font_manager._rebuild()\n",
        "\n",
        "\n",
        "def get_valid_periods(freq, from_period, to_period, text_path=None):\n",
        "    \"\"\"from ~ to 기간중 유효한 시점을 반환\"\"\"\n",
        "    formats = {'D': '%Y%m%d', 'M': '%Y%m'}\n",
        "    periods = pd.period_range(from_period, to_period,\n",
        "                freq=freq).strftime(formats[freq]).to_list()\n",
        "    if (text_path is not None):\n",
        "        periods = [period for period in periods\n",
        "                    if os.path.exists('{0}/{1}.txt'.format(text_path, period))]\n",
        "    periods.sort()\n",
        "    return periods\n",
        "\n",
        "\n",
        "def df_to_ds(df):\n",
        "    \"\"\"dataframe(pandas) 형식을 언어모형용 dataset(huggingface) 형식으로 변환\"\"\"\n",
        "    input_col = df.columns[0]\n",
        "    output_col = df.columns[1]\n",
        "    # 원문text를 vocab id로 encode하기 위해 사전훈련 언어모형의 토크나이저를 이용\n",
        "    tokenizer = KoBertTokenizer.from_pretrained(TOKENIZER)\n",
        "    tokenize = lambda x: tokenizer(x[input_col], padding='max_length', truncation=True)\n",
        "\n",
        "    # 예측변수 이름을 labels로 지정\n",
        "    df = df.rename(columns={output_col:'labels'})\n",
        "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
        "    ds = ds.class_encode_column('labels')\n",
        "    # 입력변수(원문text)를 vocab id로 encode\n",
        "    ds = ds.map(tokenize)\n",
        "    ds = ds.remove_columns([input_col, 'token_type_ids'])\n",
        "    ds.set_format('torch')\n",
        "    return ds\n",
        "\n",
        "\n",
        "def split_ds(ds, train_size=0.8):\n",
        "    \"\"\"dataset을 훈련, 테스트 데이터로 분리\"\"\"\n",
        "    ds = ds.train_test_split(train_size=train_size, stratify_by_column='labels', shuffle=True, seed=1)\n",
        "    return ds['train'], ds['test']\n",
        "\n",
        "\n",
        "class distillKoBertTokenizer(KoBertTokenizer):\n",
        "    \"\"\"distillKoBERT tokenizer 저장을 위한 임시 클래스\"\"\"\n",
        "    def save_vocabulary(self, save_directory, filename_prefix):\n",
        "        return super().save_vocabulary(save_directory)\n",
        "\n",
        "\n",
        "def load_plm(model_path=ORG_MODEL):\n",
        "    \"\"\"사전훈련 언어모형을 다운로드\"\"\"\n",
        "    # 언어모형 리파지터리 경로\n",
        "    REPO_PATH = 'monologg/distilkobert'\n",
        "\n",
        "    # KoBERT tokenizer 로드\n",
        "    tokenizer = distillKoBertTokenizer.from_pretrained(REPO_PATH)\n",
        "    tokenizer.save_pretrained(TOKENIZER)\n",
        "\n",
        "    # 설정에서 tokenizer 클래스명을 원래대로 변경\n",
        "    json_file = TOKENIZER + '/tokenizer_config.json'\n",
        "    with open(json_file, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "        json_data['tokenizer_class'] = 'KoBertTokenizer'\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(json_data, f, indent=4)\n",
        "\n",
        "    # 언어모형 설정값 얻은 후 output_hidden_states를 False로 변경\n",
        "    config = DistilBertConfig.from_pretrained(REPO_PATH)\n",
        "    config.output_hidden_states = False\n",
        "    # distilKoBERT 모형 다운로드 및 저장\n",
        "    model = DistilBertModel.from_pretrained(REPO_PATH, config=config)\n",
        "    model.save_pretrained(model_path)\n",
        "\n",
        "\n",
        "def finetune(num_labels, ds, model_path=ORG_MODEL, batch_size=8, num_epochs=10):\n",
        "    \"\"\"사전훈련 언어모형의 미세조정\"\"\"\n",
        "    dl = DataLoader(ds, batch_size=batch_size)\n",
        "    num_training_steps = num_epochs * len(dl)\n",
        "    # 사전훈련 언어모형(distillKoBERT) 로드\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
        "    # 학습을 위한 최적화 알고리즘 설정\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    scheduler = get_scheduler('linear', optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # 모형 학습 모드\n",
        "    model.train()\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in dl:\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            # 분류 예측(forward propagation)\n",
        "            outputs = model(**batch)\n",
        "            # 예측오차 역전파(back propagation)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            # 최적화(gradient update)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def classify(model_path, num_label, ds, batch_size=192):\n",
        "    \"\"\"미세조정된 모형의 분류 예측\"\"\"\n",
        "    preds = []\n",
        "    dl = DataLoader(ds, batch_size=batch_size)\n",
        "    # 분류모형 로드\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_label)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # 모형 비학습 모드\n",
        "    model.eval()\n",
        "    for batch in dl:\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        # 가장 높은 확률로 예측한 라벨을 분류결과로 저장\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        preds.append(predictions)\n",
        "\n",
        "    return torch.cat(preds).tolist()\n",
        "\n",
        "\n",
        "def classify_monthly(month, model_paths, num_labels):\n",
        "    \"\"\"월별 전체 뉴스기사를 분류\"\"\"\n",
        "    in_file_path  = '{0}/{1}.txt'.format(TEXTS, month)\n",
        "    out_file_path = '{0}/{1}.txt'.format(LABELS, month)\n",
        "\n",
        "    # 데이터 형태 변환\n",
        "    df = pd.read_csv(in_file_path, encoding='utf-8', sep=TAB, quoting=csv.QUOTE_NONE)\n",
        "    df['label'] = '0'\n",
        "    ds = df_to_ds(df[['text', 'label']])\n",
        "\n",
        "    data = {'date': df.date.to_list(), 'article':df.article.to_list()}\n",
        "    # 모형별로 데이터를 분류\n",
        "    for model, path in model_paths.items():\n",
        "        preds = classify(path, num_labels[model], ds)\n",
        "        data[model + '_label'] = preds\n",
        "\n",
        "    # 분류결과를 저장\n",
        "    df_pred = pd.DataFrame(data)\n",
        "    df_pred.to_csv(out_file_path, sep=TAB, index = False)\n",
        "\n",
        "\n",
        "def load_label_data(from_month, to_month):\n",
        "    \"\"\"해당 기간의 전체 라벨 데이터를 로드\"\"\"\n",
        "    df_labels = None\n",
        "    months = get_valid_periods('M', from_month, to_month, LABELS)\n",
        "    # 전체 파일의 내용 취합\n",
        "    progress_bar = tqdm(months)\n",
        "    for month in months:\n",
        "        file_path  = '{0}/{1}.txt'.format(LABELS, month)\n",
        "        tmp = pd.read_csv(file_path, encoding='utf-8', parse_dates=True, index_col='date',\n",
        "                          sep=TAB, quoting=csv.QUOTE_NONE)\n",
        "        if df_labels is None:\n",
        "            df_labels = tmp\n",
        "        else:\n",
        "            df_labels = pd.concat([df_labels, tmp], axis=0)\n",
        "        progress_bar.update(1)\n",
        "    return df_labels\n",
        "\n",
        "\n",
        "def get_breit_series(req_ids, alias_nm,\n",
        "                     start_d=None, end_d=None,\n",
        "                     period_trim=False):\n",
        "    \"\"\"BReiT API 호출을 통한 데이터 입수\"\"\"\n",
        "    # API 호출\n",
        "    API = \"http://datahub.boknet.intra/api/v1/obs/lists\"\n",
        "    res = requests.post(API, data={\"ids\":req_ids})\n",
        "    data_list = res.json()[\"data\"][0]\n",
        "\n",
        "    # API 호출로 받은 결과를 Data Frame으로 저장\n",
        "    data = pd.DataFrame()\n",
        "    for alias, value in zip(alias_nm, data_list):\n",
        "        df = pd.DataFrame(value[\"observations\"], dtype=\"float\")\n",
        "        df.set_index(\"period\", inplace=True)\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df.columns = [alias]\n",
        "        data = df.copy() if not len(data) else data.join(df, how=\"outer\")\n",
        "\n",
        "    # 옵션에 따라 시작일, 종료일, Trim 적용\n",
        "    if start_d:\n",
        "        data = data[data.index >= start_d]\n",
        "    if end_d:\n",
        "        data = data[data.index <= end_d]\n",
        "    if period_trim:\n",
        "        data.index = data.index.to_period('M')\n",
        "    return data\n",
        "\n",
        "\n",
        "def diff_corr(data, x_vals, y_vals, val_names, max_diff, window_size):\n",
        "    \"\"\"x, y 변수간 시차별 최대 상관계수 계산\"\"\"\n",
        "    index_array = [[val_names[x_val] for x_val in x_vals], [val_names[y_val] for y_val in y_vals]]\n",
        "    index = pd.MultiIndex.from_product(index_array)\n",
        "    df_corr = pd.DataFrame(columns=['계수', '시차'], index=index)\n",
        "\n",
        "    xy_corrs = {}\n",
        "    for x_val in x_vals:\n",
        "        y_corrs = {}\n",
        "        for y_val in y_vals:\n",
        "            time_diff_corr = np.zeros(max_diff*2+1)\n",
        "            for t_diff in range(-max_diff, max_diff+1):\n",
        "                if window_size > 1:\n",
        "                    df_diff = pd.concat([data[x_val].shift(t_diff, axis=0), data[y_val].rolling(window_size).mean()], axis=1).dropna()\n",
        "                else:\n",
        "                    df_diff = pd.concat([data[x_val].shift(t_diff, axis=0), data[y_val]], axis=1).dropna()\n",
        "                # Pearson correlation 계산\n",
        "                corr, _ = pearsonr(df_diff[x_val], df_diff[y_val])\n",
        "                time_diff_corr[t_diff + max_diff] = corr\n",
        "                y_corrs[y_val] = time_diff_corr\n",
        "            # 상관계수를 최대로 하는 시차 계산\n",
        "            max_idx = np.argmax(time_diff_corr)\n",
        "            df_corr.loc[(val_names[x_val], val_names[y_val]),:] = (time_diff_corr[max_idx], max_idx-max_diff)\n",
        "        xy_corrs[x_val] = y_corrs\n",
        "\n",
        "    df_corr.unstack(level=1)\n",
        "    return df_corr, xy_corrs\n",
        "\n",
        "\n",
        "def plot_corrs(corrs):\n",
        "    \"\"\"시차에 따른 상관계수를 bar plot으로 표시\"\"\"\n",
        "    max_diff = int((len(corrs)-1)/2)\n",
        "    bt = plt.bar([t_diff for t_diff in range(-max_diff, max_diff+1)], corrs)\n",
        "    plt.xlabel('시차(월)')\n",
        "    plt.ylabel('상관계수')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def identify_tp(data, scope=6):\n",
        "    \"\"\"범위에 따른 고점/저점 식별\"\"\"\n",
        "    # 분기별 어조지수, 소비자물가상승률 데이터 및 차분값\n",
        "    trend = data[1:]\n",
        "    trend_diff = data.diff().dropna()\n",
        "    # +1은 하락->상승 전환 변곡점(저점), -1은 상승->하락 전환 변곡점(고점)\n",
        "    turning_points = {column:[] for column in trend.columns}\n",
        "\n",
        "    # 상승->하락, 하락->상승 전환시 local minima/maxima 여부를 확인\n",
        "    for i in range(0, len(trend_diff)-scope-1):\n",
        "        for l in trend.columns:\n",
        "            # 변곡점인 경우\n",
        "            if (trend_diff[l].iloc[i] * trend_diff[l].iloc[i+1] < 0):\n",
        "                # 하락->상승 전환기\n",
        "                if trend_diff[l].iloc[i] < 0:\n",
        "                    # scope 분기 내외로 최저치일 때\n",
        "                    if (trend[l].iloc[i-scope:i+scope+1].min() == trend[l].iloc[i]):\n",
        "                        turning_points[l].append(1)\n",
        "                    else:\n",
        "                        turning_points[l].append(0)\n",
        "                # 상승->하락 전환\n",
        "                else:\n",
        "                    # scope 분기 내외로 최고치일 때\n",
        "                    if (trend[l].iloc[i-scope:i+scope+1].max() == trend[l].iloc[i]):\n",
        "                        turning_points[l].append(-1)\n",
        "                    else:\n",
        "                        turning_points[l].append(0)\n",
        "            # 변곡점 아니면 생략\n",
        "            else:\n",
        "                turning_points[l].append(0)\n",
        "\n",
        "    # 마지막 scope 분기는 0으로 지정\n",
        "    for i in range(0, scope+1):\n",
        "        for l in trend.columns:\n",
        "            turning_points[l].append(0)\n",
        "\n",
        "    return turning_points\n",
        "\n",
        "\n",
        "def plot_axis(ax, data, label, val_names, val_marks=None,\n",
        "                colorset=COLORSET, markers=MARKERS, suffix=None, legend_loc=None):\n",
        "    \"\"\"단일축 차트 표시\"\"\"\n",
        "    # X축은 데이터의 인덱스로 구성\n",
        "    option = {'ax': ax, 'x': data.index.to_series().astype(str)}\n",
        "    # Y축은 컬럼별 변수로 구성\n",
        "    for i, val in enumerate(data.columns):\n",
        "        option['y'] = data[val]\n",
        "        option['color'] = colorset[i]\n",
        "        sns.lineplot(linestyle='-', label=val_names[val]+suffix, linewidth=1.0, **option)\n",
        "        # 변수에 대한 marker 옵션이 있는 경우 표시\n",
        "        if (val_marks is not None and val in val_marks):\n",
        "            for mark in val_marks[val]:\n",
        "                sns.lineplot(linestyle='',  marker=markers[i], markevery=mark, **option)\n",
        "    # 범례 및 라벨 표시\n",
        "    ax.legend(frameon=False, loc=legend_loc)\n",
        "    ax.set(ylabel=label)\n",
        "\n",
        "\n",
        "def plot_twin(data, val_names, left_label, right_label, left_vals, right_vals):\n",
        "    \"\"\"이중축 차트 표시\"\"\"\n",
        "    #왼쪽 y축 표시\n",
        "    _, left_ax = plt.subplots()\n",
        "    plot_axis(left_ax, data[left_vals], left_label, val_names, **LEFT_OPTION)\n",
        "    #오른쪽 y축 표시\n",
        "    right_ax = plt.twinx()\n",
        "    plot_axis(right_ax, data[right_vals], right_label, val_names, **RIGHT_OPTION)\n",
        "\n",
        "    # x축 범위 제한\n",
        "    left_ax.set(xlabel=' ')\n",
        "    left_ax.set_xlim([data.index.astype(str).min(), data.index.astype(str).max()])\n",
        "    # x축 틱 표시\n",
        "    suffix = data.index.astype(str).min()[4:]\n",
        "    years = data.resample('Y').count().index.astype(str)\n",
        "    plt.xticks([year+suffix for year in years], labels=[year[-2:] for year in years])\n",
        "    plt.plot()\n",
        "\n",
        "\n",
        "def plot_twin_marks(data, val_names, left_label, right_label, left_marks, right_marks):\n",
        "    \"\"\"마커가 있는 이중축 차트 표시\"\"\"\n",
        "    #왼쪽 y축 표시\n",
        "    _, left_ax = plt.subplots()\n",
        "    left_vals = [val for val in left_marks.keys()]\n",
        "    plot_axis(left_ax, data[left_vals], left_label, val_names, left_marks, **LEFT_OPTION)\n",
        "    #오른쪽 y축 표시\n",
        "    right_ax = plt.twinx()\n",
        "    right_vals = [val for val in right_marks.keys()]\n",
        "    plot_axis(right_ax, data[right_vals], right_label, val_names, right_marks, **RIGHT_OPTION)\n",
        "\n",
        "    # x축 범위 제한\n",
        "    left_ax.set(xlabel=' ')\n",
        "    left_ax.set_xlim([data.index.astype(str).min(), data.index.astype(str).max()])\n",
        "    # left_ax.set_ylim([-0.6, 0.8])\n",
        "    # right_ax.set_ylim([-1, 7])\n",
        "    # x축 틱 표시\n",
        "    suffix = data.index.astype(str).min()[4:]\n",
        "    years = data.resample('Y').count().index.astype(str)\n",
        "    plt.xticks([year+suffix for year in years], labels=[year[-2:] for year in years])\n",
        "    plt.plot()\n",
        "\n",
        "\n",
        "class Scraper:\n",
        "    \"\"\"웹 세션 및 요청 관리를 위한 클래스\"\"\"\n",
        "    def __init__(self, url=None, headers={}):\n",
        "        self.url = url\n",
        "        self.headers = headers\n",
        "        self.sess = requests.Session()\n",
        "        retry = Retry(total=5, connect=3, backoff_factor=0.5)\n",
        "        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
        "        self.sess.mount('http://', adapter)\n",
        "        self.sess.mount('https://', adapter)\n",
        "\n",
        "    def request(self, service, params={}):\n",
        "        try:\n",
        "            res = self.sess.get(self.url + service, params=params, headers=self.headers)\n",
        "            if res.status_code != 200:\n",
        "                print('Error {0}: {1}'.format(res.status_code, res.url))\n",
        "                return None\n",
        "        except Exception as ex:\n",
        "            print('Error {0}: {1}'.format(ex, res.url))\n",
        "            return None\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "def collect_links(scraper, keywords, date, rewrite=True):\n",
        "    \"\"\"뉴스기사 링크 수집\"\"\"\n",
        "    # 뉴스검색 키워드 정의\n",
        "    # 뉴스검색을 위한 조건변수\n",
        "    params = {\n",
        "        'where': 'news',\n",
        "        'query': ' | '.join(keywords),\n",
        "        'pd': 3, 'ds': date, 'de':date\n",
        "    }\n",
        "\n",
        "    # 결과를 저장할 파일\n",
        "    output_path = '{0}/{1}.txt'\n",
        "    output_file = output_path.format(LINKS, date)\n",
        "\n",
        "    # 이미 파일이 존재하는 경우 옵션에 따라 삭제 혹은 무시\n",
        "    if os.path.exists(output_file):\n",
        "        if (rewrite):\n",
        "            os.remove(output_file)\n",
        "        else:\n",
        "            print(output_file, 'already exists!!!')\n",
        "            return\n",
        "\n",
        "    # 검색결과 페이지, 페이지별 목록 개수, 최대 페이지수 정의\n",
        "    page = 1\n",
        "    unit_pages = 10\n",
        "    max_pages = 100\n",
        "    is_last_page = False\n",
        "\n",
        "    links = 0\n",
        "    news_url_list = set()\n",
        "\n",
        "    # 뉴스검색 요청 및 링크 수집\n",
        "    while not is_last_page and page < max_pages:\n",
        "        # 시작 인덱스를 지정하여 검색요청\n",
        "        params['start'] = 1 + unit_pages*(page-1)\n",
        "        raw = scraper.request('/search.naver', params)\n",
        "        # HTML 분석하여 뉴스기사 목록 획득\n",
        "        html = BeautifulSoup(raw.text, 'html.parser')\n",
        "        list_articles = html.select('div.group_news > ul.list_news > li div.info_group')\n",
        "        # 마지막페이지인 경우 검색요청을 중단\n",
        "        if len(list_articles) < unit_pages:\n",
        "            is_last_page = True\n",
        "        else:\n",
        "            page = page + 1\n",
        "\n",
        "        # 네이버 제공 뉴스(news.naver.com) 링크 추출\n",
        "        for article in list_articles:\n",
        "            tmp_link = article.select('a')\n",
        "            # 링크 갯수가 1개 이하인 경우 제외(언론사 사이트 링크만 있는 경우)\n",
        "            if len(tmp_link) < 2:\n",
        "                continue\n",
        "            # 언론사명, 뉴스기사 링크를 추출해서 결과를 set에 저장\n",
        "            tmp_press = tmp_link[0].get_text()\n",
        "            tmp_link = tmp_link[1]['href']\n",
        "            news_url_list.add((tmp_press, tmp_link))\n",
        "\n",
        "        # 검색된 뉴스와 연관된 뉴스기사 목록(list_related_news)을 추출\n",
        "        list_related_news = []\n",
        "        for r in list_related_news:\n",
        "            tmp_press = r.select('cite')\n",
        "            tmp_link = r.select('a.sub_txt')\n",
        "            if len(tmp_press) * len(tmp_link) == 0:\n",
        "                continue\n",
        "            # 언론사명, 뉴스기사 링크를 추출해서 결과를 set에 저장\n",
        "            tmp_press = tmp_press[0].get_text()\n",
        "            tmp_link = tmp_link[0]['href']\n",
        "            news_url_list.add((tmp_press, tmp_link))\n",
        "\n",
        "    # 결과를 파일로 저장\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for t in news_url_list:\n",
        "            press = t[0].replace('언론사 선정', '').replace(LF, '')\n",
        "            link = t[1].replace(LF, '')\n",
        "            if 'sports.news.naver.com' in link:\n",
        "                continue\n",
        "            links = links + 1\n",
        "            # 저장형식 : 날짜, 언론사명, 네이버뉴스 링크 URL\n",
        "            f.write(TAB.join([date, press, link]) + LF)\n",
        "\n",
        "    # 수집한 URL 개수 출력\n",
        "    print('{}: {:,} 링크'.format(date, links))\n",
        "    return links\n",
        "\n",
        "\n",
        "def cleansing(text):\n",
        "    \"\"\"뉴스기사 텍스트 클린징\"\"\"\n",
        "    # 괄호 안 텍스트 삭제\n",
        "    patterns = '|'.join([r'\\([^)]*\\)', r'\\<[^>]*\\>', r'\\[[^]]*\\]'])\n",
        "    text = re.sub(patterns, '', text)\n",
        "\n",
        "    # 특수기호 문자 치환\n",
        "    codes = {\n",
        "                '\"': ['&lsquo;', '&ldquo;', '&rsquo;', '&rdquo;'],\n",
        "                '~': ['&sim;'],\n",
        "                '·': ['&middot;'],\n",
        "                ' ': ['\\t', '\\n', '\\r']\n",
        "                }\n",
        "    for char, codes in codes.items():\n",
        "        for code in codes:\n",
        "            text = text.replace(code, char)\n",
        "\n",
        "    # 공백 등 2번 이상 반복되는 경우 수정\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = re.sub('\"+', '\"', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_article(scraper, url):\n",
        "    \"\"\"HTML 페이지에서 뉴스기사를 추출\"\"\"\n",
        "    request = urlparse(url)\n",
        "    try:\n",
        "        raw = scraper.request(request.path, parse_qs(request.query))\n",
        "        html = BeautifulSoup(raw.text, 'html.parser')\n",
        "    except Exception as err:\n",
        "        return None\n",
        "    # 제목 추출\n",
        "    title_part = html.find('h2', class_='media_end_head_headline')\n",
        "    if title_part is None:\n",
        "        title_part = ''\n",
        "    else:\n",
        "        title_part = title_part.get_text().replace(LF, '')\n",
        "    # 본문 추출\n",
        "    body_part = html.find('div', id='newsct_article')\n",
        "    if body_part is None:\n",
        "        return None\n",
        "    # 본문 텍스트 정제\n",
        "    body_part = cleansing(body_part.get_text().replace('다.', '다. ').replace(LF, ' ')).strip()\n",
        "    return (title_part, body_part)\n",
        "\n",
        "\n",
        "def collect_texts(scraper, date, rewrite=True):\n",
        "    \"\"\"뉴스기사 본문을 수집\"\"\"\n",
        "    # 입출력 파일 경로\n",
        "    file_path = '{0}/{1}.txt'\n",
        "    in_file_path  = file_path.format(LINKS, date)\n",
        "    out_file_path = file_path.format(TEXTS, date)\n",
        "    # 문장의 최대, 최소 길이 제한\n",
        "    min_sent_len = 10\n",
        "    max_sent_len = 150\n",
        "\n",
        "    # 이미 파일이 존재하는 경우 옵션에 따라 삭제 혹은 무시\n",
        "    if os.path.exists(out_file_path):\n",
        "        if (rewrite):\n",
        "            os.remove(out_file_path)\n",
        "        else:\n",
        "            print(out_file_path, 'already exists!!!')\n",
        "            return\n",
        "\n",
        "    articles = 0\n",
        "    total_texts = 0\n",
        "    title_set = set()\n",
        "\n",
        "    # 링크별 기사 본문 수집\n",
        "    with open(in_file_path, 'r', encoding='utf-8') as in_file, \\\n",
        "      open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
        "        out_file.write(TEXT_HEADER)\n",
        "        links = in_file.readlines()\n",
        "        for link in links:\n",
        "            item = link.split(TAB)\n",
        "            if len(item) < 3:\n",
        "                continue\n",
        "\n",
        "            # 언론사명, 뉴스링크로 뉴스기사 추출\n",
        "            press_name = item[1]\n",
        "            news_link = item[2]\n",
        "            content = parse_article(scraper, news_link)\n",
        "            # 내용이 없으면 무시\n",
        "            if content is None:\n",
        "                continue\n",
        "            # 기사제목\n",
        "            title = content[0].replace(LF, '').strip()\n",
        "            # 제목이 중복되는 기사가 존재하면 무시\n",
        "            if (press_name, title) in title_set:\n",
        "                continue\n",
        "\n",
        "            title_set.add((press_name, title))\n",
        "            articles += 1\n",
        "            body = content[1].replace(LF, ' ').replace(TAB, ' ').strip()\n",
        "            # 본문을 문장별로 구분\n",
        "            sents = sent_tokenize(body)\n",
        "            texts = 0\n",
        "            for sent in [title] + sents:\n",
        "                # 문장의 길이가 너무 짧거나 긴경우에는 제외\n",
        "                if len(sent) < min_sent_len or len(sent) > max_sent_len:\n",
        "                    continue\n",
        "                texts += 1\n",
        "                # 날짜, 기사번호, 문장(제목 및 내용) 순으로 저장\n",
        "                out_file.write(TAB.join([date, str(articles), sent]) + LF)\n",
        "            total_texts += texts\n",
        "\n",
        "    # 수집한 기사, 문장 개수 출력\n",
        "    print('{}: {:,} 기사, {:,} 문장'.format(date, articles, total_texts))\n",
        "    return articles, total_texts\n",
        "\n",
        "\n",
        "def concatenate_texts(month):\n",
        "    \"\"\"해당월의 일별 뉴스기사를 통합\"\"\"\n",
        "    files = [f for f in os.listdir(TEXTS)\n",
        "                if f.endswith('.txt') and f.startswith(month)]\n",
        "    files.sort()\n",
        "\n",
        "    texts = []\n",
        "    for file in files:\n",
        "        file_path = '{0}/{1}'.format(TEXTS, file)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            texts += lines[1:]\n",
        "        os.remove(file_path)\n",
        "\n",
        "    file_path = '{0}/{1}.txt'.format(TEXTS, month)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(TEXT_HEADER)\n",
        "        for s in texts:\n",
        "            f.write(s)"
      ],
      "metadata": {
        "id": "CafV_LWyE8T7",
        "outputId": "4b1ce404-1f4b-40a4-b57d-05bd7f168a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'iti'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3085997b6c80>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 인플레이션 어조지수 모듈\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0miti\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# GPU가 인식되었는지 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'iti'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}