{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORh3s9d6OUKoQnp3GioK+d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezu98/colab/blob/main/mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "## from pmdarima.arima import auto_arima\n",
        "import os"
      ],
      "metadata": {
        "id": "bQxfcTJnDAds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n",
        "from scipy.stats import t as t_statistic\n"
      ],
      "metadata": {
        "id": "cUkU1uvW1mbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.diagnostic import breaks_cusumolsresid\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from statsmodels.regression.linear_model import OLS"
      ],
      "metadata": {
        "id": "7FdJX3WCGEQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "fFIoyt6-hPns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Excel file into a Pandas dataframe\n",
        "df = pd.read_excel('raw.xlsx', sheet_name='Sheet2')\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "1skTCEX5HhTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the quadratic trend model\n",
        "df['time'] = np.arange(len(df))\n",
        "df['time_sq'] = df['time'] ** 2\n",
        "df['constant'] = 1"
      ],
      "metadata": {
        "id": "36-TJuKhHOyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "F-aFPwH6Z0bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Index 시리즈 피팅\n",
        "1.1 Index 시리즈가 2차항을 가지고 있는지 평가"
      ],
      "metadata": {
        "id": "6o_07f4aU2DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(df['Index'], df[['constant', 'time', 'time_sq']])\n",
        "results = model.fit(cov_type='HAC', cov_kwds={'maxlags': 1})\n",
        "print(results.summary())\n",
        "\n",
        "# Extract the fitted series\n",
        "df['fitted_values'] = results.fittedvalues"
      ],
      "metadata": {
        "id": "6jLWtuYmVE6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the HP filter to the 'index' series without considering the quadratic trend\n",
        "cycle, trend = sm.tsa.filters.hpfilter(df['Index'], lamb=1600)\n",
        "\n",
        "# Add the HP-filtered trend back to the DataFrame\n",
        "df['hp_trend'] = trend"
      ],
      "metadata": {
        "id": "pL0UP6aiNk4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the original data, the quadratic trend, and the HP-filtered trend\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'], df['Index'], label='Original Data')\n",
        "plt.plot(df['Date'], df['fitted_values'], label='Quadratic Trend', linestyle='--', color='r')\n",
        "plt.plot(df['Date'], df['hp_trend'], label='HP Filtered Trend', linestyle='--', color='g')\n",
        "plt.legend()\n",
        "\n",
        "# Set the X-axis with the 'date' column\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Index')\n",
        "plt.title('Comparison of Quadratic Trend and HP Filtered Trend with Original Data')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c6OdaWlVVzRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "& Structural Change\n",
        "2019M10"
      ],
      "metadata": {
        "id": "_R1kvHugBYLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IRdMZkSvz1-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "& 번외: 기대인플레이션 Persistance"
      ],
      "metadata": {
        "id": "JaBt0gfsz2nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Define AR model to check a persistance\n",
        "\n",
        "# Define the maximum number of lags you want to check\n",
        "max_lags = 10\n",
        "\n",
        "# Select the optimal number of lags using information criteria\n",
        "result = ar_select_order(df['CPI_exp_1y'], max_lags, old_names=False, ic='aic', trend='ct')  ## aic, bic, hqic\n",
        "\n",
        "# Obtain the selected lags based on information criteria\n",
        "selected_lags = result.ar_lags\n",
        "\n",
        "# Build the AR model with the selected lags\n",
        "model = AutoReg(df['CPI_exp_1y'], lags=selected_lags, trend='ct')\n",
        "ar_result = model.fit()\n",
        "\n",
        "# Get the parameters of the AR model\n",
        "params = ar_result.params\n",
        "\n",
        "# Sum the coefficients of the AR terms\n",
        "sum_of_coefficients = np.sum(params[1:])  # Exclude the intercept term\n",
        "\n",
        "# Print the selected lags and the sum of coefficients\n",
        "print(f\"The selected lags based on information criteria: {selected_lags}\")\n",
        "print(f\"The sum of coefficients of the selected AR terms is: {sum_of_coefficients}\")"
      ],
      "metadata": {
        "id": "mlqpMOj7z2pi",
        "outputId": "8379a0bb-dc4e-4e39-db13-39c89da32c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The selected lags based on information criteria: [1, 2, 3, 4]\n",
            "The sum of coefficients of the selected AR terms is: 0.9638205197714442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Set the null hypothesis value\n",
        "null_hypothesis_value = 1\n",
        "\n",
        "# Calculate the standard error of the sum of coefficients\n",
        "standard_error_sum = np.sqrt(np.sum(ar_result.bse[1:]**2))\n",
        "\n",
        "# Calculate the t-statistic\n",
        "t_stat = (sum_of_coefficients - null_hypothesis_value) / standard_error_sum\n",
        "\n",
        "# Calculate the degrees of freedom\n",
        "degrees_of_freedom = len(selected_lags)\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 2 * t_statistic.cdf(-np.abs(t_stat), degrees_of_freedom)\n",
        "\n",
        "# Print the t-statistic and the corresponding p-value\n",
        "print(f\"t-statistic: {t_stat}\")\n",
        "print(f\"p-value: {p_value}\")"
      ],
      "metadata": {
        "id": "sGZbVHLy4Lha",
        "outputId": "536fc6b6-505d-4250-dfa1-258b6553cb19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t-statistic: -0.21504817064576667\n",
            "p-value: 0.840249154166426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "YjsKXQINSe1I",
        "outputId": "6b1c5e8d-0765-4a8d-b984-10273281a574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Date  CPI   KRWUSD  KRWUSD_lag  KRWUSD_lag_mc    OIL  \\\n",
            "CPI_exp_1y                                                              \n",
            "3.088965   2006-09-01  2.5   953.68      960.72          10.57  63.82   \n",
            "3.076999   2006-10-01  2.2   954.23      953.68          -7.04  58.85   \n",
            "3.087811   2006-11-01  2.1   936.22      954.23           0.55  59.13   \n",
            "3.059719   2006-12-01  2.1   925.75      936.22         -18.01  62.00   \n",
            "3.084474   2007-01-01  1.7   936.36      925.75         -10.47  54.24   \n",
            "...               ...  ...      ...         ...            ...    ...   \n",
            "3.113319   2023-04-01  3.7  1320.01     1305.73          34.99  79.44   \n",
            "3.221883   2023-05-01  3.3  1328.21     1320.01          14.28  71.59   \n",
            "3.474617   2023-06-01  2.7  1296.71     1328.21           8.20  70.23   \n",
            "3.637580   2023-07-01  2.3  1286.30     1296.71         -31.50  76.39   \n",
            "3.664992   2023-08-01  3.4  1318.47     1286.30         -10.41  81.40   \n",
            "\n",
            "            OIL_lag  OIL_lag_mc  UNEMPLOYMENT  CPI_Level  ...  D_covid  D_fc  \\\n",
            "CPI_exp_1y                                                ...                  \n",
            "3.088965      73.04       -1.36           3.5     76.868  ...        0     0   \n",
            "3.076999      63.82       -9.22           3.6     76.496  ...        0     0   \n",
            "3.087811      58.85       -4.97           3.5     76.125  ...        0     0   \n",
            "3.059719      59.13        0.28           3.4     76.348  ...        0     0   \n",
            "3.084474      62.00        2.87           3.4     76.496  ...        0     0   \n",
            "...             ...         ...           ...        ...  ...      ...   ...   \n",
            "3.113319      73.37       -3.47           2.6    110.800  ...        0     0   \n",
            "3.221883      79.44        6.07           2.5    111.130  ...        0     0   \n",
            "3.474617      71.59       -7.85           2.6    111.120  ...        0     0   \n",
            "3.637580      70.23       -1.36           2.8    111.200  ...        0     0   \n",
            "3.664992      76.39        6.16           2.4    112.330  ...        0     0   \n",
            "\n",
            "              SRT    LRT    mae_1y     IP    cd  time  time_sq  constant  \n",
            "CPI_exp_1y                                                                \n",
            "3.088965    4.550  4.890  0.588965   72.6  4.64     0        0         1  \n",
            "3.076999    4.560  4.790  0.876999   75.3  4.57     1        1         1  \n",
            "3.087811    4.580  4.910  0.987811   74.6  4.60     2        4         1  \n",
            "3.059719    4.650  4.950  0.959719   73.3  4.76     3        9         1  \n",
            "3.084474    4.800  5.040  1.384474   73.8  4.92     4       16         1  \n",
            "...           ...    ...       ...    ...   ...   ...      ...       ...  \n",
            "3.113319    3.241  3.319  2.054167  109.8  3.50   199    39601         1  \n",
            "3.221883    3.378  3.402  1.813333  110.6  3.64   200    40000         1  \n",
            "3.474617    3.518  3.611  1.449167  110.6  3.75   201    40401         1  \n",
            "3.637580    3.586  3.681  1.135000  109.7  3.75   202    40804         1  \n",
            "3.664992    3.567  3.860  1.005000  112.1  3.70   203    41209         1  \n",
            "\n",
            "[204 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the 'cpi_exp_1y' column is already in datetime format\n",
        "df.set_index('CPI_exp_1y', inplace=True)\n",
        "\n",
        "# Convert start and end dates to datetime format\n",
        "start_date_subset = pd.to_datetime('2019-10-01')\n",
        "end_date_subset = pd.to_datetime('2023-08-01')\n",
        "\n",
        "# Divide the time series into subsets\n",
        "subset = df.loc[(df.index >= start_date_subset) & (df.index <= end_date_subset)]\n",
        "\n",
        "# Print the subset\n",
        "print(subset)"
      ],
      "metadata": {
        "id": "FNZ6E7iSSERH",
        "outputId": "127bde48-6848-439a-ba4f-50dcdfec8935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'CPI_exp_1y'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-2d704d09f5e1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check if the 'cpi_exp_1y' column is already in datetime format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CPI_exp_1y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CPI_exp_1y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Convert start and end dates to datetime format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'CPI_exp_1y'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### sub-set process\n",
        "\n",
        "# Define the maximum number of lags you want to check\n",
        "max_lags = 10\n",
        "\n",
        "# Select the optimal number of lags using information criteria\n",
        "result = ar_select_order(df['CPI_exp_1y'], max_lags, old_names=False, ic='aic', trend='ct')  ## aic, bic, hqic\n",
        "\n",
        "# Obtain the selected lags based on information criteria\n",
        "selected_lags = result.ar_lags\n",
        "\n",
        "# Build the AR model with the selected lags\n",
        "model = AutoReg(df['CPI_exp_1y'], lags=selected_lags, trend='ct')\n",
        "ar_result = model.fit()\n",
        "\n",
        "# Get the parameters of the AR model\n",
        "params = ar_result.params\n",
        "\n",
        "# Sum the coefficients of the AR terms\n",
        "sum_of_coefficients = np.sum(params[1:])  # Exclude the intercept term\n",
        "\n",
        "# Print the selected lags and the sum of coefficients\n",
        "print(f\"The selected lags based on information criteria: {selected_lags}\")\n",
        "print(f\"The sum of coefficients of the selected AR terms is: {sum_of_coefficients}\")\n",
        "\n",
        "### Set the null hypothesis value\n",
        "null_hypothesis_value = 1\n",
        "\n",
        "# Calculate the standard error of the sum of coefficients\n",
        "standard_error_sum = np.sqrt(np.sum(ar_result.bse[1:]**2))\n",
        "\n",
        "# Calculate the t-statistic\n",
        "t_stat = (sum_of_coefficients - null_hypothesis_value) / standard_error_sum\n",
        "\n",
        "# Calculate the degrees of freedom\n",
        "degrees_of_freedom = len(selected_lags)\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 2 * t_statistic.cdf(-np.abs(t_stat), degrees_of_freedom)\n",
        "\n",
        "# Print the t-statistic and the corresponding p-value\n",
        "print(f\"t-statistic: {t_stat}\")\n",
        "print(f\"p-value: {p_value}\")"
      ],
      "metadata": {
        "id": "mGnuqEjoRWxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전과 이후 구분"
      ],
      "metadata": {
        "id": "6VkArYr5MjZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows of the dataframe\n",
        "# print(df.head())  # five row display\n",
        "df.columns  # column index\n",
        "df.info()\n",
        "# df.shape  # matrix size"
      ],
      "metadata": {
        "id": "oF1DycSaDM-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8MEbSPrEJRb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SImple OLS\n",
        "\n",
        "X = df[['mee','cha', 'mper']]\n",
        "\n",
        "# Define a list of dependent variables\n",
        "dependent_vars = ['absc_call', 'absc_cd', 'absc_b_3m', 'absc_b_1y', 'absc_b_3y', 'absc_b_5y', 'absc_b_10y']\n",
        "\n",
        "# Create an empty list to store the results of each OLS regression\n",
        "results = []\n",
        "\n",
        "# Loop through the dependent variables and fit the OLS model with each variable\n",
        "for var in dependent_vars:\n",
        "    y = df[var]\n",
        "    X = sm.add_constant(X)\n",
        "    model =  sm.OLS(y, X).fit(cov_type='HC3')\n",
        "    results.append(model)\n",
        "\n",
        "    # Print the results of each OLS regression\n",
        "    print(f\"Model {len(results)}: {var} as dependent variable\\n{model.summary()}\")\n",
        "\n",
        "# Plot the residual graph for the current model\n",
        "    plt.plot(model.resid)\n",
        "    plt.axhline(y=0, color='red', linestyle='--')\n",
        "    plt.title(f\"Residual plot for {var}\")\n",
        "    plt.xlabel(\"Observation\")\n",
        "    plt.ylabel(\"Residual\")\n",
        "    plt.show()\n",
        "\n",
        "# 1. 종속변수와 독립변수간 선형관계 - 당연히 만족\n",
        "# 2. 독립변수와 오차항간 상관관계가 없을 것 - 독립변수 설정 잘하면 문제 없음\n",
        "# 3. 오차항의 기대값은 0 - 랜덤웍\n",
        "# 4. 오차항은 정규분포 - 모집단 많으면 해결\n",
        "\n",
        "# 5. 오차항의 분산이 모든 관찰치에서 일정한 상수일 것, 아닐 경우 이분산성(Heteroskedasticity) 발생\n",
        "## 무조건부 이분산성은 독립변수 관찰값과 상관없는 이분산, 조건부(Conditional Variance) 이분산은 독립변수 관찰값과 상관있는 이분산, 조건부 이분산은 GARCH 사용\n",
        "## Breusch-Pagan Test로 검정\n",
        "## H0 = 이분산성 비존재, P value < 0.05시 기각가능해서 이분산 존재\n",
        "## HAC (Newey-West Standard error, HAC Heteroskedasticity and Autocorrelation Consistent) standard errors로 수정가능, 이 경우 robust standard error\n",
        "\n",
        "    # Breusch-Pagan test for heteroscedasticity\n",
        "    bp_test = het_breuschpagan(model.resid, X)\n",
        "    print(f\"Breusch-Pagan test statistic: {bp_test[0]:.3f}\")\n",
        "    print(f\"Breusch-Pagan p-value: {bp_test[1]:.3f}\")\n",
        "    if bp_test[1] < 0.05:\n",
        "        print(\"Warning: The residuals exhibit heteroscedasticity.\")\n",
        "\n",
        "# 6. 오차항간 시차상관관계가 없을 것, 아닐 경우 계열상관성(Autoregression) 발생\n",
        "## HAC (Newey-West Standard error, HAC Heteroskedasticity and Autocorrelation Consistent) standard errors로 수정가능, 이 경우 robust standard error\n",
        "## Calculate Durbin-Watson statistic\n",
        "## Durbin-Watson statistic for autocorrelation\n",
        "    dw = durbin_watson(model.resid)\n",
        "    print(f\"Durbin-Watson statistic: {dw:.3f}\")\n",
        "## check for autocorrelation\n",
        "\n",
        "    if dw < 1.5 or dw > 2.5:\n",
        "        print(\"Warning: The residuals exhibit autocorrelation.\")\n",
        "\n",
        "\n",
        " # Jarque-Bera test for normality of residuals\n",
        "    jb_test = jarque_bera(model.resid)\n",
        "    test_statistic = jb_test[0]\n",
        "    p_value = jb_test[1]\n",
        "    if p_value < 0.05:\n",
        "        print(\"Warning: The residuals are not normally distributed (p-value = {:.4f})\".format(p_value))\n",
        "    else:\n",
        "        print(\"The residuals are normally distributed (p-value = {:.4f})\".format(p_value))\n",
        "\n",
        "    # ADF test for stationarity of residuals\n",
        "    residuals = model.resid\n",
        "    adf_test = sm.tsa.stattools.adfuller(residuals)\n",
        "    test_statistic = adf_test[0]\n",
        "    p_value = adf_test[1]\n",
        "    if p_value < 0.05:\n",
        "        print(\"Warning: The residuals have a unit root (p-value = {:.4f})\".format(p_value))\n",
        "    else:\n",
        "        print(\"The residuals do not have a unit root (p-value = {:.4f})\".format(p_value))"
      ],
      "metadata": {
        "id": "RHg_oDgsgeYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 독립변수간 상관관계가 없을 것, 아닐 경우 다중공선성(Multicollinearity) 발생\n",
        "## calculate the VIF for each independent variable\n",
        "vif = pd.DataFrame()\n",
        "independent_vars=['mee2','cha','mper']   # only pass the column names\n",
        "vif[\"variables\"] = independent_vars\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df[independent_vars].values, i) for i in range(len(independent_vars))]\n",
        "\n",
        "### check for high VIFs (typically a VIF greater than 5 or 10 is considered high)\n",
        "high_vif = vif[vif['VIF'] > 5]\n",
        "\n",
        "# print the variables with high VIFs\n",
        "if len(high_vif) > 0:\n",
        "    print(\"Warning: The following variables have high VIFs:\")\n",
        "    print(high_vif)\n",
        "\n",
        "    # if there are high VIFs, drop one of the variables with the highest VIF\n",
        "    var_to_drop = high_vif.sort_values('VIF', ascending=False)['variables'].iloc[0]\n",
        "    df.drop(var_to_drop, axis=1, inplace=True)\n",
        "    independent_vars.remove(var_to_drop)\n",
        "else:\n",
        "    print(\"No variable has a high VIF.\")"
      ],
      "metadata": {
        "id": "yAkByoqkDMYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 금융데이터처럼 변동성이 시간에 따라 변화(clustering) = 시계열 데이터의 조건부 분산이 과거 시점의 오차항과 자기상관\n",
        "# 시변하는 조건부 분산을 GARCH로 예측\n",
        "## 실제 value와 fit value로 arch가 잘 맞는지 보고\n",
        "## redisual이 정규분포하는지 보고\n",
        "## conditional variance가 시변하는지 봄\n",
        "### 이후 conditional variance그래프를 그리고 주요 뉴스 전후로 어떻게 바뀌는지 확인\n",
        "\n",
        "# 참고\n",
        "## AR(1)은 t기의 오차항이 t-1기의 오차항과 관련\n",
        "## ARCH는 t기의 오차항의 분산이 t-1기의 오차항 분산과 관련\n",
        "### 오차항의 분산이 전기 오차항의 제곱에 관련 있는 경우를 autoregressive conditional heteroscedasticity(ARCH)\n",
        "### 오차항의 분산이 여러 기의 오차항 제곱과 관련 있는 경우 generalized ARCH(GARCH)\n",
        "### 금융모형 다수 등장"
      ],
      "metadata": {
        "id": "WOAEq96IK0vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GARCH (Generalized Autoregressive Conditional Heteroscedasticity) models, the parameters (p,q) represent the orders of the autoregressive and moving average terms, respectively, that are used to model the conditional variance of a time series.\n",
        "\n",
        "The \"p\" parameter specifies the order of the autoregressive term, which models the dependence of the current variance on the past variances. The \"q\" parameter specifies the order of the moving average term, which models the dependence of the current variance on the past squared error terms.\n",
        "\n",
        "Together, the (p,q) parameters determine the complexity of the GARCH model and how well it can capture the volatility patterns in the data. In practice, these parameters are usually chosen through a process of model selection, such as using information criteria like the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC), or through cross-validation techniques.\n",
        "\n",
        "delta(y_t) = beta_0 + beta_1*y_(t-1)(=AR1)+e_t(=error term) ~ e_t=beta_2(square(hat^2_t))   ~ N(0, hat_t)\n",
        "hat^2_t = Omega + sigma(alpha*h_(t-p)(=AR term, p)) + (Beta*e_(t-q))(MA term, q)\n",
        "\n",
        "AR(1) 과정에서 오차항의 분산이 이분산성(시변성), 클러스터링 분산을 보일 경우\n",
        "오차항의 시변하는 분산 hat^2을 ARIMA 과정을 통해 별도로 추정\n",
        "이 때 Omega는 시변성이 없었을 경우 나타날 Constant mean, alpha는 AR Term의 계수\n",
        "Beta는 MA Term의 계수로 과거 에러텀 제곱의 영향"
      ],
      "metadata": {
        "id": "ow0LSz-aWqcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ranges of p and q to search over\n",
        "p_range = range(1, 3)\n",
        "q_range = range(1, 3)\n",
        "\n",
        "# Initialize the best model and criterion values\n",
        "best_aic = np.inf\n",
        "best_bic = np.inf\n",
        "best_p = 0\n",
        "best_q = 0\n",
        "\n",
        "# Loop through all combinations of p and q values\n",
        "for p in p_range:\n",
        "    for q in q_range:\n",
        "        # Fit a GARCH model for each combination of p and q\n",
        "        model = arch_model(df['b_3y_l'], vol='GARCH', p=p, q=q, mean='constant', dist='Normal')\n",
        "        result = model.fit(disp='off')\n",
        "\n",
        "        # Calculate the AIC and BIC for each model\n",
        "        aic = result.aic\n",
        "        bic = result.bic\n",
        "\n",
        "        # Check if the current model has a lower AIC or BIC than the previous best model\n",
        "        if aic < best_aic:\n",
        "            best_aic = aic\n",
        "            best_p = p\n",
        "            best_q = q\n",
        "\n",
        "        if bic < best_bic:\n",
        "            best_bic = bic\n",
        "            best_p = p\n",
        "            best_q = q\n",
        "\n",
        "# Print the optimal (p, q) values and criterion values\n",
        "print(\"Optimal (p, q) values based on AIC: ({}, {})\".format(best_p, best_q))\n",
        "print(\"AIC value for optimal model: {}\".format(best_aic))\n",
        "\n",
        "print(\"Optimal (p, q) values based on BIC: ({}, {})\".format(best_p, best_q))\n",
        "print(\"BIC value for optimal model: {}\".format(best_bic))"
      ],
      "metadata": {
        "id": "PdlSIKVwpsMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the GARCH(p,q) model\n",
        "\n",
        "# Define the independent variables\n",
        "y = df['b_3y_l']\n",
        "\n",
        "# Fit the GARCH(1,1) model\n",
        "am = arch_model(y, mean=\"AR\", vol=\"GARCH\", p=1, q=1, dist='Normal')     ## Clear Trend가 있을 경우 mean = AR, 아닐 경우 Conditional mean이 Constant하다고 하는 것이 보통\n",
        "results = am.fit()\n",
        "\n",
        "# Print model summary\n",
        "print(results.summary())\n",
        "\n",
        "## log-likelihood value = -6476.46 = fit well\n",
        "## coefficients of vol is fit well\n",
        "### omega coefficient = the constant component of the variance = 0.0253 > i.e., long-run average의 존재\n",
        "### alpha1 = conditional variance lagged one period > 0.1 > short past squard residuals affect currentvolitaility, AR(1) term\n",
        "### beta1 = conditional variance itself > 0.8000 > long past squared residuals affect current volitaility, MA(1) term\n",
        "## beta1 < 1 and alpha+beta < 1 (1,1)\n",
        "## i.e., the tendency for periods of high volatility to follow periods of high volatility and for periods of low volatility to follow periods of low volatility."
      ],
      "metadata": {
        "id": "u7q0qptpLPy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual and predicted values of the dependent variable:\n",
        "## means how well the model fits the actual data. closely = fit\n",
        "predicted = results.conditional_volatility\n",
        "actual = np.sqrt(results.resid**2)\n",
        "\n",
        "plt.plot(actual, label='Actual')\n",
        "plt.plot(predicted, label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the standardized residuals:\n",
        "## standardized residuals, normally distributed around zero = fit\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "std_resid = results.resid / results.conditional_volatility\n",
        "# (obs - actu / con var) = like z-score.\n",
        "# rule of thumb = between +-2 = ok\n",
        "\n",
        "plt.plot(std_resid)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the volatility of the dependent variable:\n",
        "## volatility of the dependent variable changes over time.\n",
        "## the volatility is constant = not fit\n",
        "plt.plot(results.conditional_volatility)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LlRfbDSYBHzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# conditional_volatility = results.conditional_volatility\n",
        "# fx = sns.boxplot(x='cha_change', y=conditional_volatility, data=df, palette='Set2',showfliers=False)\n",
        "# plt.ylim([0, 0.1])\n",
        "# fx.set_xlabel(\"Policy Rate Change\")\n",
        "# fx.set_ylabel(\"Conditional Variance\")\n",
        "\n",
        "fig, fx = plt.subplots(figsize=(2.5, 4))\n",
        "\n",
        "# df_filtered = df[df[\"mper_change\"] != \"all range\"]\n",
        "# fx = sns.boxplot(x='mper_change', y=conditional_volatility, data=df_filtered, palette='Set2',showfliers=False)\n",
        "# plt.ylim([0, 0.1])\n",
        "# fx.set_xlabel(\"MPR Publication\")\n",
        "# fx.set_ylabel(\"\")\n",
        "# fx.set(yticklabels=[])\n",
        "\n",
        "# df_filtered = df[df[\"mee2_change\"] != \"all range\"]\n",
        "# fx = sns.boxplot(x='mee2_change', y=conditional_volatility, data=df_filtered, palette='Set2', showfliers=False)\n",
        "# plt.ylim([0, 0.1])\n",
        "# fx.set_xlabel(\"Minutes w/o rate change\")\n",
        "# fx.set_ylabel(\"\")\n",
        "# fx.set(yticklabels=[])\n",
        "\n",
        "\n",
        "df_filtered = df[df[\"spe_change\"] != \"all range\"]\n",
        "fx = sns.boxplot(x='spe_change', y=conditional_volatility, data=df_filtered, palette='Set2', showfliers=False)\n",
        "plt.ylim([0, 0.1])\n",
        "fx.set_xlabel(\"Governer's Speech\")\n",
        "fx.set_ylabel(\"\")\n",
        "fx.set(yticklabels=[])\n"
      ],
      "metadata": {
        "id": "-xXcCE-lNpEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform tests for heteroskedasticity and autocorrelation\n",
        "lb_test = acorr_lm(results.resid)     # for autocorrelation\n",
        "arch_test = het_arch(results.resid)   # for heteroscedasticity\n",
        "\n",
        "\n",
        "# Check for heteroskedasticity and print a warning if detected\n",
        "print(\"ARCH-LM test statistic:\", arch_test[0])\n",
        "print(\"ARCH-LM test p-value:\", arch_test[1])\n",
        "\n",
        "if arch_test[1] < 0.05:\n",
        "    print(\"\\nWarning: Heteroskedasticity detected according to the ARCH-LM test.\")\n",
        "\n",
        "# Check for autocorrelation and print a warning if detected\n",
        "\n",
        "print(\"Lagrange Multiplier test statistic:\", lb_test[0])\n",
        "print(\"Lagrange Multiplier test p-value:\", lb_test[1])\n",
        "\n",
        "if lb_test[1] < 0.05:\n",
        "    print(\"\\nWarning: Autocorrelation detected according to the Lagrange Multiplier test.\")\n",
        "\n",
        "# Check if the model is ARCH-effective using the p-value of the LM test\n",
        "if lb_test[1] < 0.05:\n",
        "    print(\"\\nThe model is not ARCH-effective according to the LM test.\")\n",
        "else:\n",
        "    print(\"\\nThe model is ARCH-effective according to the LM test.\")"
      ],
      "metadata": {
        "id": "3fV4XiqYBNKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your fitted GARCH model is stored in the variable 'results'\n",
        "# forecast_volatility = results.forecast(horizon=1)\n",
        "# conditional_volatility = np.sqrt(forecast_volatility.variance.iloc[-len(df):])\n",
        "\n",
        "\n",
        "# Get the conditional volatility values\n",
        "conditionalvolatility = pd.Series(results.conditional_volatility, index=df.index)\n",
        "actualvolatility = np.sqrt(results.resid**2)\n",
        "\n",
        "# Export the data to an Excel file\n",
        "conditionalvolatility.to_excel('conditional_volatility.xlsx', header=['Conditional Volatility'])\n",
        "actualvolatility.to_excel('acualvolatility.xlsx', header=['Actual Volatility'])"
      ],
      "metadata": {
        "id": "8pJzyKXUBMvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between EGARCH (Exponential GARCH) and GARCH (Generalized Autoregressive Conditional Heteroscedasticity) models is the way they model the conditional volatility of a time series.\n",
        "\n",
        "GARCH models assume that the conditional volatility is a function of past squared error terms and past conditional variances. EGARCH models, on the other hand, allow the conditional variance to depend on both past squared error terms and past absolute error terms. This means that EGARCH models can capture the asymmetric effects of positive and negative shocks on volatility.\n",
        "\n",
        "In other words, the EGARCH model allows for the possibility that an unexpected shock that leads to a large negative return has a different impact on volatility than an unexpected shock that leads to a large positive return. This is important in financial applications, where volatility tends to increase more after large negative returns (i.e., \"leverage effect\").\n",
        "\n",
        "Overall, the EGARCH model is a more flexible model that can capture the asymmetric effects of shocks on volatility, while the GARCH model is a more simple model that assumes symmetric effects."
      ],
      "metadata": {
        "id": "Fs-Gcxia6t8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fit the EGARCH(p,q) model\n",
        "\n",
        "# # Define the independent variables\n",
        "# y = df['b_3y_l']\n",
        "\n",
        "# # Fit the EGARCH(1,1) model\n",
        "# am = arch_model(y, mean=\"AR\", vol=\"EGARCH\", p=1, q=1, dist='Normal')     ## Clear Trend가 있을 경우 mean = AR, 아닐 경우 Conditional mean이 Constant하다고 하는 것이 보통\n",
        "# results = am.fit()\n",
        "\n",
        "# # Print model summary\n",
        "# print(results.summary())"
      ],
      "metadata": {
        "id": "532BVoV76zJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}